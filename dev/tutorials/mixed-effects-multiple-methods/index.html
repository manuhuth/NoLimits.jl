<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Mixed-Effects Tutorial 1: Nonlinear Random-Effects Model Across Multiple Estimation Methods · NoLimits.jl</title><meta name="title" content="Mixed-Effects Tutorial 1: Nonlinear Random-Effects Model Across Multiple Estimation Methods · NoLimits.jl"/><meta property="og:title" content="Mixed-Effects Tutorial 1: Nonlinear Random-Effects Model Across Multiple Estimation Methods · NoLimits.jl"/><meta property="twitter:title" content="Mixed-Effects Tutorial 1: Nonlinear Random-Effects Model Across Multiple Estimation Methods · NoLimits.jl"/><meta name="description" content="Documentation for NoLimits.jl."/><meta property="og:description" content="Documentation for NoLimits.jl."/><meta property="twitter:description" content="Documentation for NoLimits.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="NoLimits.jl logo"/></a><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../installation/">Installation</a></li><li><a class="tocitem" href="../../capabilities/">Capabilities</a></li><li><input class="collapse-toggle" id="menuitem-4" type="checkbox"/><label class="tocitem" for="menuitem-4"><span class="docs-label">Model Building</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../model-building/">Overview</a></li><li><a class="tocitem" href="../../model-building/model-macro/">@Model</a></li><li><a class="tocitem" href="../../model-building/helpers/">@helpers</a></li><li><a class="tocitem" href="../../model-building/fixed-effects/">@fixedEffects</a></li><li><a class="tocitem" href="../../model-building/covariates/">@covariates</a></li><li><a class="tocitem" href="../../model-building/random-effects/">@randomEffects</a></li><li><a class="tocitem" href="../../model-building/pre-differential-equation/">@preDifferentialEquation</a></li><li><a class="tocitem" href="../../model-building/differential-equation/">@DifferentialEquation</a></li><li><a class="tocitem" href="../../model-building/initial-de/">@initialDE</a></li><li><a class="tocitem" href="../../model-building/formulas/">@formulas</a></li><li><a class="tocitem" href="../../model-building/universal-function-approximators/">Function Approximators (NNs + SoftTrees)</a></li></ul></li><li><a class="tocitem" href="../../data-model-construction/">Data Model Construction</a></li><li><input class="collapse-toggle" id="menuitem-6" type="checkbox"/><label class="tocitem" for="menuitem-6"><span class="docs-label">Estimation</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../estimation/">Overview</a></li><li><a class="tocitem" href="../../estimation/laplace/">Laplace</a></li><li><a class="tocitem" href="../../estimation/laplace-map/">Laplace MAP</a></li><li><a class="tocitem" href="../../estimation/mcem/">MCEM</a></li><li><a class="tocitem" href="../../estimation/saem/">SAEM</a></li><li><a class="tocitem" href="../../estimation/mcmc/">MCMC</a></li><li><a class="tocitem" href="../../estimation/vi/">VI</a></li><li><a class="tocitem" href="../../estimation/mle/">MLE</a></li><li><a class="tocitem" href="../../estimation/mle-map/">MAP</a></li><li><a class="tocitem" href="../../estimation/multistart/">Multistart</a></li></ul></li><li><input class="collapse-toggle" id="menuitem-7" type="checkbox"/><label class="tocitem" for="menuitem-7"><span class="docs-label">Uncertainty Quantification</span><i class="docs-chevron"></i></label><ul class="collapsed"><li><a class="tocitem" href="../../uncertainty-quantification/">Overview</a></li><li><a class="tocitem" href="../../uncertainty-quantification/wald/">Wald</a></li><li><a class="tocitem" href="../../uncertainty-quantification/profile-likelihood/">Profile likelihood</a></li><li><a class="tocitem" href="../../uncertainty-quantification/mcmc-based-uncertainty/">MCMC-based uncertainty</a></li></ul></li><li><a class="tocitem" href="../../plotting/">Plotting</a></li><li><input class="collapse-toggle" id="menuitem-9" type="checkbox" checked/><label class="tocitem" for="menuitem-9"><span class="docs-label">Tutorials</span><i class="docs-chevron"></i></label><ul class="collapsed"><li class="is-active"><a class="tocitem" href>Mixed-Effects Tutorial 1: Nonlinear Random-Effects Model Across Multiple Estimation Methods</a><ul class="internal"><li><a class="tocitem" href="#What-You-Will-Learn"><span>What You Will Learn</span></a></li><li><a class="tocitem" href="#Step-1:-Data-Setup"><span>Step 1: Data Setup</span></a></li><li><a class="tocitem" href="#Step-2:-Define-the-Nonlinear-Mixed-Effects-Model"><span>Step 2: Define the Nonlinear Mixed-Effects Model</span></a></li><li><a class="tocitem" href="#Step-3:-Build-the-DataModel-and-Configure-Estimation-Methods"><span>Step 3: Build the DataModel and Configure Estimation Methods</span></a></li><li><a class="tocitem" href="#Step-4:-Fit-All-Methods"><span>Step 4: Fit All Methods</span></a></li><li><a class="tocitem" href="#Step-5:-Compare-Objective-Values-(Laplace,-MCEM,-SAEM)"><span>Step 5: Compare Objective Values (Laplace, MCEM, SAEM)</span></a></li><li><a class="tocitem" href="#Step-6:-Fitted-Trajectories-for-the-First-Two-Individuals"><span>Step 6: Fitted Trajectories for the First Two Individuals</span></a></li><li><a class="tocitem" href="#Step-7:-Observation-Distribution-Diagnostics-(First-Individual)"><span>Step 7: Observation Distribution Diagnostics (First Individual)</span></a></li><li><a class="tocitem" href="#Step-8:-Uncertainty-Quantification-Across-Methods"><span>Step 8: Uncertainty Quantification Across Methods</span></a></li><li><a class="tocitem" href="#Interpretation-and-Practical-Guidance"><span>Interpretation and Practical Guidance</span></a></li></ul></li><li><a class="tocitem" href="../mixed-effects-ode-mcem/">Mixed-Effects Tutorial 2: ODE Model with Dosing Events (MCEM)</a></li><li><a class="tocitem" href="../mixed-effects-nn-saem/">Mixed-Effects Tutorial 3: Neural Differential-Equation Components (SAEM)</a></li><li><a class="tocitem" href="../mixed-effects-softtree-saem/">Mixed-Effects Tutorial 4: SoftTree Differential-Equation Components (SAEM)</a></li><li><a class="tocitem" href="../mixed-effects-seizure-counts-poisson-nb-mcem/">Mixed-Effects Tutorial 5: Seizure Counts with Poisson and NegativeBinomial Outcomes (MCEM)</a></li><li><a class="tocitem" href="../mixed-effects-left-censored-virload50-laplace/">Mixed-Effects Tutorial 6: Left-Censored Nonlinear Model (Laplace)</a></li><li><a class="tocitem" href="../mixed-effects-vi/">Mixed-Effects Tutorial 7: Variational Inference (VI)</a></li><li><a class="tocitem" href="../fixed-effects-nonlinear-mle-map/">Fixed-Effects Tutorial 1: Nonlinear Longitudinal Model (MLE + MAP)</a></li><li><a class="tocitem" href="../fixed-effects-vi/">Fixed-Effects Tutorial 2: Variational Inference (VI)</a></li></ul></li><li><a class="tocitem" href="../../how-to-contribute/">How to Contribute</a></li><li><a class="tocitem" href="../../api/">API</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Tutorials</a></li><li class="is-active"><a href>Mixed-Effects Tutorial 1: Nonlinear Random-Effects Model Across Multiple Estimation Methods</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Mixed-Effects Tutorial 1: Nonlinear Random-Effects Model Across Multiple Estimation Methods</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/manuhuth/NoLimits.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/manuhuth/NoLimits.jl/blob/main/docs/src/tutorials/mixed-effects-multiple-methods.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Mixed-Effects-Tutorial-1:-Nonlinear-Random-Effects-Model-Across-Multiple-Estimation-Methods"><a class="docs-heading-anchor" href="#Mixed-Effects-Tutorial-1:-Nonlinear-Random-Effects-Model-Across-Multiple-Estimation-Methods">Mixed-Effects Tutorial 1: Nonlinear Random-Effects Model Across Multiple Estimation Methods</a><a id="Mixed-Effects-Tutorial-1:-Nonlinear-Random-Effects-Model-Across-Multiple-Estimation-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Mixed-Effects-Tutorial-1:-Nonlinear-Random-Effects-Model-Across-Multiple-Estimation-Methods" title="Permalink"></a></h1><p>Nonlinear mixed-effects (NLME) models are a cornerstone of longitudinal data analysis in the biological sciences. They describe how individual trajectories vary around a shared population-level trend – capturing, for example, how different organisms grow at different rates toward different asymptotes, even when the underlying biological mechanism is the same. A natural question arises in practice: <strong>how sensitive are my conclusions to the estimation algorithm I choose?</strong> This tutorial addresses that question directly. You will fit a single nonlinear growth model to a classic biological dataset using four distinct estimation strategies, then compare the results in terms of fitted trajectories, observation-level predictions, and parameter uncertainty. By the end, you will have a practical template for multi-method comparison and a clear intuition for when each approach is most appropriate.</p><h2 id="What-You-Will-Learn"><a class="docs-heading-anchor" href="#What-You-Will-Learn">What You Will Learn</a><a id="What-You-Will-Learn-1"></a><a class="docs-heading-anchor-permalink" href="#What-You-Will-Learn" title="Permalink"></a></h2><p>By the end of this tutorial, you will be able to:</p><ul><li><strong>Build</strong> a nonlinear mixed-effects model with lognormal random effects on a growth asymptote.</li><li><strong>Configure</strong> four fundamentally different estimation strategies – Laplace approximation, MCEM, SAEM, and full Bayesian MCMC – with sensible defaults.</li><li><strong>Compare</strong> methods in predictive space using NoLimits&#39; diagnostic and visualization tools, rather than relying solely on objective function values.</li><li><strong>Interpret</strong> where the estimators converge, where they diverge, and what each method uniquely provides.</li></ul><p>The goal is not just to run four fits, but to build understanding of the trade-offs involved in choosing an estimation strategy for your own longitudinal analyses.</p><h2 id="Step-1:-Data-Setup"><a class="docs-heading-anchor" href="#Step-1:-Data-Setup">Step 1: Data Setup</a><a id="Step-1:-Data-Setup-1"></a><a class="docs-heading-anchor-permalink" href="#Step-1:-Data-Setup" title="Permalink"></a></h2><p>In this first step, you will load the Orange tree growth dataset, a classic longitudinal dataset originally from Draper and Smith (1981) and available in R&#39;s <code>datasets</code> package. The dataset records the trunk circumference of five orange trees measured at seven time points over approximately four years. Although small, it is representative of a broad class of problems in biology: repeated measurements of a continuous outcome on a set of individuals, with growth that follows a saturating nonlinear trajectory. The between-tree variation in final size makes it a natural candidate for random-effects modeling.</p><p>The code below loads the required packages and retrieves the data directly from the Rdatasets GitHub repository.</p><pre><code class="language-julia hljs">using NoLimits
using CSV
using DataFrames
using Distributions
using Downloads
using Random
using SciMLBase
using Turing

include(joinpath(@__DIR__, &quot;_data_loaders.jl&quot;))

Random.seed!(42)

df = load_orange()

first(df, 8)</code></pre><h2 id="Step-2:-Define-the-Nonlinear-Mixed-Effects-Model"><a class="docs-heading-anchor" href="#Step-2:-Define-the-Nonlinear-Mixed-Effects-Model">Step 2: Define the Nonlinear Mixed-Effects Model</a><a id="Step-2:-Define-the-Nonlinear-Mixed-Effects-Model-1"></a><a class="docs-heading-anchor-permalink" href="#Step-2:-Define-the-Nonlinear-Mixed-Effects-Model" title="Permalink"></a></h2><p>Next, you will specify the statistical model. The biological reasoning is straightforward: each tree&#39;s circumference follows a saturating growth curve, increasing from an initial size toward a tree-specific maximum. You will model this maximum (the asymptote) as a random effect, allowing each tree to have its own upper bound while sharing a common growth shape across the population.</p><p>Concretely, the model uses a logistic-style saturating function with three population-level parameters: an initial size <code>phi1</code>, a log-scale population mean for the asymptote <code>log_vmax</code>, and a midpoint parameter <code>phi3</code> that controls the timing of the growth inflection. Each tree&#39;s individual asymptote <code>vmax_i</code> is drawn from a lognormal distribution, which ensures positivity and places between-tree variability on a multiplicative scale – a natural choice when larger individuals tend to show proportionally larger variation. The observation model is also lognormal, so residual variability scales with the predicted circumference rather than being additive. To maintain numerical stability during optimization, the predicted mean is passed through a <code>softplus</code> function that enforces positivity without introducing hard discontinuities.</p><p>All fixed effects are given weakly informative priors. These priors are not strictly necessary for the optimization-based methods (Laplace, MCEM, SAEM), but they are required for MCMC and serve to regularize the likelihood surface for all methods.</p><pre><code class="language-julia hljs">using NoLimits
using Distributions

model = @Model begin
    @helpers begin
        softplus(u) = u &gt; 20 ? u : log1p(exp(u))
    end

    @covariates begin
        age = Covariate()
    end

    @fixedEffects begin
        phi1 = RealNumber(30.0, prior=LogNormal(log(30.0), 0.5), calculate_se=true)
        log_vmax = RealNumber(log(190.0), prior=Normal(log(190.0), 0.5), calculate_se=true)
        phi3 = RealNumber(700.0, prior=LogNormal(log(700.0), 0.5), calculate_se=true)
        omega = RealNumber(1.0, scale=:log, prior=LogNormal(log(1.0), 0.5), calculate_se=true)
        sigma = RealNumber(6.0, scale=:log, prior=LogNormal(log(6.0), 0.5), calculate_se=true)
    end

    @randomEffects begin
        vmax_i = RandomEffect(LogNormal(log_vmax, omega); column=:Tree)
    end

    @formulas begin
        mu_raw = phi1 + (vmax_i - phi1) / (1 + exp(-(age - phi3) / 100))
        mu = softplus(mu_raw) + 1e-6
        circumference ~ LogNormal(log(mu), sigma)
    end
end</code></pre><h3 id="Model-Summary"><a class="docs-heading-anchor" href="#Model-Summary">Model Summary</a><a id="Model-Summary-1"></a><a class="docs-heading-anchor-permalink" href="#Model-Summary" title="Permalink"></a></h3><p>You can inspect the model structure to verify that all blocks were parsed correctly and that the parameter dimensions, scales, and priors match what you intended.</p><pre><code class="language-julia hljs">model_summary = NoLimits.summarize(model)
model_summary</code></pre><h2 id="Step-3:-Build-the-DataModel-and-Configure-Estimation-Methods"><a class="docs-heading-anchor" href="#Step-3:-Build-the-DataModel-and-Configure-Estimation-Methods">Step 3: Build the DataModel and Configure Estimation Methods</a><a id="Step-3:-Build-the-DataModel-and-Configure-Estimation-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Step-3:-Build-the-DataModel-and-Configure-Estimation-Methods" title="Permalink"></a></h2><p>In this step, you will wrap the model and data together into a <code>DataModel</code> – a structure that validates the data schema, groups individuals into batches for the random effects structure, and prepares internal representations for each estimation method.</p><p>You will then configure four estimation methods. Each represents a fundamentally different strategy for handling the random effects integral that appears in the marginal likelihood:</p><ul><li><p><strong>Laplace</strong> approximates the integral analytically using a second-order Taylor expansion around each individual&#39;s best estimate of the random effects (the empirical Bayes estimate). It is fast and deterministic, making it a good default for moderate-sized problems. However, the approximation can lose accuracy when the true distribution of random effects is far from Gaussian.</p></li><li><p><strong>MCEM</strong> (Monte Carlo Expectation-Maximization) uses MCMC sampling within each iteration to approximate the expected complete-data log-likelihood, then maximizes that approximation. In plain terms, it alternates between &quot;filling in&quot; the missing random effects via sampling and updating the population parameters given those samples. It is more robust than Laplace to non-Gaussian random effects but requires more computation.</p></li><li><p><strong>SAEM</strong> (Stochastic Approximation EM) follows a similar alternating logic but replaces the full sampling step with a stochastic approximation that updates a running average of sufficient statistics. This means it converges with fewer samples per iteration than MCEM, making it attractive for larger problems, though its stochastic nature can make convergence harder to diagnose.</p></li><li><p><strong>MCMC</strong> (Markov chain Monte Carlo) samples the full joint posterior over both fixed and random effects. Rather than returning a single &quot;best&quot; parameter estimate, it produces a collection of plausible parameter sets that together characterize uncertainty. This provides the richest picture of parameter uncertainty – including asymmetric or multimodal posteriors – but is the most computationally expensive and requires careful convergence assessment.</p></li></ul><p>The configuration values below are chosen to balance runtime and stability for this tutorial; in a research setting, you would typically increase iteration counts, sample sizes, and warmup periods.</p><pre><code class="language-julia hljs">dm = DataModel(model, df; primary_id=:Tree, time_col=:age)

laplace_method = NoLimits.Laplace(; multistart_n=0, multistart_k=0, optim_kwargs=(maxiters=120,))

mcem_method = NoLimits.MCEM(;
    maxiters=6,
    sample_schedule=i -&gt; min(40 + 20 * (i - 1), 140),
    turing_kwargs=(n_samples=40, n_adapt=15, progress=false),
    optim_kwargs=(maxiters=120,),
    progress=false,
)

saem_method = NoLimits.SAEM(;
    maxiters=80,
    mcmc_steps=16,
    t0=15,
    kappa=0.65,
    turing_kwargs=(n_adapt=20, progress=false),
    optim_kwargs=(maxiters=160,),
    verbose=false,
    progress=false,
)

mcmc_method = NoLimits.MCMC(;
    sampler=NUTS(0.75),
    progress=false,
    turing_kwargs=(n_samples=1000, n_adapt=500, progress=false),
)

serialization = SciMLBase.EnsembleThreads()</code></pre><h3 id="DataModel-Summary"><a class="docs-heading-anchor" href="#DataModel-Summary">DataModel Summary</a><a id="DataModel-Summary-1"></a><a class="docs-heading-anchor-permalink" href="#DataModel-Summary" title="Permalink"></a></h3><p>Before proceeding to estimation, inspect the DataModel summary to confirm that individuals, covariates, and random effect groupings were detected correctly.</p><pre><code class="language-julia hljs">dm_summary = NoLimits.summarize(dm)
dm_summary</code></pre><h2 id="Step-4:-Fit-All-Methods"><a class="docs-heading-anchor" href="#Step-4:-Fit-All-Methods">Step 4: Fit All Methods</a><a id="Step-4:-Fit-All-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Step-4:-Fit-All-Methods" title="Permalink"></a></h2><p>With the model, data, and methods configured, you will now fit the same DataModel with all four estimators. Each call to <code>fit_model</code> returns a <code>FitResult</code> object that stores the estimated parameters, convergence diagnostics, and a reference to the DataModel for downstream analysis.</p><p>Each method receives a different random seed to ensure reproducibility while allowing independent stochastic behavior across methods.</p><pre><code class="language-julia hljs">res_laplace = fit_model(dm, laplace_method; serialization=serialization, rng=Random.Xoshiro(11))
res_mcem = fit_model(dm, mcem_method; serialization=serialization, rng=Random.Xoshiro(12))
res_saem = fit_model(dm, saem_method; serialization=serialization, rng=Random.Xoshiro(13))
res_mcmc = fit_model(dm, mcmc_method; serialization=serialization, rng=Random.Xoshiro(14))
</code></pre><h3 id="FitResult-Summaries"><a class="docs-heading-anchor" href="#FitResult-Summaries">FitResult Summaries</a><a id="FitResult-Summaries-1"></a><a class="docs-heading-anchor-permalink" href="#FitResult-Summaries" title="Permalink"></a></h3><p>Each fit result can be summarized to display estimated parameter values, convergence status, and method-specific diagnostics. Reviewing these summaries side by side is a quick first check for whether the methods have arrived at broadly similar parameter estimates.</p><pre><code class="language-julia hljs">fit_summary_laplace = NoLimits.summarize(res_laplace)
fit_summary_mcem = NoLimits.summarize(res_mcem)
fit_summary_saem = NoLimits.summarize(res_saem)
fit_summary_mcmc = NoLimits.summarize(res_mcmc)

fit_summary_laplace</code></pre><h2 id="Step-5:-Compare-Objective-Values-(Laplace,-MCEM,-SAEM)"><a class="docs-heading-anchor" href="#Step-5:-Compare-Objective-Values-(Laplace,-MCEM,-SAEM)">Step 5: Compare Objective Values (Laplace, MCEM, SAEM)</a><a id="Step-5:-Compare-Objective-Values-(Laplace,-MCEM,-SAEM)-1"></a><a class="docs-heading-anchor-permalink" href="#Step-5:-Compare-Objective-Values-(Laplace,-MCEM,-SAEM)" title="Permalink"></a></h2><p>It is tempting to compare objective function values across methods, but this requires care: each method optimizes a different quantity, so raw values are not directly comparable.</p><pre><code class="language-julia hljs">objectives = (
    laplace=NoLimits.get_objective(res_laplace),
    mcem=NoLimits.get_objective(res_mcem),
    saem=NoLimits.get_objective(res_saem),
)

objectives</code></pre><p>The signs and magnitudes differ because each method defines its objective differently:</p><ul><li><strong>Laplace</strong> reports the minimized value of the Laplace-approximated marginal likelihood (a loss function, so lower is better).</li><li><strong>MCEM</strong> and <strong>SAEM</strong> report the EM auxiliary quantity <code>Q</code> at the final iterate, which is a lower bound on the log-likelihood (higher is better).</li></ul><p>Because these quantities differ by construction, a positive Laplace value and negative MCEM/SAEM values are not contradictory. Within a single method family, however, objective values can be compared meaningfully – for example, when evaluating different starting points or model specifications with the same estimator.</p><h2 id="Step-6:-Fitted-Trajectories-for-the-First-Two-Individuals"><a class="docs-heading-anchor" href="#Step-6:-Fitted-Trajectories-for-the-First-Two-Individuals">Step 6: Fitted Trajectories for the First Two Individuals</a><a id="Step-6:-Fitted-Trajectories-for-the-First-Two-Individuals-1"></a><a class="docs-heading-anchor-permalink" href="#Step-6:-Fitted-Trajectories-for-the-First-Two-Individuals" title="Permalink"></a></h2><p>The most informative way to compare methods is in predictive space: do the fitted trajectories agree when overlaid on the observed data? In this step, you will generate fit plots for the first two trees under each method. For MCMC, you will additionally overlay posterior predictive quantile bands (5th and 95th percentiles), which provide a visual summary of prediction uncertainty that accounts for both parameter and random-effect uncertainty.</p><pre><code class="language-julia hljs">inds = collect(1:min(2, length(dm.individuals)))

p_fit_laplace = plot_fits(
    res_laplace;
    observable=:circumference,
    individuals_idx=inds,
    ncols=2,
    shared_x_axis=true,
    shared_y_axis=true,
)

p_fit_mcem = plot_fits(
    res_mcem;
    observable=:circumference,
    individuals_idx=inds,
    ncols=2,
    shared_x_axis=true,
    shared_y_axis=true,
)

p_fit_saem = plot_fits(
    res_saem;
    observable=:circumference,
    individuals_idx=inds,
    ncols=2,
    shared_x_axis=true,
    shared_y_axis=true,
)

p_fit_mcmc = plot_fits(
    res_mcmc;
    observable=:circumference,
    individuals_idx=inds,
    ncols=2,
    shared_x_axis=true,
    shared_y_axis=true,
    plot_mcmc_quantiles=true,
    mcmc_quantiles=[5, 95],
    mcmc_warmup=500,
    mcmc_draws=300,
)
</code></pre><p>When all four methods are well-calibrated, you should see broadly similar trajectories. Differences, when they appear, tend to be most visible in the tails of the growth curve where data are sparse.</p><p>Laplace fit plot:</p><pre><code class="language-julia hljs">p_fit_laplace</code></pre><p>MCEM fit plot:</p><pre><code class="language-julia hljs">p_fit_mcem</code></pre><p>SAEM fit plot:</p><pre><code class="language-julia hljs">p_fit_saem</code></pre><p>MCMC fit plot (with posterior predictive bands):</p><pre><code class="language-julia hljs">p_fit_mcmc</code></pre><h2 id="Step-7:-Observation-Distribution-Diagnostics-(First-Individual)"><a class="docs-heading-anchor" href="#Step-7:-Observation-Distribution-Diagnostics-(First-Individual)">Step 7: Observation Distribution Diagnostics (First Individual)</a><a id="Step-7:-Observation-Distribution-Diagnostics-(First-Individual)-1"></a><a class="docs-heading-anchor-permalink" href="#Step-7:-Observation-Distribution-Diagnostics-(First-Individual)" title="Permalink"></a></h2><p>Beyond trajectory-level agreement, you can examine how well each method captures the observation-level distribution. The plots below compare the observed circumference value for the first observation of the first tree against the model-implied observation distribution at that data point. If the model is well-specified, the observed value should fall in a region of reasonable density under the predicted distribution.</p><p>This diagnostic is particularly useful for detecting model misspecification. If the observed value consistently falls in the tail of the predicted distribution across individuals and methods, the observation model (here, lognormal) may need revision.</p><pre><code class="language-julia hljs">p_obs_laplace = plot_observation_distributions(
    res_laplace;
    observables=:circumference,
    individuals_idx=1,
    obs_rows=1,
)

p_obs_mcem = plot_observation_distributions(
    res_mcem;
    observables=:circumference,
    individuals_idx=1,
    obs_rows=1,
)

p_obs_saem = plot_observation_distributions(
    res_saem;
    observables=:circumference,
    individuals_idx=1,
    obs_rows=1,
)

p_obs_mcmc = plot_observation_distributions(
    res_mcmc;
    observables=:circumference,
    individuals_idx=1,
    obs_rows=1,
    mcmc_warmup=500,
    mcmc_draws=300,
)
</code></pre><p>Laplace observation distribution:</p><pre><code class="language-julia hljs">p_obs_laplace</code></pre><p>MCEM observation distribution:</p><pre><code class="language-julia hljs">p_obs_mcem</code></pre><p>SAEM observation distribution:</p><pre><code class="language-julia hljs">p_obs_saem</code></pre><p>MCMC observation distribution:</p><pre><code class="language-julia hljs">p_obs_mcmc</code></pre><h2 id="Step-8:-Uncertainty-Quantification-Across-Methods"><a class="docs-heading-anchor" href="#Step-8:-Uncertainty-Quantification-Across-Methods">Step 8: Uncertainty Quantification Across Methods</a><a id="Step-8:-Uncertainty-Quantification-Across-Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Step-8:-Uncertainty-Quantification-Across-Methods" title="Permalink"></a></h2><p>A key reason to compare methods is to understand how they characterize parameter uncertainty. The optimization-based methods (Laplace, MCEM, SAEM) return point estimates; you can obtain approximate uncertainty via Wald-type confidence intervals, which are derived from the curvature of the objective function at the optimum. Intuitively, a sharply peaked objective implies tight uncertainty, while a flat objective implies wide intervals. MCMC, by contrast, directly produces samples from the posterior distribution, giving a richer and often more accurate picture of uncertainty – especially when the posterior is skewed or multimodal.</p><p>Below, you will compute uncertainty quantification (UQ) summaries for all four methods and generate density plots of the resulting parameter distributions on the natural (untransformed) scale.</p><pre><code class="language-julia hljs">uq_laplace = compute_uq(
    res_laplace;
    method=:wald,
    vcov=:hessian,
    pseudo_inverse=true,
    serialization=serialization,
    n_draws=400,
    rng=Random.Xoshiro(101),
)
uq_mcem = compute_uq(
    res_mcem;
    method=:wald,
    vcov=:hessian,
    re_approx=:laplace,
    pseudo_inverse=true,
    serialization=serialization,
    n_draws=400,
    rng=Random.Xoshiro(102),
)
uq_saem = compute_uq(
    res_saem;
    method=:wald,
    vcov=:hessian,
    re_approx=:laplace,
    pseudo_inverse=true,
    serialization=serialization,
    n_draws=400,
    rng=Random.Xoshiro(103),
)
uq_mcmc = compute_uq(
    res_mcmc;
    method=:chain,
    serialization=serialization,
    mcmc_warmup=500,
    mcmc_draws=300,
    rng=Random.Xoshiro(104),
)

p_uq_laplace = plot_uq_distributions(uq_laplace; scale=:natural, plot_type=:density, show_legend=false)
p_uq_mcem = plot_uq_distributions(uq_mcem; scale=:natural, plot_type=:density, show_legend=false)
p_uq_saem = plot_uq_distributions(uq_saem; scale=:natural, plot_type=:density, show_legend=false)
p_uq_mcmc = plot_uq_distributions(uq_mcmc; scale=:natural, plot_type=:density, show_legend=false)
</code></pre><h3 id="UQ-Summaries"><a class="docs-heading-anchor" href="#UQ-Summaries">UQ Summaries</a><a id="UQ-Summaries-1"></a><a class="docs-heading-anchor-permalink" href="#UQ-Summaries" title="Permalink"></a></h3><p>The summaries below report point estimates alongside uncertainty intervals for each parameter. Where the methods agree, you can be confident that inference is robust. Where they diverge, the discrepancy may signal sensitivity to the estimation strategy, an under-identified parameter, or simply a need for more data.</p><pre><code class="language-julia hljs">uq_summary_laplace = NoLimits.summarize(uq_laplace)
uq_summary_mcem = NoLimits.summarize(uq_mcem)
uq_summary_saem = NoLimits.summarize(uq_saem)
uq_summary_mcmc = NoLimits.summarize(uq_mcmc)

fit_uq_summary_laplace = NoLimits.summarize(res_laplace, uq_laplace)
fit_uq_summary_mcem = NoLimits.summarize(res_mcem, uq_mcem)
fit_uq_summary_saem = NoLimits.summarize(res_saem, uq_saem)
fit_uq_summary_mcmc = NoLimits.summarize(res_mcmc, uq_mcmc)

fit_uq_summary_laplace</code></pre><p>Laplace UQ distribution:</p><pre><code class="language-julia hljs">p_uq_laplace</code></pre><p>MCEM UQ distribution:</p><pre><code class="language-julia hljs">p_uq_mcem</code></pre><p>SAEM UQ distribution:</p><pre><code class="language-julia hljs">p_uq_saem</code></pre><p>MCMC UQ distribution:</p><pre><code class="language-julia hljs">p_uq_mcmc</code></pre><h2 id="Interpretation-and-Practical-Guidance"><a class="docs-heading-anchor" href="#Interpretation-and-Practical-Guidance">Interpretation and Practical Guidance</a><a id="Interpretation-and-Practical-Guidance-1"></a><a class="docs-heading-anchor-permalink" href="#Interpretation-and-Practical-Guidance" title="Permalink"></a></h2><p>Several principles emerge from this multi-method comparison:</p><p><strong>Evaluate agreement in predictive space, not objective space.</strong> Because each method optimizes a different quantity, comparing raw objective values across methods is misleading. Instead, compare fitted trajectories, observation-level distributions, and uncertainty intervals. When methods agree in predictive space, you can be more confident that your conclusions are robust to the choice of estimator.</p><p><strong>Method agreement on central structure is the norm for well-specified models.</strong> For a dataset like Orange, where the model is a reasonable description of the data-generating process, Laplace, MCEM, and SAEM will typically recover similar point estimates and trajectory shapes. Disagreement is a useful diagnostic signal – it may indicate model misspecification, insufficient iterations, or a challenging likelihood surface.</p><p><strong>MCMC provides the richest uncertainty characterization.</strong> Posterior predictive bands and full marginal posterior distributions capture asymmetry, multimodality, and correlation structure that Wald-based intervals cannot. When computational budget allows, MCMC is the gold standard for uncertainty quantification. Increasing the number of samples and warmup iterations will generally stabilize the posterior summaries.</p><p><strong>Choose your method based on your inferential goal.</strong> If you need fast point estimates with approximate standard errors for model selection or screening, Laplace is often sufficient. If you need robust estimates under flexible random-effect distributions, SAEM or MCEM may be preferable. If full posterior inference is the goal – for example, for decision-making under uncertainty or for propagating parameter uncertainty into downstream predictions – MCMC is the appropriate choice.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../plotting/">« Plotting</a><a class="docs-footer-nextpage" href="../mixed-effects-ode-mcem/">Mixed-Effects Tutorial 2: ODE Model with Dosing Events (MCEM) »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Friday 20 February 2026 17:04">Friday 20 February 2026</span>. Using Julia version 1.12.5.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
