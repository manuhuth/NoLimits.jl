var documenterSearchIndex = {"docs":
[{"location":"estimation/laplace/#Laplace","page":"Laplace","title":"Laplace","text":"The Laplace approximation is a widely used technique for integrating out random effects in nonlinear mixed-effects models. Rather than evaluating the marginal likelihood integral exactly – which is intractable for nonlinear models – the Laplace method approximates it via a second-order Taylor expansion around the empirical Bayes (EB) mode of each individual's random-effects vector. The resulting closed-form approximation can be optimized over the fixed effects using standard gradient-based methods, combining computational efficiency with support for complex nonlinear model structures.","category":"section"},{"location":"estimation/laplace/#Applicability","page":"Laplace","title":"Applicability","text":"Requires a model with random effects.\nRequires at least one free fixed effect.\nSupports nonlinear models, including ODE-based models.\n\nIf the model has no random effects, Laplace will raise an error. Use a fixed-effects method such as MLE or MAP instead.","category":"section"},{"location":"estimation/laplace/#Basic-Usage","page":"Laplace","title":"Basic Usage","text":"The following example fits a simple nonlinear mixed-effects model with a single subject-level random effect.\n\nusing NoLimits\nusing DataFrames\nusing Distributions\n\nmodel = @Model begin\n    @fixedEffects begin\n        a = RealNumber(0.2)\n        b = RealNumber(0.1)\n        sigma = RealNumber(0.3, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n    end\n\n    @randomEffects begin\n        eta_id = RandomEffect(TDist(6.0); column=:ID)\n    end\n\n    @formulas begin\n        mu = a + b * t + exp(eta_id)   # nonlinear in random effects\n        y ~ Normal(mu, sigma)\n    end\nend\n\ndf = DataFrame(\n    ID = [:A, :A, :B, :B, :C, :C],\n    t = [0.0, 1.0, 0.0, 1.0, 0.0, 1.0],\n    y = [1.0, 1.3, 0.9, 1.2, 1.1, 1.5],\n)\n\ndm = DataModel(model, df; primary_id=:ID, time_col=:t)\n\nres = fit_model(dm, NoLimits.Laplace(; optim_kwargs=(maxiters=100,)))","category":"section"},{"location":"estimation/laplace/#Constructor-Options","page":"Laplace","title":"Constructor Options","text":"The Laplace constructor exposes options that control the outer fixed-effects optimization, the inner EB optimization, Hessian stabilization, and the computational strategy for log-determinant gradients. Most users will only need to adjust a few of these; the defaults are chosen to work well across a range of model types.\n\nusing Optimization\nusing OptimizationOptimJL\nusing LineSearches\n\nlaplace_method = NoLimits.Laplace(;\n    optimizer=OptimizationOptimJL.LBFGS(linesearch=LineSearches.BackTracking()),\n    optim_kwargs=NamedTuple(),\n    adtype=Optimization.AutoForwardDiff(),\n    inner_optimizer=OptimizationOptimJL.LBFGS(linesearch=LineSearches.BackTracking()),\n    inner_kwargs=NamedTuple(),\n    inner_adtype=Optimization.AutoForwardDiff(),\n    inner_grad_tol=:auto,\n    multistart_n=50,\n    multistart_k=10,\n    multistart_grad_tol=:auto,\n    multistart_max_rounds=1,\n    multistart_sampling=:lhs,\n    jitter=1e-6,\n    max_tries=6,\n    jitter_growth=10.0,\n    adaptive_jitter=true,\n    jitter_scale=1e-6,\n    use_trace_logdet_grad=true,\n    use_hutchinson=false,\n    hutchinson_n=8,\n    theta_tol=0.0,\n    lb=nothing,\n    ub=nothing,\n)\n\nNotes:\n\ninner_grad_tol=:auto uses method-specific defaults (1e-8 for non-ODE, 1e-2 for ODE paths).\nuse_trace_logdet_grad=true is the default gradient path for the log-determinant term.\nuse_hutchinson=true activates the stochastic Hutchinson approximation for logdet-related terms.\nlb/ub are bounds on transformed fixed-effect parameters.","category":"section"},{"location":"estimation/laplace/#Option-Groups","page":"Laplace","title":"Option Groups","text":"The constructor keywords fall into several logical groups, summarized in the table below.\n\nGroup Keywords What they control\nOuter optimization optimizer, optim_kwargs, adtype Optimization over fixed effects.\nInner EB optimization inner_optimizer, inner_kwargs, inner_adtype, inner_grad_tol Optimization of batch-level EB modes used by Laplace objective/gradient evaluation.\nEB multistart multistart_n, multistart_k, multistart_grad_tol, multistart_max_rounds, multistart_sampling Number/selection of initial points and restart policy for EB optimization.\nHessian stabilization jitter, max_tries, jitter_growth, adaptive_jitter, jitter_scale Cholesky stabilization for -H in log-determinant calculations.\nLogdet gradient strategy use_trace_logdet_grad, use_hutchinson, hutchinson_n Computational path for logdet-related derivatives.\nCaching theta_tol Reuse tolerance for objective/gradient cache across nearby fixed-effect values.\nBounds lb, ub Optional transformed-scale bounds for free fixed effects.","category":"section"},{"location":"estimation/laplace/#Inner-vs-Outer-Optimizer-Choices-(Optimization.jl-Interface)","page":"Laplace","title":"Inner vs Outer Optimizer Choices (Optimization.jl Interface)","text":"Laplace uses a two-level (nested) optimization scheme. Both levels dispatch through the Optimization.jl interface:\n\nOuter layer: optimizes the fixed effects (theta) to maximize the Laplace-approximated marginal log-likelihood.\nInner layer: for each outer iteration, finds the EB mode (b*) for each batch of random effects.\n\nPractical implications:\n\nOuter optimizer (optimizer, optim_kwargs, adtype)\nRuns once at the top level.\nCan use local gradient methods (default LBFGS) or global/derivative-free methods.\nIf using BlackBoxOptim (OptimizationBBO.*), finite bounds are required.\nInner optimizer (inner_optimizer, inner_kwargs, inner_adtype, inner_grad_tol)\nRuns repeatedly across batches and across outer iterations.\nShould typically be a fast local optimizer; defaults are chosen for that path.\nTighter inner tolerances can improve objective/gradient quality but increase runtime.\n\nRepository-verified behavior:\n\nDefault outer/inner optimizers are OptimizationOptimJL.LBFGS(...).\nOuter BlackBoxOptim is supported with finite bounds (lb, ub); without bounds, an error is raised.\n\nExamples:\n\nusing NoLimits\nusing OptimizationOptimJL\nusing OptimizationBBO\nusing LineSearches\n\n# 1) Local-gradient outer + local-gradient inner (default style)\nlaplace_local = NoLimits.Laplace(;\n    optimizer=OptimizationOptimJL.LBFGS(linesearch=LineSearches.BackTracking()),\n    inner_optimizer=OptimizationOptimJL.LBFGS(linesearch=LineSearches.BackTracking()),\n)\n\n# 2) Global outer search + local inner EB solves\n#    (requires finite transformed-scale bounds for free fixed effects)\nlb, ub = default_bounds_from_start(dm; margin=1.0)\nlaplace_global_outer = NoLimits.Laplace(;\n    optimizer=OptimizationBBO.BBO_adaptive_de_rand_1_bin_radiuslimited(),\n    optim_kwargs=(maxiters=80,),\n    lb=lb,\n    ub=ub,\n    inner_optimizer=OptimizationOptimJL.LBFGS(linesearch=LineSearches.BackTracking()),\n    inner_kwargs=(maxiters=40,),\n)","category":"section"},{"location":"estimation/laplace/#Detailed-Behavior","page":"Laplace","title":"Detailed Behavior","text":"This section provides additional detail on each option group.\n\ninner_grad_tol\n:auto resolves to 1e-8 for non-ODE models and 1e-2 for ODE models.\nmultistart_sampling\nSupported values are :lhs (Latin hypercube sampling) and :random.\nmultistart_n, multistart_k\nControl the size of the candidate start pool and the number of starts selected for EB multistart optimization.\njitter, max_tries, jitter_growth\nIf the Cholesky factorization of -H fails, it is retried with increasing diagonal jitter (jitter * jitter_growth^attempt).\nadaptive_jitter, jitter_scale\nWhen adaptive_jitter=true, the initial jitter is scaled by the Hessian diagonal magnitude (jitter_scale * mean(abs(diag(-H)))), then bounded below by jitter.\nuse_trace_logdet_grad\nUses trace-based derivatives of logdet terms (default path).\nuse_hutchinson, hutchinson_n\nUses stochastic Hutchinson estimation for logdet derivative terms with hutchinson_n probe vectors.\nWhen use_hutchinson=true, this path is used instead of the exact logdet gradient.\ntheta_tol\nCache tolerance for reusing objective/gradient values at nearby parameter vectors.\n0.0 means reuse only for effectively identical vectors.\nlb, ub\nBounds are interpreted on transformed parameters and are applied only to free fixed effects.\nIf constants are set via constants, bounds for those constant parameters are ignored.","category":"section"},{"location":"estimation/laplace/#Advanced-Option-Containers","page":"Laplace","title":"Advanced Option Containers","text":"For more granular control, Laplace also accepts structured option containers that replace the corresponding keyword groups:\n\ninner_options\nhessian_options\ncache_options\nmultistart_options\n\nWhen one of these is provided, it replaces the corresponding scalar keyword bundle in that option group.","category":"section"},{"location":"estimation/laplace/#Tuning-Examples","page":"Laplace","title":"Tuning Examples","text":"The examples below illustrate common tuning strategies for different use cases.\n\n# Fast exploratory run\nlaplace_fast = NoLimits.Laplace(;\n    optim_kwargs=(maxiters=60,),\n    multistart_n=0,\n    multistart_k=0,\n)\n\n# More robust EB search\nlaplace_robust = NoLimits.Laplace(;\n    inner_grad_tol=1e-4,\n    multistart_n=120,\n    multistart_k=24,\n    multistart_max_rounds=3,\n    multistart_sampling=:lhs,\n)\n\n# Stochastic logdet-gradient path\nlaplace_hutch = NoLimits.Laplace(;\n    use_hutchinson=true,\n    hutchinson_n=16,\n)","category":"section"},{"location":"estimation/laplace/#Fixing-Known-Random-Effect-Levels-(constants_re)","page":"Laplace","title":"Fixing Known Random-Effect Levels (constants_re)","text":"In some settings, certain random-effect levels are known a priori – for example, a reference group set to zero or a level estimated in a previous analysis. Laplace supports fixing selected random-effect levels to specified values while estimating the remaining levels via EB.\n\nconstants_re = (; eta_id=(; A=0.0))\n\nres_fixed = fit_model(\n    dm,\n    NoLimits.Laplace(; optim_kwargs=(maxiters=100,));\n    constants_re=constants_re,\n)\n\nValidation rules for constants_re:\n\nKeys must match random-effect names declared in @randomEffects.\nLevel names must exist in the corresponding grouping column.\nValues must have the correct dimension (scalar for univariate RE; vector for multivariate RE).","category":"section"},{"location":"estimation/laplace/#Accessing-Results","page":"Laplace","title":"Accessing Results","text":"After fitting, results are accessed through the standard accessor interface.\n\ntheta_u = get_params(res; scale=:untransformed)\ntheta_t = get_params(res; scale=:transformed)\nobj = get_objective(res)\nok = get_converged(res)\n\nre_df = get_random_effects(res)\nre_df_laplace = get_laplace_random_effects(res; flatten=true, include_constants=true)\n\nll = get_loglikelihood(res)\n\nget_random_effects and get_laplace_random_effects each return one DataFrame per random effect, with rows corresponding to the levels of the grouping column (e.g., one row per individual).","category":"section"},{"location":"estimation/laplace/#Serialization","page":"Laplace","title":"Serialization","text":"Laplace supports serial and threaded evaluation of individual- or batch-level computations through the serialization keyword:\n\nusing SciMLBase\n\nres_serial = fit_model(dm, NoLimits.Laplace(); serialization=EnsembleSerial())\nres_threads = fit_model(dm, NoLimits.Laplace(); serialization=EnsembleThreads())","category":"section"},{"location":"estimation/laplace/#Bounds-and-BlackBoxOptim","page":"Laplace","title":"Bounds and BlackBoxOptim","text":"When using a derivative-free global optimizer from BlackBoxOptim, finite bounds on all free fixed effects are required. A convenience function generates default bounds from the initial parameter values:\n\nlb, ub = default_bounds_from_start(dm; margin=1.0)\n\nThese bounds are then passed directly to the Laplace constructor via lb and ub.","category":"section"},{"location":"model-building/fixed-effects/#@fixedEffects","page":"@fixedEffects","title":"@fixedEffects","text":"Fixed effects are the parameters shared across all individuals in a dataset. The @fixedEffects block declares these parameters together with their initial values, scales, bounds, priors, and standard-error flags. The resulting objects are referenced throughout the model: in formulas, differential equations, pre-ODE computations, and random-effect distributions.\n\nIn nonlinear mixed-effects (NLME) models, fixed effects represent population-level quantities. In fixed-effects-only models, they constitute the full set of estimands.","category":"section"},{"location":"model-building/fixed-effects/#Syntax","page":"@fixedEffects","title":"Syntax","text":"Each statement in @fixedEffects is an assignment whose left-hand side is a symbol and whose right-hand side is a parameter constructor call. The parameter name is injected automatically from the left-hand side unless an explicit name= keyword is provided.\n\nfe = @fixedEffects begin\n    ka = RealNumber(1.0, scale=:log)\n    beta = RealVector([0.2, -0.1], scale=[:identity, :identity])\nend","category":"section"},{"location":"model-building/fixed-effects/#Block-Rules","page":"@fixedEffects","title":"Block Rules","text":"The following rules are enforced at macro-expansion time:\n\nThe block must use begin ... end syntax.\nOnly assignment statements are permitted.\nThe left-hand side of each assignment must be a single symbol.\nThe right-hand side must be one of the supported parameter constructor calls listed below.\nAn empty block is valid and produces an empty fixed-effects object.","category":"section"},{"location":"model-building/fixed-effects/#Supported-Parameter-Types","page":"@fixedEffects","title":"Supported Parameter Types","text":"NoLimits provides parameter types for scalars, vectors, structured matrices, and learned function approximators. Each type controls how values are stored, transformed during optimization, and optionally regularized via priors.\n\nConstructor Purpose\nRealNumber(value; scale, lower, upper, prior, calculate_se) Scalar parameter (:identity, :log, or :logit scale)\nRealVector(value; scale, lower, upper, prior, calculate_se) Vector parameter with per-element scale (:identity, :log, :logit, or mixed)\nRealPSDMatrix(value; scale, prior, calculate_se) Symmetric positive semi-definite matrix (:cholesky or :expm)\nRealDiagonalMatrix(value; scale, prior, calculate_se) Diagonal matrix (:log scale on diagonal entries)\nProbabilityVector(value; scale, prior, calculate_se) Probability simplex vector of length k≥2 (:stickbreak scale)\nDiscreteTransitionMatrix(value; scale, prior, calculate_se) Square row-stochastic matrix n×n, n≥2 (:stickbreakrows scale)\nContinuousTransitionMatrix(value; scale, prior, calculate_se) Square rate matrix (Q-matrix) n×n, n≥2 (:lograterows scale)\nNNParameters(chain; function_name, seed, prior, calculate_se) Lux neural network weights\nSoftTreeParameters(input_dim, depth; function_name, n_output, seed, prior, calculate_se) Soft decision tree parameters\nSplineParameters(knots; function_name, degree, prior, calculate_se) B-spline coefficients\nNPFParameter(n_input, n_layers; seed, init, prior, calculate_se) Normalizing planar flow parameters","category":"section"},{"location":"model-building/fixed-effects/#Example:-Classical-Parameter-Blocks","page":"@fixedEffects","title":"Example: Classical Parameter Blocks","text":"The most common use case combines scalar, vector, and matrix parameters to define a population model. Priors can be assigned to any parameter for Bayesian estimation or regularization.\n\nusing NoLimits\nusing Distributions\nusing LinearAlgebra\n\nfe = @fixedEffects begin\n    ka = RealNumber(1.0, scale=:log, prior=LogNormal(0.0, 0.5), calculate_se=true)\n    beta = RealVector(\n        [0.2, -0.1],\n        scale=[:identity, :identity],\n        lower=[-Inf, -Inf],\n        upper=[Inf, Inf],\n        prior=MvNormal(zeros(2), Matrix(I, 2, 2)),\n        calculate_se=true,\n    )\n    Omega = RealPSDMatrix(Matrix(I, 2, 2), scale=:cholesky, prior=Wishart(3, Matrix(I, 2, 2)))\n    D = RealDiagonalMatrix([0.3, 0.2], calculate_se=true)\nend","category":"section"},{"location":"model-building/fixed-effects/#Transforms,-Bounds,-and-Priors","page":"@fixedEffects","title":"Transforms, Bounds, and Priors","text":"Internally, @fixedEffects constructs both transformed and untransformed parameter representations, along with associated bounds, priors, and standard-error masks. Accessor functions provide a uniform interface to these components.\n\ntheta_u = get_θ0_untransformed(fe)\ntheta_t = get_θ0_transformed(fe)\ntheta_u_rt = get_inverse_transform(fe)(theta_t)\n\nnames = get_names(fe)\nflat_names = get_flat_names(fe)\nse_names = get_se_names(fe)\npriors = get_priors(fe)\nlp = logprior(fe, theta_u)\n\nThe behaviour of each scale option is summarized below:\n\n:log scale applies an elementwise log transform and is supported for RealNumber, RealVector, and RealDiagonalMatrix. For inherently positive quantities such as standard deviations, :log enforces positivity in transformed space; an explicit lower bound is therefore optional.\n:logit scale applies the logit transform (log(x/(1-x)), clamped to [-20, 20]) and is supported for RealNumber and RealVector. Use this for parameters that must lie in (0, 1), such as probabilities. The inverse is the sigmoid function. The initial value must be strictly between 0 and 1; the constructor errors otherwise. Bounds are enforced implicitly via clamping — no explicit lower/upper are needed.\n:cholesky (for RealPSDMatrix) parameterizes the matrix via its Cholesky factor with log-transformed diagonal entries.\n:expm (for RealPSDMatrix) parameterizes the matrix via matrix logarithm/exponential, storing only the upper-triangular elements.\n:stickbreak (for ProbabilityVector) maps a k-probability simplex to k-1 unconstrained reals via the logistic stick-breaking transform. Each element νᵢ = pᵢ/(1-Σⱼ<ᵢ pⱼ) is passed through logit. The last probability is determined and not stored. Silently normalises the initial value if the sum is within 1e-6 of 1.\n:stickbreakrows (for DiscreteTransitionMatrix) applies the stick-breaking transform independently to each row of an n×n row-stochastic matrix, yielding n*(n-1) unconstrained parameters.\n:lograterows (for ContinuousTransitionMatrix) maps each off-diagonal entry of a rate matrix to its logarithm, yielding n*(n-1) unconstrained reals. The diagonal is always recomputed as minus the row sum and is never stored as a free parameter. Initial off-diagonal values must be non-negative.\n\nFor RealVector, scales can be mixed per element by passing a Vector{Symbol}, e.g. scale=[:logit, :log, :identity]. Mixed vectors use an elementwise dispatch that applies each element's transform independently.","category":"section"},{"location":"model-building/fixed-effects/#Example:-Learned-Function-Approximators","page":"@fixedEffects","title":"Example: Learned Function Approximators","text":"Neural networks, soft decision trees, and B-splines can be declared as fixed effects and are automatically exposed as callable model functions. This enables flexible, data-driven components within an otherwise parametric model specification.\n\nusing NoLimits\nusing Lux\n\nchain = Chain(Dense(2, 4, tanh), Dense(4, 1))\nknots = [0.0, 0.0, 0.0, 0.5, 1.0, 1.0, 1.0]\n\nfe_learned = @fixedEffects begin\n    z_nn = NNParameters(chain; function_name=:NN1, calculate_se=false)\n    z_st = SoftTreeParameters(2, 2; function_name=:ST1, calculate_se=false)\n    z_sp = SplineParameters(knots; function_name=:SP1, degree=2, calculate_se=false)\nend\n\nmodel_funs = get_model_funs(fe_learned)\nparams = get_params(fe_learned)\n\ny_nn = model_funs.NN1([0.4, 0.6], params.z_nn.value)[1]\ny_st = model_funs.ST1([0.4, 0.6], params.z_st.value)[1]\ny_sp = model_funs.SP1(0.5, params.z_sp.value)","category":"section"},{"location":"model-building/fixed-effects/#Example:-Constrained-Stochastic-Matrices","page":"@fixedEffects","title":"Example: Constrained Stochastic Matrices","text":"Three dedicated parameter types handle the structural constraints that arise in Hidden Markov Models and other latent-variable models:\n\nType Purpose Transform Free parameters\nProbabilityVector(value) Probability simplex of length k, k≥2 :stickbreak k-1\nDiscreteTransitionMatrix(value) n×n row-stochastic matrix, n≥2 :stickbreakrows n*(n-1)\nContinuousTransitionMatrix(value) n×n rate matrix (Q-matrix), n≥2 :lograterows n*(n-1)\n\nAll three types are AD-compatible and can be used anywhere in a model formula where the corresponding matrix or vector is expected. The values they provide in formulas are plain Julia arrays (Vector or Matrix), enabling direct indexing and arithmetic.\n\nThe first example uses ProbabilityVector for the initial state distribution and DiscreteTransitionMatrix for the row-stochastic transition matrix of a discrete-time two-state HMM:\n\nusing NoLimits\nusing Distributions\n\nmodel_disc = @Model begin\n    @fixedEffects begin\n        pi0   = ProbabilityVector([0.6, 0.4])\n        P     = DiscreteTransitionMatrix([0.9 0.1; 0.2 0.8])\n        mu1   = RealNumber(0.0)\n        mu2   = RealNumber(2.0)\n        sigma = RealNumber(0.5, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n    end\n\n    @formulas begin\n        y ~ DiscreteTimeDiscreteStatesHMM(\n            P,\n            (Normal(mu1, sigma), Normal(mu2, sigma)),\n            Categorical(pi0),\n        )\n    end\nend\n\nFor continuous-time transitions, ContinuousTransitionMatrix declares the full rate matrix. Off-diagonal entries must be non-negative; the diagonal is always derived as minus the row sum and never appears among the free parameters:\n\nusing NoLimits\nusing Distributions\n\nmodel_cont = @Model begin\n    @fixedEffects begin\n        Q     = ContinuousTransitionMatrix([-0.2 0.2; 0.3 -0.3])\n        mu1   = RealNumber(0.0)\n        mu2   = RealNumber(2.0)\n        sigma = RealNumber(0.5, scale=:log)\n    end\n\n    @covariates begin\n        t       = Covariate()\n        delta_t = Covariate()\n    end\n\n    @formulas begin\n        y ~ ContinuousTimeDiscreteStatesHMM(\n            Q,\n            (Normal(mu1, sigma), Normal(mu2, sigma)),\n            Categorical([0.5, 0.5]),\n            delta_t,\n        )\n    end\nend","category":"section"},{"location":"model-building/fixed-effects/#Example:-Normalizing-Flows-for-Flexible-Random-Effect-Distributions","page":"@fixedEffects","title":"Example: Normalizing Flows for Flexible Random-Effect Distributions","text":"Normalizing planar flows (NPFParameter) allow the random-effect distribution to depart from standard parametric families. The flow parameters are declared as fixed effects and referenced inside @randomEffects via NormalizingPlanarFlow. The corresponding model function is registered automatically.\n\nusing NoLimits\nusing Distributions\n\nmodel = @Model begin\n    @fixedEffects begin\n        psi = NPFParameter(1, 3, seed=1, calculate_se=false)\n        sigma = RealNumber(0.3, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n    end\n\n    @randomEffects begin\n        eta = RandomEffect(NormalizingPlanarFlow(psi); column=:ID)\n    end\n\n    @formulas begin\n        y ~ Normal(log1p(eta^2), sigma)\n    end\nend","category":"section"},{"location":"estimation/laplace-map/#Laplace-MAP","page":"Laplace MAP","title":"Laplace MAP","text":"LaplaceMAP extends the Laplace estimator by incorporating fixed-effect priors into the objective function, yielding maximum a posteriori (MAP) estimates of the fixed effects under the same Laplace approximation to the marginal likelihood.","category":"section"},{"location":"estimation/laplace-map/#Relationship-to-Laplace","page":"Laplace MAP","title":"Relationship to Laplace","text":"Both Laplace and LaplaceMAP integrate out random effects using a second-order (Laplace) approximation to the marginal likelihood. They differ only in the treatment of fixed effects:\n\nLaplace maximizes the Laplace-approximated marginal log-likelihood alone.\nLaplaceMAP adds the log-prior density of the fixed effects to that objective, performing MAP estimation.\n\nAll constructor options, inner/outer optimization settings, and Hessian stabilization strategies are shared between the two methods. See the Laplace documentation for a full description of available options.","category":"section"},{"location":"estimation/laplace-map/#Prior-Requirement","page":"Laplace MAP","title":"Prior Requirement","text":"Because LaplaceMAP uses fixed-effect priors in the objective, every fixed effect declared in @fixedEffects must have an associated prior distribution. If any fixed effect is declared as Priorless, fitting will throw an error.","category":"section"},{"location":"estimation/laplace-map/#Minimal-Usage","page":"Laplace MAP","title":"Minimal Usage","text":"The following example demonstrates a simple LaplaceMAP fit with log-normal observations and subject-level random effects.\n\nusing NoLimits\nusing DataFrames\nusing Distributions\n\nmodel = @Model begin\n    @fixedEffects begin\n        a = RealNumber(0.2, prior=Normal(0.0, 1.0))\n        sigma = RealNumber(0.5, scale=:log, prior=LogNormal(0.0, 0.5))\n    end\n\n    @covariates begin\n        t = Covariate()\n    end\n\n    @randomEffects begin\n        eta = RandomEffect(Normal(0.0, 1.0); column=:ID)\n    end\n\n    @formulas begin\n        y ~ LogNormal(a + eta, sigma)\n    end\nend\n\ndf = DataFrame(\n    ID = [:A, :A, :B, :B],\n    t = [0.0, 1.0, 0.0, 1.0],\n    y = [1.0, 1.1, 0.9, 1.0],\n)\n\ndm = DataModel(model, df; primary_id=:ID, time_col=:t)\n\nres = fit_model(dm, NoLimits.LaplaceMAP(; optim_kwargs=(maxiters=80,)))","category":"section"},{"location":"estimation/laplace-map/#Practical-Comparison","page":"Laplace MAP","title":"Practical Comparison","text":"To compare the effect of prior regularization, the same data model can be fit with both methods:\n\nres_laplace = fit_model(dm, NoLimits.Laplace())\nres_laplace_map = fit_model(dm, NoLimits.LaplaceMAP())\n\nThe model structure, random-effects integration, and optimizer configuration are identical between the two calls. The only difference is that LaplaceMAP includes the fixed-effect log-prior contribution in the objective. This regularization can improve estimation stability when data are sparse or when certain parameters are weakly identified by the data alone.","category":"section"},{"location":"estimation/mcem/#MCEM","page":"MCEM","title":"MCEM","text":"The Monte Carlo Expectation-Maximization (MCEM) algorithm is a likelihood-based approach for fitting nonlinear mixed-effects models when the marginal likelihood cannot be computed in closed form. It alternates between two steps:\n\nan MCMC E-step that draws samples from the conditional distribution of random effects given the current fixed-effect estimates, and\nan optimization-based M-step that updates the fixed effects by maximizing the Monte Carlo approximation of the expected complete-data log-likelihood.\n\nThis formulation accommodates arbitrary nonlinear observation models, including those defined through ordinary differential equations, without requiring analytically tractable integrals over the random effects.","category":"section"},{"location":"estimation/mcem/#Applicability","page":"MCEM","title":"Applicability","text":"MCEM is designed for models that include both fixed and random effects:\n\nThe model must declare at least one random effect and at least one free fixed effect.\nMultiple random-effect grouping columns and multivariate random effects are fully supported.\n\nIf fixed-effect priors are defined in the model, MCEM ignores them in its objective. To incorporate priors, use LaplaceMAP or MCMC instead.","category":"section"},{"location":"estimation/mcem/#Basic-Usage","page":"MCEM","title":"Basic Usage","text":"The following example illustrates a minimal MCEM workflow with a nonlinear observation model.\n\nusing NoLimits\nusing DataFrames\nusing Distributions\nusing Turing\n\nmodel = @Model begin\n    @fixedEffects begin\n        a = RealNumber(0.2)\n        b = RealNumber(0.1)\n        sigma = RealNumber(0.3, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n    end\n\n    @randomEffects begin\n        eta = RandomEffect(TDist(6.0); column=:ID)\n    end\n\n    @formulas begin\n        mu = a + b * t + exp(eta)   # nonlinear in random effects\n        y ~ Normal(mu, sigma)\n    end\nend\n\ndf = DataFrame(\n    ID = [:A, :A, :B, :B, :C, :C],\n    t = [0.0, 1.0, 0.0, 1.0, 0.0, 1.0],\n    y = [1.0, 1.3, 0.9, 1.2, 1.1, 1.5],\n)\n\ndm = DataModel(model, df; primary_id=:ID, time_col=:t)\n\nmethod = NoLimits.MCEM(;\n    sampler=MH(),\n    turing_kwargs=(n_samples=20, n_adapt=0, progress=false),\n    maxiters=10,\n)\n\nres = fit_model(dm, method)","category":"section"},{"location":"estimation/mcem/#Constructor-Options","page":"MCEM","title":"Constructor Options","text":"The full set of constructor arguments is shown below. All arguments have defaults and are keyword-only.\n\nusing Optimization\nusing OptimizationOptimJL\nusing LineSearches\nusing Turing\n\nmethod = NoLimits.MCEM(;\n    optimizer=OptimizationOptimJL.LBFGS(linesearch=LineSearches.BackTracking()),\n    optim_kwargs=NamedTuple(),\n    adtype=Optimization.AutoForwardDiff(),\n    sampler=Turing.NUTS(0.75),\n    turing_kwargs=NamedTuple(),\n    sample_schedule=250,\n    warm_start=true,\n    verbose=false,\n    progress=true,\n    maxiters=100,\n    rtol_theta=1e-4,\n    atol_theta=1e-6,\n    rtol_Q=1e-4,\n    atol_Q=1e-6,\n    consecutive_params=3,\n    ebe_optimizer=OptimizationOptimJL.LBFGS(linesearch=LineSearches.BackTracking()),\n    ebe_optim_kwargs=NamedTuple(),\n    ebe_adtype=Optimization.AutoForwardDiff(),\n    ebe_grad_tol=:auto,\n    ebe_multistart_n=50,\n    ebe_multistart_k=10,\n    ebe_multistart_max_rounds=5,\n    ebe_multistart_sampling=:lhs,\n    ebe_rescue_on_high_grad=true,\n    ebe_rescue_multistart_n=128,\n    ebe_rescue_multistart_k=32,\n    ebe_rescue_max_rounds=8,\n    ebe_rescue_grad_tol=:auto,\n    ebe_rescue_multistart_sampling=:lhs,\n    lb=nothing,\n    ub=nothing,\n)","category":"section"},{"location":"estimation/mcem/#Option-Groups","page":"MCEM","title":"Option Groups","text":"The constructor arguments are organized into the following functional groups.\n\nGroup Keywords What they control\nM-step optimizer optimizer, optim_kwargs, adtype Optimization of fixed effects using Optimization.jl.\nE-step sampler sampler, turing_kwargs, sample_schedule, warm_start Sampling random effects per iteration.\nEM stopping maxiters, rtol_theta, atol_theta, rtol_Q, atol_Q, consecutive_params Iteration limit and convergence checks.\nFinal EB estimation ebe_* and ebe_rescue_* options Post-fit empirical Bayes mode computation used by random-effects accessors and diagnostics.\nBounds lb, ub Optional transformed-scale bounds for free fixed effects in M-step optimization.\n\nThe E-step sampling interface is built on Turing.jl: the sampler and turing_kwargs arguments are forwarded to the underlying Turing sampling calls.","category":"section"},{"location":"estimation/mcem/#Constructor-Input-Reference","page":"MCEM","title":"Constructor Input Reference","text":"","category":"section"},{"location":"estimation/mcem/#M-step-Optimization-Inputs","page":"MCEM","title":"M-step Optimization Inputs","text":"These arguments configure the fixed-effect optimization performed at each MCEM iteration.\n\noptimizer\nOptimizer for fixed effects in each MCEM iteration.\nDefault: OptimizationOptimJL.LBFGS(linesearch=LineSearches.BackTracking()).\noptim_kwargs\nKeyword arguments forwarded to Optimization.solve for the M-step optimizer.\nadtype\nAutomatic differentiation backend for the M-step objective in OptimizationFunction.","category":"section"},{"location":"estimation/mcem/#E-step-Sampling-Inputs","page":"MCEM","title":"E-step Sampling Inputs","text":"These arguments control the MCMC sampling of random effects at each iteration.\n\nsampler\nTuring sampler used for random-effect sampling in each E-step.\nDefault: Turing.NUTS(0.75).\nturing_kwargs\nAdditional keyword arguments passed through to Turing sampling calls.\nThe keys n_samples and n_adapt are interpreted explicitly by MCEM, then passed as sampling and adaptation sizes.\nsample_schedule\nControls the number of MCMC samples drawn per iteration.\nAccepted forms: an integer (constant across iterations), a vector (iteration-indexed), or a function iter -> n_samples.\nIf a schedule value is <= 0, MCEM falls back to turing_kwargs[:n_samples] (or 100 if absent).\nwarm_start\nWhen true, reuses previous batch latent-state parameter values as initialization for the next E-step.\nverbose\nEnables iteration-level logging of diagnostic quantities (Q, dtheta, dQ, sample count).\nprogress\nEnables or disables the progress bar.","category":"section"},{"location":"estimation/mcem/#EM-Convergence-Inputs","page":"MCEM","title":"EM Convergence Inputs","text":"MCEM monitors both fixed-effect parameter stability and Q-function stability to determine convergence.\n\nmaxiters\nMaximum number of MCEM iterations.\nrtol_theta, atol_theta\nRelative and absolute tolerances for fixed-effect parameter stabilization.\nrtol_Q, atol_Q\nRelative and absolute tolerances for Q-function stabilization.\nconsecutive_params\nNumber of consecutive iterations that must simultaneously satisfy both parameter and Q stabilization criteria before convergence is declared.","category":"section"},{"location":"estimation/mcem/#Final-EB-Mode-Inputs","page":"MCEM","title":"Final EB Mode Inputs","text":"After the EM iterations complete, MCEM computes empirical Bayes (EB) modal estimates of the random effects. These estimates are used by downstream accessors and diagnostic functions.\n\nebe_optimizer, ebe_optim_kwargs, ebe_adtype\nOptimizer, solve arguments, and AD backend for EB mode optimization.\nebe_grad_tol\nGradient tolerance for the EB optimization (:auto selects a data-adaptive value).\nebe_multistart_n, ebe_multistart_k, ebe_multistart_max_rounds, ebe_multistart_sampling\nMultistart configuration for EB optimization, controlling the number of initial points, top candidates retained, maximum restart rounds, and sampling strategy.\nebe_rescue_on_high_grad\nEnables a rescue multistart procedure if the final EB gradient norm remains above threshold.\nebe_rescue_multistart_n, ebe_rescue_multistart_k, ebe_rescue_max_rounds, ebe_rescue_grad_tol, ebe_rescue_multistart_sampling\nConfiguration for the rescue strategy.","category":"section"},{"location":"estimation/mcem/#Bound-Inputs","page":"MCEM","title":"Bound Inputs","text":"lb, ub\nOptional transformed-scale bounds for free fixed effects in the M-step optimization.\nParameters held constant (via the constants fit keyword) are removed from the optimized vector; any corresponding bound entries are ignored.","category":"section"},{"location":"estimation/mcem/#Detailed-Behavior","page":"MCEM","title":"Detailed Behavior","text":"This section provides additional details on selected options.\n\nsample_schedule\nCan be:\nan integer (constant samples per MCEM iteration),\na vector (iteration-indexed schedule),\na function (iter -> n_samples).\nsampler\nCommon tested choices are MH() and NUTS(...).\nConvergence\nMCEM checks both parameter stabilization and Q stabilization.\nBoth must satisfy tolerances for consecutive_params iterations.\nwarm_start=true\nReuses previous latent-state parameterization in the E-step where possible.\nstore_eb_modes (fit keyword)\ntrue: stores final EB modes in fit result.\nfalse: EB modes can be recomputed later when calling random-effects accessors.","category":"section"},{"location":"estimation/mcem/#Fit-Keywords","page":"MCEM","title":"Fit Keywords","text":"In addition to the method constructor arguments, fit_model accepts several keyword arguments that control data-level settings for the MCEM run.\n\nres = fit_model(\n    dm,\n    method;\n    constants=(a=0.2,),\n    constants_re=(; eta=(; A=0.0,)),\n    penalty=NamedTuple(),\n    ode_args=(),\n    ode_kwargs=NamedTuple(),\n    serialization=EnsembleSerial(),\n    rng=Random.default_rng(),\n    theta_0_untransformed=nothing,\n    store_eb_modes=true,\n)\n\nThe constants_re argument allows specific random-effect levels to be fixed at known values while the remaining levels are estimated.","category":"section"},{"location":"estimation/mcem/#Optimization.jl-Interface-(M-step-and-EB)","page":"MCEM","title":"Optimization.jl Interface (M-step and EB)","text":"MCEM uses Optimization.jl in two places:\n\nThe M-step optimization over fixed effects at each iteration.\nThe EB mode optimization (final or recomputed EB modes) through the ebe_* configuration.\n\nTested M-step optimizers include:\n\nOptimizationOptimJL.LBFGS(...) (default)\nOptimizationOptimisers.Adam(...)\nOptimizationBBO.BBO_adaptive_de_rand_1_bin_radiuslimited()\n\nWhen using derivative-free optimizers such as those from OptimizationBBO, finite bounds must be supplied:\n\nusing OptimizationBBO\n\nlb, ub = default_bounds_from_start(dm; margin=1.0)\n\nmethod_bbo = NoLimits.MCEM(;\n    optimizer=OptimizationBBO.BBO_adaptive_de_rand_1_bin_radiuslimited(),\n    optim_kwargs=(iterations=20,),\n    sampler=MH(),\n    turing_kwargs=(n_samples=10, n_adapt=0, progress=false),\n    maxiters=5,\n    lb=lb,\n    ub=ub,\n)\n\nThe M-step and EB optimizers can be configured independently. The following example uses L-BFGS for both but with distinct iteration limits and multistart settings:\n\nusing OptimizationOptimJL\nusing LineSearches\n\nmethod_two_stage = NoLimits.MCEM(;\n    optimizer=OptimizationOptimJL.LBFGS(linesearch=LineSearches.BackTracking()),\n    optim_kwargs=(maxiters=120,),\n    ebe_optimizer=OptimizationOptimJL.LBFGS(linesearch=LineSearches.BackTracking()),\n    ebe_optim_kwargs=(maxiters=60,),\n    ebe_grad_tol=:auto,\n    ebe_multistart_n=80,\n    ebe_multistart_k=16,\n)","category":"section"},{"location":"estimation/mcem/#Accessing-Results","page":"MCEM","title":"Accessing Results","text":"After fitting, results are accessed through the standard accessor interface. MCEM returns point estimates (the EM solution) rather than a posterior chain.\n\ntheta_u = NoLimits.get_params(res; scale=:untransformed)\nobj = get_objective(res)\nok = get_converged(res)\n\nre_df = get_random_effects(res)","category":"section"},{"location":"capabilities/#Capabilities","page":"Capabilities","title":"Capabilities","text":"NoLimits.jl provides a broad set of modeling, estimation, and diagnostic capabilities for nonlinear mixed-effects analysis. All features listed below are implemented and tested in the current package.","category":"section"},{"location":"capabilities/#Modeling","page":"Capabilities","title":"Modeling","text":"Nonlinear mixed-effects models for longitudinal data, with one or multiple random-effect grouping structures (e.g., subject-level and site-level variability in a single model).\nFixed-effects-only models for settings where random effects are not needed.\nODE-based and non-ODE models within the same modeling framework: algebraic structural models and mechanistic ODE systems share the same specification language.\nMultiple outcomes in one model, including mixed outcome types (e.g., continuous and count outcomes jointly).\nHidden Markov outcome models via DiscreteTimeDiscreteStatesHMM and ContinuousTimeDiscreteStatesHMM, enabling latent-state-dependent observation processes. Dedicated parameter types ProbabilityVector, DiscreteTransitionMatrix, and ContinuousTransitionMatrix provide AD-compatible, automatically constrained representations of initial-state distributions and transition or rate matrices.\nLeft-censored and interval-censored observations through censored(...) in the observation model.","category":"section"},{"location":"capabilities/#Random-Effects-Flexibility","page":"Capabilities","title":"Random-Effects Flexibility","text":"Univariate and multivariate random effects.\nRandom-effect distributions beyond the Gaussian family: heavy-tailed (TDist), skewed (SkewNormal), positive-valued (LogNormal, Gamma), and other distributions from Distributions.jl.\nFlow-based random effects via NormalizingPlanarFlow for highly flexible latent distributions.\nRandom-effect distributions parameterized by covariates, neural networks, soft decision trees, or spline functions – enabling covariate-dependent heterogeneity that goes beyond standard variance models.","category":"section"},{"location":"capabilities/#Machine-Learning-Integration","page":"Capabilities","title":"Machine-Learning Integration","text":"Neural-network parameter blocks (NNParameters) can be embedded in formulas, ODE dynamics, initial conditions, and random-effect distribution parameterizations. Neural-ODE-style models arise naturally when learned components appear inside @DifferentialEquation.\nSoft decision tree parameter blocks (SoftTreeParameters) provide an alternative differentiable function approximator with the same integration points as neural networks.\nSpline parameter blocks (SplineParameters) for smooth, learnable basis-function expansions.\nAll learned components can be used at the population level (fixed effects only) or individualized through full-parameter random effects.","category":"section"},{"location":"capabilities/#Covariate-Handling","page":"Capabilities","title":"Covariate Handling","text":"Time-varying covariates that change across observations within an individual.\nConstant covariates that are fixed within a grouping level (e.g., baseline age, treatment arm).\nDynamic (interpolated) covariates that provide continuous-time access within ODE integration, with support for eight interpolation methods from DataInterpolations.jl.","category":"section"},{"location":"capabilities/#Estimation-Methods","page":"Capabilities","title":"Estimation Methods","text":"Model type Available methods\nMixed-effects Laplace approximation, LaplaceMAP, MCEM, SAEM, MCMC, VI\nFixed-effects only MLE, MAP, MCMC, VI\nCross-method Multistart optimization wrapper\n\nAll methods share a unified fit_model interface, allowing direct comparison of estimation approaches on the same model and data.","category":"section"},{"location":"capabilities/#Uncertainty-Quantification","page":"Capabilities","title":"Uncertainty Quantification","text":"Wald-based intervals from Hessian or sandwich covariance estimates.\nProfile-likelihood intervals via LikelihoodProfiler.jl.\nPosterior-draw intervals from MCMC chains or VI variational posteriors (direct or refit).\nA unified compute_uq interface across all backends.","category":"section"},{"location":"capabilities/#Diagnostics-and-Visualization","page":"Capabilities","title":"Diagnostics and Visualization","text":"Fitted-trajectory plots overlaid on observed data.\nVisual predictive checks (VPCs).\nResidual diagnostics: QQ plots, PIT histograms, autocorrelation plots.\nRandom-effects diagnostics: marginal density plots, pairwise scatter, standardized EBE distributions.\nObservation-distribution plots showing full predictive distributions at selected data points.\nUncertainty distribution plots for estimated parameters.\nMultistart objective waterfalls and parameter stability summaries.","category":"section"},{"location":"capabilities/#Composability","page":"Capabilities","title":"Composability","text":"A defining feature of NoLimits.jl is that the capabilities above are freely composable. A single model can simultaneously use ODE dynamics, multiple learned function approximators, several random-effect grouping levels with non-Gaussian distributions, covariates at different temporal resolutions, and multiple outcome types. This composability is central to the package design and is exercised throughout the test suite.","category":"section"},{"location":"estimation/saem/#SAEM","page":"SAEM","title":"SAEM","text":"The Stochastic Approximation Expectation-Maximization (SAEM) algorithm is a widely used method for parameter estimation in nonlinear mixed-effects models. Unlike standard EM, SAEM replaces the intractable E-step expectation with a stochastic approximation that is updated incrementally across iterations. Each iteration consists of three steps:\n\nE-step: MCMC sampling of random effects conditional on the current fixed-effect estimates.\nSA-step: stochastic smoothing of sufficient statistics (or stored latent snapshots) using a decreasing gain sequence.\nM-step: fixed-effect update, performed either through numerical optimization or through user-supplied closed-form expressions.\n\nSAEM is particularly well suited to models with complex nonlinearities, including ODE-based dynamics and function-approximator components such as neural networks or soft decision trees, because its convergence properties do not require closed-form integration over the random effects.","category":"section"},{"location":"estimation/saem/#Applicability","page":"SAEM","title":"Applicability","text":"SAEM is designed for models that include both fixed and random effects:\n\nThe model must declare at least one random effect and at least one free fixed effect.\nMultiple random-effect grouping columns and multivariate random effects are fully supported.\n\nIf fixed-effect priors are defined in the model, SAEM ignores them in its objective. To incorporate priors, use LaplaceMAP or MCMC instead.","category":"section"},{"location":"estimation/saem/#Basic-Usage","page":"SAEM","title":"Basic Usage","text":"The following example demonstrates a minimal SAEM workflow with a nonlinear mixed-effects model.\n\nusing NoLimits\nusing DataFrames\nusing Distributions\nusing Turing\n\nmodel = @Model begin\n    @fixedEffects begin\n        a = RealNumber(0.2)\n        b = RealNumber(0.1)\n        sigma = RealNumber(0.3, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n    end\n\n    @randomEffects begin\n        eta = RandomEffect(Normal(0.0, 0.4); column=:ID)\n    end\n\n    @formulas begin\n        mu = exp(a + b * t + eta)   # nonlinear in random effects\n        y ~ LogNormal(log(mu), sigma)\n    end\nend\n\ndf = DataFrame(\n    ID = [:A, :A, :B, :B, :C, :C],\n    t = [0.0, 1.0, 0.0, 1.0, 0.0, 1.0],\n    y = [1.0, 1.25, 0.95, 1.18, 1.05, 1.42],\n)\n\ndm = DataModel(model, df; primary_id=:ID, time_col=:t)\n\nmethod = NoLimits.SAEM(;\n    sampler=MH(),\n    turing_kwargs=(n_samples=20, n_adapt=0, progress=false),\n    mcmc_steps=20,\n    maxiters=40,\n)\n\nres = fit_model(dm, method)","category":"section"},{"location":"estimation/saem/#Constructor-Options","page":"SAEM","title":"Constructor Options","text":"The full set of constructor arguments is shown below. All arguments have defaults and are keyword-only.\n\nusing Optimization\nusing OptimizationOptimJL\nusing LineSearches\nusing Turing\n\nmethod = NoLimits.SAEM(;\n    optimizer=OptimizationOptimJL.LBFGS(linesearch=LineSearches.BackTracking()),\n    optim_kwargs=NamedTuple(),\n    adtype=Optimization.AutoForwardDiff(),\n    sampler=Turing.NUTS(0.75),\n    turing_kwargs=NamedTuple(),\n    update_schedule=:all,\n    warm_start=true,\n    verbose=false,\n    progress=true,\n    mcmc_steps=80,\n    max_store=50,\n    t0=20,\n    kappa=0.65,\n    maxiters=300,\n    rtol_theta=5e-5,\n    atol_theta=5e-7,\n    rtol_Q=5e-5,\n    atol_Q=5e-7,\n    consecutive_params=4,\n    suffstats=nothing,\n    q_from_stats=nothing,\n    mstep_closed_form=nothing,\n    builtin_stats=:auto,\n    builtin_mean=:none,\n    resid_var_param=:σ,\n    re_cov_params=NamedTuple(),\n    re_mean_params=NamedTuple(),\n    ebe_optimizer=OptimizationOptimJL.LBFGS(linesearch=LineSearches.BackTracking()),\n    ebe_optim_kwargs=NamedTuple(),\n    ebe_adtype=Optimization.AutoForwardDiff(),\n    ebe_grad_tol=:auto,\n    ebe_multistart_n=50,\n    ebe_multistart_k=10,\n    ebe_multistart_max_rounds=5,\n    ebe_multistart_sampling=:lhs,\n    ebe_rescue_on_high_grad=true,\n    ebe_rescue_multistart_n=128,\n    ebe_rescue_multistart_k=32,\n    ebe_rescue_max_rounds=0,\n    ebe_rescue_grad_tol=:auto,\n    ebe_rescue_multistart_sampling=:lhs,\n    lb=nothing,\n    ub=nothing,\n)","category":"section"},{"location":"estimation/saem/#Option-Groups","page":"SAEM","title":"Option Groups","text":"The constructor arguments are organized into the following functional groups.\n\nGroup Keywords What they control\nM-step optimizer optimizer, optim_kwargs, adtype Fixed-effect update in each SAEM iteration via Optimization.jl.\nE-step sampler sampler, turing_kwargs, mcmc_steps, update_schedule, warm_start Random-effect sampling and batch update selection.\nSA schedule and stopping t0, kappa, maxiters, rtol_theta, atol_theta, rtol_Q, atol_Q, consecutive_params Stochastic approximation gain schedule and convergence criteria.\nCustom statistics hooks suffstats, q_from_stats, mstep_closed_form, max_store User-defined sufficient statistics and optional closed-form M-step.\nBuilt-in statistics hooks builtin_stats, builtin_mean, resid_var_param, re_cov_params, re_mean_params Automatic closed-form parameter updates for supported distribution structures.\nFinal EB modes ebe_*, ebe_rescue_* Post-fit empirical Bayes mode optimization used by random-effects accessors.\nBounds lb, ub Optional transformed-scale bounds on free fixed effects.","category":"section"},{"location":"estimation/saem/#Constructor-Input-Reference","page":"SAEM","title":"Constructor Input Reference","text":"","category":"section"},{"location":"estimation/saem/#E-step-Sampling-Inputs","page":"SAEM","title":"E-step Sampling Inputs","text":"These arguments configure the MCMC sampling of random effects at each SAEM iteration.\n\nsampler\nTuring sampler used for the random-effect E-step.\nCommon choices include NUTS(...) and MH().\nturing_kwargs\nAdditional keyword arguments passed to Turing sampling calls.\nmcmc_steps\nNumber of MCMC samples drawn per iteration.\nIf mcmc_steps <= 0, SAEM falls back to turing_kwargs[:n_samples] (or 1).\nupdate_schedule\nControls which batches of individuals are updated at each iteration, enabling minibatch variants of SAEM.\nSupported values:\n:all updates all batches.\ninteger m updates a random minibatch of size min(m, nbatches).\nfunction (nbatches, iter, rng) -> Vector{Int} returns the batch indices to update.\nwarm_start\nWhen true, reuses latent-state sampler state between iterations where available.\n\nThe E-step sampling interface is built on Turing.jl.","category":"section"},{"location":"estimation/saem/#M-step-Optimization-Inputs","page":"SAEM","title":"M-step Optimization Inputs","text":"When the M-step is performed numerically (i.e., no closed-form update is provided), these arguments control the fixed-effect optimization.\n\noptimizer\nOptimizer for the M-step fixed-effect update.\nDefault: OptimizationOptimJL.LBFGS(...).\noptim_kwargs\nKeyword arguments forwarded to Optimization.solve.\nadtype\nAutomatic differentiation backend used to construct the OptimizationFunction.\n\nSAEM uses the SciML Optimization.jl interface for numerical M-step updates.","category":"section"},{"location":"estimation/saem/#SA-Schedule-and-Convergence-Inputs","page":"SAEM","title":"SA Schedule and Convergence Inputs","text":"The stochastic approximation gain sequence controls the rate at which running statistics are updated toward new samples. A two-phase schedule is used: an initial averaging phase followed by a decay phase.\n\nt0, kappa\nDefine the SA gain schedule gamma_t:\ngamma_t = 1 for t <= t0 (averaging phase)\ngamma_t = (t - t0)^(-kappa) for t > t0 (decay phase)\nmaxiters\nMaximum number of SAEM iterations.\nrtol_theta, atol_theta\nRelative and absolute stabilization tolerances for fixed effects.\nrtol_Q, atol_Q\nRelative and absolute stabilization tolerances for the SAEM Q criterion.\nconsecutive_params\nNumber of consecutive iterations that must simultaneously satisfy both parameter and Q criteria before convergence is declared.","category":"section"},{"location":"estimation/saem/#Custom-Statistics-Inputs","page":"SAEM","title":"Custom Statistics Inputs","text":"SAEM supports a fully user-defined sufficient-statistics pathway, allowing closed-form M-step updates for models where the sufficient statistics are known analytically.\n\nsuffstats\nCallback for user-defined sufficient statistics:\nsuffstats(dm, batch_infos, b_current, theta_u, fixed_maps) -> s_new\nq_from_stats\nCallback for Q evaluation from smoothed statistics:\nq_from_stats(s, theta_u, dm) -> Real\nmstep_closed_form\nCallback for user-defined closed-form M-step:\nmstep_closed_form(s, dm) -> ComponentArray\nThe closed-form M-step is activated only when both suffstats and mstep_closed_form are provided.\nmax_store\nNumber of latent snapshot iterations retained for numerical Q evaluation.\nUsed in the numerical Q path (i.e., when suffstats is not active).","category":"section"},{"location":"estimation/saem/#Built-in-Update-Inputs","page":"SAEM","title":"Built-in Update Inputs","text":"For common distribution structures, SAEM can automatically derive closed-form updates for selected parameter blocks without requiring user-supplied callbacks.\n\nbuiltin_stats\n:auto, :closed_form, or :none.\n:auto attempts to infer compatible closed-form mappings from the model structure.\n:gaussian_re is accepted as a backward-compatible alias for :closed_form.\nbuiltin_mean\n:glm or :none.\nresid_var_param, re_cov_params, re_mean_params\nSpecify the target parameters for built-in updates when enabled.\n\nWhen suffstats is provided, builtin_mean=:glm is skipped by design to avoid conflicting updates.","category":"section"},{"location":"estimation/saem/#Final-EB-Mode-Inputs","page":"SAEM","title":"Final EB Mode Inputs","text":"After convergence, SAEM computes empirical Bayes modal estimates of the random effects for use by downstream accessors and diagnostics.\n\nebe_optimizer, ebe_optim_kwargs, ebe_adtype, ebe_grad_tol\nConfiguration for the final EB mode optimization.\nebe_multistart_n, ebe_multistart_k, ebe_multistart_max_rounds, ebe_multistart_sampling\nMultistart configuration for EB mode optimization.\nebe_rescue_on_high_grad and remaining ebe_rescue_*\nRescue strategy activated if the final EB gradient norm remains above threshold.","category":"section"},{"location":"estimation/saem/#Bound-Inputs","page":"SAEM","title":"Bound Inputs","text":"lb, ub\nOptional transformed-scale bounds for free fixed effects.\nWhen a closed-form M-step is used, SAEM projects closed-form updates into these bounds on the transformed scale.","category":"section"},{"location":"estimation/saem/#Which-Models-Have-Closed-Form-M-step-Updates?","page":"SAEM","title":"Which Models Have Closed-Form M-step Updates?","text":"SAEM provides two closed-form pathways that can substantially accelerate convergence by avoiding numerical optimization for selected parameter blocks.\n\nFull user-defined closed-form M-step: Activated only when both suffstats and mstep_closed_form are provided.\nBuilt-in blockwise closed-form updates (builtin_stats=:closed_form or :auto): Selected distribution-parameter blocks are updated in closed form, while remaining free parameters are updated through numerical optimization.\n\nBuilt-in blockwise closed-form updates are available for:\n\nRandom-effect distribution parameters in Normal, MvNormal, LogNormal, and Exponential blocks (through re_mean_params and re_cov_params).\nObservation distribution parameters in Normal, LogNormal, Exponential, Bernoulli, and Poisson blocks (through resid_var_param, including named outcome-specific mappings).\n\nThese updates are compatible with arbitrarily nonlinear model structure, including ODE-based dynamics and function-approximator components, provided that the updated parameters appear in the supported distribution blocks.","category":"section"},{"location":"estimation/saem/#Example-1:-Neural-Network-Based-Nonlinear-ODE-Model-with-Closed-Form-RE-Mean-and-Outcome-Scale-Blocks","page":"SAEM","title":"Example 1: Neural-Network-Based Nonlinear ODE Model with Closed-Form RE-Mean and Outcome-Scale Blocks","text":"The following example illustrates a mixed-effects ODE model in which neural network parameter vectors serve as random-effect distribution means. Despite the highly nonlinear dynamics, the random-effect mean parameters and observation scale parameter admit closed-form SAEM updates.\n\nusing NoLimits\nusing LinearAlgebra\nusing Lux\n\nchain_A1 = Chain(Dense(1, 4, tanh), Dense(4, 1))\nchain_A2 = Chain(Dense(1, 4, tanh), Dense(4, 1))\nchain_C1 = Chain(Dense(1, 4, tanh), Dense(4, 1))\nchain_C2 = Chain(Dense(1, 4, tanh), Dense(4, 1))\n\nmodel = @Model begin\n    @helpers begin\n        softplus(u) = u > 20 ? u : log1p(exp(u))\n    end\n\n    @fixedEffects begin\n        sigma = RealNumber(1.0, scale=:log)\n        zA1 = NNParameters(chain_A1; function_name=:NNA1, calculate_se=false)\n        zA2 = NNParameters(chain_A2; function_name=:NNA2, calculate_se=false)\n        zC1 = NNParameters(chain_C1; function_name=:NNC1, calculate_se=false)\n        zC2 = NNParameters(chain_C2; function_name=:NNC2, calculate_se=false)\n    end\n\n    @covariates begin\n        t = Covariate()\n        d = ConstantCovariate(; constant_on=:ID)\n    end\n\n    @randomEffects begin\n        etaA1 = RandomEffect(MvNormal(zA1, Diagonal(ones(length(zA1)))); column=:ID)\n        etaA2 = RandomEffect(MvNormal(zA2, Diagonal(ones(length(zA2)))); column=:ID)\n        etaC1 = RandomEffect(MvNormal(zC1, Diagonal(ones(length(zC1)))); column=:ID)\n        etaC2 = RandomEffect(MvNormal(zC2, Diagonal(ones(length(zC2)))); column=:ID)\n    end\n\n    @DifferentialEquation begin\n        fA1(t) = softplus(NNA1([t / 24], etaA1)[1])\n        fA2(t) = softplus(NNA2([softplus(depot)], etaA2)[1])\n        fC1(t) = -softplus(NNC1([softplus(center)], etaC1)[1])\n        fC2(t) = softplus(NNC2([t / 24], etaC2)[1])\n        D(depot) ~ -d * fA1(t) - fA2(t)\n        D(center) ~ d * fA1(t) + fA2(t) + fC1(t) + d * fC2(t)\n    end\n\n    @initialDE begin\n        depot = d\n        center = 0.0\n    end\n\n    @formulas begin\n        y ~ Normal(center(t), sigma)\n    end\nend\n\nsaem_method = NoLimits.SAEM(;\n    builtin_stats=:closed_form,\n    re_mean_params=(; etaA1=:zA1, etaA2=:zA2, etaC1=:zC1, etaC2=:zC2),\n    re_cov_params=NamedTuple(),\n    resid_var_param=:sigma,\n)\n\nThe closed-form blocks arise from the following model structure:\n\nEach random-effect block is MvNormal(mean_parameter, fixed_covariance) (e.g., etaA1 ~ MvNormal(zA1, I)). With re_mean_params, SAEM updates the mean vectors (zA1, zA2, zC1, zC2) using smoothed conditional means of the sampled random effects – a closed-form Gaussian mean update.\nThe observation model is y ~ Normal(center(t), sigma). With resid_var_param=:sigma, SAEM updates sigma from smoothed residual second moments – a closed-form Normal scale update.\nSetting re_cov_params=NamedTuple() leaves the random-effect covariance fixed, so only mean and outcome-scale closed-form blocks are applied.\n\nThe ODE dynamics and neural network transformations introduce substantial nonlinearity, but this does not affect the availability of closed-form updates for the distribution-parameter blocks.","category":"section"},{"location":"estimation/saem/#Example-2:-Soft-Decision-Tree-Based-Nonlinear-ODE-Model-with-Closed-Form-RE-Mean-and-Outcome-Scale-Blocks","page":"SAEM","title":"Example 2: Soft-Decision-Tree-Based Nonlinear ODE Model with Closed-Form RE-Mean and Outcome-Scale Blocks","text":"This example follows the same structural pattern as Example 1, replacing neural network components with soft decision trees.\n\nusing NoLimits\nusing LinearAlgebra\n\nmodel = @Model begin\n    @helpers begin\n        softplus(u) = u > 20 ? u : log1p(exp(u))\n    end\n\n    @fixedEffects begin\n        sigma = RealNumber(1.0, scale=:log)\n        gA1 = SoftTreeParameters(1, 2; function_name=:STA1, calculate_se=false)\n        gA2 = SoftTreeParameters(1, 2; function_name=:STA2, calculate_se=false)\n        gC1 = SoftTreeParameters(1, 2; function_name=:STC1, calculate_se=false)\n        gC2 = SoftTreeParameters(1, 2; function_name=:STC2, calculate_se=false)\n    end\n\n    @covariates begin\n        t = Covariate()\n        d = ConstantCovariate(; constant_on=:ID)\n    end\n\n    @randomEffects begin\n        etaA1 = RandomEffect(MvNormal(gA1, Diagonal(ones(length(gA1)))); column=:ID)\n        etaA2 = RandomEffect(MvNormal(gA2, Diagonal(ones(length(gA2)))); column=:ID)\n        etaC1 = RandomEffect(MvNormal(gC1, Diagonal(ones(length(gC1)))); column=:ID)\n        etaC2 = RandomEffect(MvNormal(gC2, Diagonal(ones(length(gC2)))); column=:ID)\n    end\n\n    @DifferentialEquation begin\n        fA1(t) = softplus(STA1([t / 24], etaA1)[1])\n        fA2(t) = softplus(STA2([softplus(depot)], etaA2)[1])\n        fC1(t) = -softplus(STC1([softplus(center)], etaC1)[1])\n        fC2(t) = softplus(STC2([t / 24], etaC2)[1])\n        D(depot) ~ -d * fA1(t) - fA2(t)\n        D(center) ~ d * fA1(t) + fA2(t) + fC1(t) + d * fC2(t)\n    end\n\n    @initialDE begin\n        depot = d\n        center = 0.0\n    end\n\n    @formulas begin\n        y ~ Normal(center(t), sigma)\n    end\nend\n\nsaem_method = NoLimits.SAEM(;\n    builtin_stats=:closed_form,\n    re_mean_params=(; etaA1=:gA1, etaA2=:gA2, etaC1=:gC1, etaC2=:gC2),\n    re_cov_params=NamedTuple(),\n    resid_var_param=:sigma,\n)\n\nThe reasoning is analogous to the neural network case:\n\nEach random-effect block is MvNormal(mean_parameter, fixed_covariance) with soft-tree parameter vectors as means. The re_mean_params mapping enables closed-form Gaussian mean updates for gA1, gA2, gC1, and gC2.\nThe observation model is Normal(..., sigma), so resid_var_param=:sigma yields a closed-form scale update.\nRandom-effect covariance is fixed by construction (re_cov_params=NamedTuple()), so no covariance update is performed.","category":"section"},{"location":"estimation/saem/#Example-3:-Mechanistic-ODE-with-Auto-Detected-Closed-Form-Blocks","page":"SAEM","title":"Example 3: Mechanistic ODE with Auto-Detected Closed-Form Blocks","text":"When the model uses standard distribution parameterizations, SAEM can automatically detect compatible closed-form update targets via builtin_stats=:auto. The following example illustrates this with a mechanistic two-compartment ODE model.\n\nusing NoLimits\nusing LinearAlgebra\n\nmodel_saem = @Model begin\n    @fixedEffects begin\n        tka = RealNumber(0.45)\n        tcl = RealNumber(1.0)\n        tv = RealNumber(3.45)\n        omega1 = RealNumber(1.0, scale=:log)\n        omega2 = RealNumber(1.0, scale=:log)\n        omega3 = RealNumber(1.0, scale=:log)\n        sigma_eps = RealNumber(1.0, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n    end\n\n    @randomEffects begin\n        eta = RandomEffect(MvNormal([tka, tcl, tv], Diagonal([omega1, omega2, omega3])); column=:id)\n    end\n\n    @preDifferentialEquation begin\n        ka = exp(eta[1])\n        cl = exp(eta[2])\n        v = exp(eta[3])\n    end\n\n    @DifferentialEquation begin\n        D(depot) ~ -ka * depot\n        D(center) ~ ka * depot - cl / v * center\n    end\n\n    @initialDE begin\n        depot = 1.0\n        center = 0.0\n    end\n\n    @formulas begin\n        y1 ~ Normal(center(t) / v, sigma_eps)\n    end\nend\n\nsaem_method = NoLimits.SAEM(; builtin_stats=:auto)\n\nWith builtin_stats=:auto, SAEM inspects the model structure and identifies the following closed-form update targets:\n\nThe random-effect distribution is MvNormal([tka, tcl, tv], Diagonal([omega1, omega2, omega3])). The mean parameters (tka, tcl, tv) admit closed-form Gaussian mean updates, and the diagonal covariance parameters (omega1, omega2, omega3) admit closed-form variance updates.\nThe observation model is Normal(center(t) / v, sigma_eps), so sigma_eps admits a closed-form Normal scale update.\n\nFor MvNormal diagonal targets, the built-in update operates on the diagonal covariance entries (variances) for the mapped parameters.","category":"section"},{"location":"estimation/saem/#Custom-Sufficient-Statistics-and-Closed-Form-M-step","page":"SAEM","title":"Custom Sufficient Statistics and Closed-Form M-step","text":"For models where the sufficient statistics are known analytically, SAEM supports a fully custom statistics pathway. The per-iteration procedure is as follows:\n\nSAEM samples random effects for the updated batches.\nThe user-defined callback computes new statistics: s_new = suffstats(dm, batch_infos, b_current, theta_u, fixed_maps).\nSA smoothing is applied: s <- s + gamma_t * (s_new - s).\nThe M-step uses either the custom closed-form update (if both suffstats and mstep_closed_form are set) or falls back to numerical optimization via Optimization.jl.\nQ evaluation for convergence monitoring uses q_from_stats(s, theta_u, dm) when both suffstats and q_from_stats are set; otherwise, a numerical Q is computed from stored latent snapshots.","category":"section"},{"location":"estimation/saem/#Callback-Contracts","page":"SAEM","title":"Callback Contracts","text":"suffstats(dm, batch_infos, b_current, theta_u, fixed_maps) -> s_new\nThe return value s_new can be a scalar, array, or NamedTuple.\nKeys and shapes must remain stable across iterations.\nfixed_maps is the normalized random-effect constant map derived from constants_re.\nq_from_stats(s, theta_u, dm) -> Real\nA Q-like criterion computed from the smoothed statistics s.\nmstep_closed_form(s, dm) -> ComponentArray\nMust return the full untransformed fixed-effect parameter container.\nThe closed-form M-step is activated only when suffstats and mstep_closed_form are both provided.\n\nWhen using custom sufficient statistics, it is recommended to also provide q_from_stats so that convergence monitoring remains consistent with the statistic design.\n\nusing NoLimits\nusing DataFrames\nusing Distributions\nusing Turing\nusing ComponentArrays\n\nmodel = @Model begin\n    @fixedEffects begin\n        a = RealNumber(0.2)\n        b = RealNumber(0.1)\n        sigma = RealNumber(0.3, scale=:log)\n        tau = RealNumber(0.4, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n    end\n\n    @randomEffects begin\n        eta = RandomEffect(Normal(0.0, tau); column=:ID)\n    end\n\n    @formulas begin\n        mu = exp(a + b * t + eta)   # nonlinear in random effects\n        y ~ Exponential(mu * sigma)\n    end\nend\n\ndf = DataFrame(\n    ID = [:A, :A, :B, :B],\n    t = [0.0, 1.0, 0.0, 1.0],\n    y = [1.0, 1.08, 0.96, 1.14],\n)\n\ndm = DataModel(model, df; primary_id=:ID, time_col=:t)\n\nfunction suffstats(dm, batch_infos, b_current, theta_u, fixed_maps)\n    s_sum = 0.0\n    s_sq = 0.0\n    n = 0\n    for b in b_current\n        s_sum += sum(b)\n        s_sq += sum(abs2, b)\n        n += length(b)\n    end\n    return (; s_sum, s_sq, n=max(n, 1))\nend\n\nq_from_stats = (s, theta_u, dm) -> -0.5 * (s.s_sq - (s.s_sum^2) / s.n)\n\ntheta_template = ComponentArray(a=0.2, b=0.1, sigma=0.3, tau=0.4)\nfunction mstep_closed_form(s, dm)\n    theta_u = deepcopy(theta_template)\n    theta_u.a = 0.2 + 0.01 * s.s_sum\n    theta_u.b = 0.1 + 0.001 * s.s_sq\n    sigma_hat = sqrt(max(s.s_sq / s.n, 1e-8))\n    theta_u.sigma = sigma_hat\n    theta_u.tau = max(0.2, 0.5 * sigma_hat)\n    return theta_u\nend\n\nmethod = NoLimits.SAEM(;\n    sampler=MH(),\n    turing_kwargs=(n_samples=12, n_adapt=0, progress=false),\n    maxiters=20,\n    suffstats=suffstats,\n    q_from_stats=q_from_stats,\n    mstep_closed_form=mstep_closed_form,\n)\n\nres = fit_model(dm, method)\n\nThe mstep_closed_form expressions above are illustrative only; they should be replaced with model-specific closed-form derivations in practice.","category":"section"},{"location":"estimation/saem/#Optimization.jl-Interface-Example","page":"SAEM","title":"Optimization.jl Interface Example","text":"When the M-step is performed numerically, any optimizer supported by Optimization.jl can be used.\n\nusing OptimizationOptimJL\nusing OptimizationOptimisers\nusing LineSearches\n\nmethod_lbfgs = NoLimits.SAEM(;\n    optimizer=OptimizationOptimJL.LBFGS(linesearch=LineSearches.BackTracking()),\n    optim_kwargs=(maxiters=120,),\n)\n\nmethod_adam = NoLimits.SAEM(;\n    optimizer=OptimizationOptimisers.Adam(0.05),\n    optim_kwargs=(maxiters=150,),\n)","category":"section"},{"location":"estimation/saem/#Accessing-Results","page":"SAEM","title":"Accessing Results","text":"After fitting, results are accessed through the standard accessor interface. Like MCEM, SAEM returns point estimates rather than a posterior chain.\n\ntheta_u = NoLimits.get_params(res; scale=:untransformed)\nobj = get_objective(res)\nok = get_converged(res)\n\nre_df = get_random_effects(res)","category":"section"},{"location":"model-building/universal-function-approximators/#Function-Approximators:-Neural-Networks-and-Soft-Trees","page":"Function Approximators (NNs + SoftTrees)","title":"Function Approximators: Neural Networks and Soft Trees","text":"Nonlinear mixed-effects models often require flexible functional forms to capture relationships that cannot be specified a priori. NoLimits supports two classes of learnable function approximators – neural networks and soft decision trees – that can be embedded directly into any model block. Their parameters are estimated jointly with all other model parameters during fitting.\n\nThe supported parameter constructors are:\n\nNNParameters(...) – wraps a Lux.jl neural network architecture.\nSoftTreeParameters(...) – constructs a differentiable soft decision tree.\n\nBoth are declared in @fixedEffects and exposed as callable model functions through the function_name keyword argument.","category":"section"},{"location":"model-building/universal-function-approximators/#Where-They-Can-Be-Used","page":"Function Approximators (NNs + SoftTrees)","title":"Where They Can Be Used","text":"Model functions created from NNParameters and SoftTreeParameters are available throughout the model specification. Specifically, they can appear in:\n\n@randomEffects – parameterizing the distributions of random effects\n@preDifferentialEquation – computing time-constant derived quantities\n@DifferentialEquation – within the right-hand side of ODE systems\n@initialDE – setting initial conditions\n@formulas – constructing the observation model","category":"section"},{"location":"model-building/universal-function-approximators/#Pattern-1:-Population-Level-Approximators-with-Separate-Random-Effects","page":"Function Approximators (NNs + SoftTrees)","title":"Pattern 1: Population-Level Approximators with Separate Random Effects","text":"In this pattern, the approximator parameters are shared across all individuals (population-level fixed effects), while between-subject variability is captured by separate, additive random effects. This is the simplest way to introduce flexible nonlinearity without dramatically increasing the dimensionality of the random-effects space.\n\nusing NoLimits\nusing Distributions\nusing Lux\n\nchain = Lux.Chain(Lux.Dense(2, 4, tanh), Lux.Dense(4, 1))\n\nmodel = @Model begin\n    @fixedEffects begin\n        sigma = RealNumber(0.3, scale=:log)\n        z_nn = NNParameters(chain; function_name=:NN1, calculate_se=false)\n        z_st = SoftTreeParameters(2, 2; function_name=:ST1, calculate_se=false)\n    end\n\n    @covariates begin\n        t = Covariate()\n        x = ConstantCovariateVector([:Age, :BMI]; constant_on=:ID)\n    end\n\n    @randomEffects begin\n        eta = RandomEffect(Normal(0.0, 1.0); column=:ID)\n    end\n\n    @formulas begin\n        mu = NN1([x.Age, x.BMI], z_nn)[1] + ST1([x.Age, x.BMI], z_st)[1] + tanh(eta) + eta^2\n        y ~ Gamma(abs(mu) + 1e-6, sigma)\n    end\nend","category":"section"},{"location":"model-building/universal-function-approximators/#Pattern-2:-Full-Parameter-Individualization-via-Random-Effects","page":"Function Approximators (NNs + SoftTrees)","title":"Pattern 2: Full-Parameter Individualization via Random Effects","text":"When the functional form itself is expected to vary across individuals, the entire parameter vector of an approximator can be treated as a random effect. Each individual receives a personalized set of network or tree weights drawn from a multivariate distribution centered on the population-level parameters. This enables fully individualized nonlinear mappings at the cost of a high-dimensional random-effects distribution.\n\nusing NoLimits\nusing Distributions\nusing Lux\nusing LinearAlgebra\n\nchain_A1 = Lux.Chain(Lux.Dense(1, 4, tanh), Lux.Dense(4, 1))\nchain_A2 = Lux.Chain(Lux.Dense(1, 4, tanh), Lux.Dense(4, 1))\n\nmodel = @Model begin\n    @helpers begin\n        softplus(u) = u > 20 ? u : log1p(exp(u))\n    end\n\n    @covariates begin\n        t = Covariate()\n        d = ConstantCovariate(; constant_on=:ID)\n    end\n\n    @fixedEffects begin\n        sigma = RealNumber(0.3, scale=:log)\n        zA1 = NNParameters(chain_A1; function_name=:NNA1, calculate_se=false)\n        zA2 = NNParameters(chain_A2; function_name=:NNA2, calculate_se=false)\n        gC1 = SoftTreeParameters(1, 2; function_name=:STC1, calculate_se=false)\n        gC2 = SoftTreeParameters(1, 2; function_name=:STC2, calculate_se=false)\n    end\n\n    @randomEffects begin\n        etaA1 = RandomEffect(MvNormal(zA1, Diagonal(ones(length(zA1)))); column=:ID)\n        etaA2 = RandomEffect(MvNormal(zA2, Diagonal(ones(length(zA2)))); column=:ID)\n        etaC1 = RandomEffect(MvNormal(gC1, Diagonal(ones(length(gC1)))); column=:ID)\n        etaC2 = RandomEffect(MvNormal(gC2, Diagonal(ones(length(gC2)))); column=:ID)\n    end\n\n    @DifferentialEquation begin\n        a_A(t) = softplus(depot)\n        x_C(t) = softplus(center)\n\n        fA1(t) = softplus(NNA1([t / 24], etaA1)[1])\n        fA2(t) = softplus(NNA2([a_A(t)], etaA2)[1])\n        fC1(t) = -softplus(STC1([x_C(t)], etaC1)[1])\n        fC2(t) = softplus(STC2([t / 24], etaC2)[1])\n\n        D(depot) ~ -d * fA1(t) - fA2(t)\n        D(center) ~ d * fA1(t) + fA2(t) + fC1(t) + d * fC2(t)\n    end\n\n    @initialDE begin\n        depot = d\n        center = 0.0\n    end\n\n    @formulas begin\n        y ~ LogNormal(center(t), sigma)\n    end\nend","category":"section"},{"location":"model-building/universal-function-approximators/#Pattern-3:-Hybrid-Models-Combining-Both-Strategies","page":"Function Approximators (NNs + SoftTrees)","title":"Pattern 3: Hybrid Models Combining Both Strategies","text":"A single model can combine population-level and fully individualized approximators. For instance, one network may capture a shared population-level transformation while another is individualized through random effects. This provides a principled way to decompose variation into components that are common across individuals and components that are subject-specific.\n\nusing NoLimits\nusing Distributions\nusing Lux\nusing LinearAlgebra\n\nchain = Lux.Chain(Lux.Dense(1, 4, tanh), Lux.Dense(4, 1))\n\nmodel = @Model begin\n    @covariates begin\n        t = Covariate()\n        c = ConstantCovariate(; constant_on=:ID)\n    end\n\n    @fixedEffects begin\n        sigma = RealNumber(0.3, scale=:log)\n        z_fix = NNParameters(chain; function_name=:NNfix, calculate_se=false)\n        g_mix = SoftTreeParameters(1, 2; function_name=:STmix, calculate_se=false)\n    end\n\n    @randomEffects begin\n        eta_g = RandomEffect(MvNormal(g_mix, Diagonal(ones(length(g_mix)))); column=:ID)\n    end\n\n    @DifferentialEquation begin\n        D(x1) ~ -abs(NNfix([t / 24], z_fix)[1]) * x1 + abs(STmix([t / 24], eta_g)[1])\n    end\n\n    @initialDE begin\n        x1 = c\n    end\n\n    @formulas begin\n        y ~ Exponential(log1p(x1(t)^2) + sigma)\n    end\nend","category":"section"},{"location":"model-building/universal-function-approximators/#Practical-Notes","page":"Function Approximators (NNs + SoftTrees)","title":"Practical Notes","text":"The function_name keyword controls the callable name used to invoke the approximator in model expressions. Each approximator must have a unique function name.\nLearned parameter blocks are typically declared with calculate_se=false, since standard error computation for high-dimensional parameter vectors is often neither feasible nor informative.\nThe same @Model DSL is used for fixed-effects-only and mixed-effects workflows; only the presence and structure of @randomEffects determines whether individualization occurs.","category":"section"},{"location":"uncertainty-quantification/profile-likelihood/#Profile-Likelihood","page":"Profile likelihood","title":"Profile Likelihood","text":"Profile-likelihood confidence intervals are constructed by examining how the objective function changes as each parameter is varied away from its optimum, while all other parameters are re-optimized at each step. Unlike Wald intervals, which assume a locally quadratic log-likelihood, profile intervals can capture asymmetric uncertainty and are better suited to parameters near boundary constraints or in models with nonlinear reparameterizations. They are considered the gold standard for frequentist confidence intervals in nonlinear models.\n\nIn NoLimits.jl, profile-likelihood UQ is accessed through:\n\ncompute_uq(res; method=:profile, ...)\n\nThe underlying interval computation is performed via LikelihoodProfiler.jl.","category":"section"},{"location":"uncertainty-quantification/profile-likelihood/#Applicability","page":"Profile likelihood","title":"Applicability","text":"Profile UQ is available for fitted results from the following estimation methods:\n\nMLE\nMAP\nLaplace\nLaplaceMAP","category":"section"},{"location":"uncertainty-quantification/profile-likelihood/#Minimal-Usage","page":"Profile likelihood","title":"Minimal Usage","text":"using NoLimits\nusing Random\n\nuq_profile = compute_uq(\n    res;\n    method=:profile,\n    level=0.95,\n    profile_method=:LIN_EXTRAPOL,\n    profile_scan_width=1.0,\n    profile_scan_tol=1e-2,\n    profile_loss_tol=1e-2,\n    profile_max_iter=300,\n    rng=Random.Xoshiro(1),\n)","category":"section"},{"location":"uncertainty-quantification/profile-likelihood/#Core-Controls","page":"Profile likelihood","title":"Core Controls","text":"The following parameters govern the profile-likelihood algorithm and are exposed through compute_uq:\n\nprofile_method (default :LIN_EXTRAPOL): the profiling algorithm used by LikelihoodProfiler.jl.\nprofile_scan_width (must be positive): search window around the point estimate, specified in transformed-coordinate units and subject to parameter bounds.\nprofile_scan_tol: tolerance for the scanning phase of the profiler.\nprofile_loss_tol: tolerance on the objective function difference used to determine the interval boundary.\nprofile_local_alg (default :LN_NELDERMEAD): local optimization algorithm applied near interval endpoints.\nprofile_max_iter: maximum number of iterations for the local optimizer.\nprofile_ftol_abs: absolute function tolerance for the local optimizer.\nprofile_kwargs: additional keyword arguments forwarded directly to the underlying profiler call.\n\nIn practice, profile_scan_width determines how far from the estimate the profiler searches. If intervals appear truncated, increasing this value or raising profile_max_iter may help the profiler locate the true boundary.","category":"section"},{"location":"uncertainty-quantification/profile-likelihood/#Fit-Context-Overrides","page":"Profile likelihood","title":"Fit-Context Overrides","text":"The profile backend accepts the same fit-context overrides available in other UQ backends:\n\nconstants\nconstants_re\npenalty\node_args, ode_kwargs\nserialization\nrng\n\nWhen not provided, stored values from the original fit are reused.","category":"section"},{"location":"uncertainty-quantification/profile-likelihood/#Parameter-Inclusion-Rules","page":"Profile likelihood","title":"Parameter Inclusion Rules","text":"Profile UQ is evaluated only on free fixed-effect coordinates that are eligible for uncertainty calculation.\n\nA coordinate is excluded when:\n\nits fixed-effect block is held constant via constants, or\nits block has calculate_se=false.\n\nIf no eligible coordinates remain, profile UQ raises an error.\n\nfe = @fixedEffects begin\n    a = RealNumber(0.2, calculate_se=true)     # included\n    b = RealNumber(0.1, calculate_se=false)    # excluded\nend","category":"section"},{"location":"uncertainty-quantification/profile-likelihood/#Returned-Quantities","page":"Profile likelihood","title":"Returned Quantities","text":"The result is a UQResult with backend :profile. The available accessors are shown below.\n\nbackend = get_uq_backend(uq_profile)                # :profile\nsource = get_uq_source_method(uq_profile)\nnames = get_uq_parameter_names(uq_profile)\n\nest_nat = get_uq_estimates(uq_profile; scale=:natural)\nest_tr = get_uq_estimates(uq_profile; scale=:transformed)\n\nints_nat = get_uq_intervals(uq_profile; scale=:natural)\nints_tr = get_uq_intervals(uq_profile; scale=:transformed)\n\nV_nat = get_uq_vcov(uq_profile; scale=:natural)     # nothing\ndraws_nat = get_uq_draws(uq_profile; scale=:natural) # nothing\n\ndiag = get_uq_diagnostics(uq_profile)\n\nBecause profile likelihood characterizes uncertainty by tracing the objective function surface rather than by sampling, covariance matrices and draw matrices are not available for this backend. Only interval estimates are returned.","category":"section"},{"location":"uncertainty-quantification/profile-likelihood/#Diagnostics-and-Boundary-Behavior","page":"Profile likelihood","title":"Diagnostics and Boundary Behavior","text":"get_uq_diagnostics(uq_profile) returns profiler metadata that is useful for assessing the quality of the computed intervals:\n\nAlgorithm settings: profile_method, tolerances, and local algorithm used.\nObjective values: loss_at_estimate and loss_critical (the threshold corresponding to the requested confidence level).\nPer-parameter endpoint status: left_status and right_status indicate whether each boundary was successfully located.\nPer-parameter endpoint counters: left_counter and right_counter report the number of profiler evaluations at each boundary.\nEndpoint availability: endpoint_found flags whether both interval endpoints were determined.\nPer-parameter errors: errors captures any profiler-level issues encountered during computation.\n\nThese diagnostics are essential for identifying incomplete or numerically unstable intervals. If an interval endpoint was not found, common remedies include widening profile_scan_width, increasing profile_max_iter, or relaxing profile_loss_tol.","category":"section"},{"location":"model-building/formulas/#@formulas","page":"@formulas","title":"@formulas","text":"The @formulas block specifies the observation model that links latent quantities to measured data. It defines deterministic intermediate expressions and the statistical distributions assumed for each observed outcome. Within @Model, this block is required.","category":"section"},{"location":"model-building/formulas/#Core-Syntax","page":"@formulas","title":"Core Syntax","text":"Two statement forms are supported inside @formulas:\n\nDeterministic assignment: name = expression defines an intermediate quantity computed from model components.\nObservation definition: name ~ distribution_expression declares a likelihood contribution for a named outcome column.\n\nform = @formulas begin\n    lin = a + eta^2 + x.Age\n    y ~ Laplace(lin, sigma)\nend","category":"section"},{"location":"model-building/formulas/#Parsing-and-Validation-Rules","page":"@formulas","title":"Parsing and Validation Rules","text":"The macro enforces the following constraints at model-construction time:\n\nThe block must use begin ... end syntax.\nOnly assignments (=) and observation statements (~) are permitted.\nThe left-hand side of each statement must be a plain symbol.\nThe reserved symbols t and ξ cannot appear on the left-hand side.\nDuplicate names within deterministic definitions, within observation definitions, or across both categories are rejected.","category":"section"},{"location":"model-building/formulas/#Symbol-Resolution-and-Namespace-Rules","page":"@formulas","title":"Symbol Resolution and Namespace Rules","text":"Expressions inside @formulas can reference symbols drawn from several model namespaces:\n\nFixed effects\nRandom effects\nPreDE outputs\nConstant covariates\nVarying covariates\nHelper functions (@helpers)\nModel functions from learned parameter blocks (e.g., neural networks, soft trees)\n\nIf a symbol name is shared across more than one namespace among fixed effects, random effects, preDE outputs, and covariates, the macro raises an ambiguity error to prevent silent resolution conflicts.","category":"section"},{"location":"model-building/formulas/#State-and-Signal-Access-from-ODE-Models","page":"@formulas","title":"State and Signal Access from ODE Models","text":"When the model includes a differential equation system, ODE states and derived signals can be referenced in formulas subject to the following rules:\n\nStates and signals must be called with an explicit time argument, e.g., x1(t) or s(t).\nBare references without a time argument (e.g., x1, s) are rejected.\nThe set of required states and signals is inferred automatically and stored as required_states and required_signals.\n\nAt runtime, formulas that depend on ODE outputs receive solution accessor functions generated by get_de_accessors_builder(...).","category":"section"},{"location":"model-building/formulas/#Time-Offsets-in-State-and-Signal-Calls","page":"@formulas","title":"Time Offsets in State and Signal Calls","text":"Formulas support constant time offsets when accessing ODE states and signals, enabling evaluation at shifted time points:\n\nx1(t + 0.25)\nx1(t - 0.5)\nx1(t + (1/4))\n\nThese offsets are exposed through get_formulas_time_offsets(...) and handled automatically during DataModel construction:\n\nConstant offsets extend the integration window and save grid as needed.\nNegative offsets that would require evaluation before the start of an individual's trajectory are rejected.\nNon-constant offsets (e.g., expressions involving covariates) require dense ODE solving mode.","category":"section"},{"location":"model-building/formulas/#Example:-Multiple-Deterministic-Nodes-and-Outcomes","page":"@formulas","title":"Example: Multiple Deterministic Nodes and Outcomes","text":"The following model illustrates the use of multiple intermediate deterministic expressions feeding into two distinct outcome distributions.\n\nusing NoLimits\nusing Distributions\n\nmodel = @Model begin\n    @fixedEffects begin\n        a = RealNumber(0.2)\n        b = RealNumber(0.1)\n        s1 = RealNumber(0.3, scale=:log)\n        s2 = RealNumber(0.4, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n        x = ConstantCovariateVector([:Age, :BMI]; constant_on=:ID)\n    end\n\n    @randomEffects begin\n        eta = RandomEffect(TDist(6.0); column=:ID)\n    end\n\n    @formulas begin\n        d1 = a + 0.01 * x.Age^2 + tanh(eta)\n        d2 = d1 + b * log1p(x.BMI^2) + eta^2\n        y1 ~ LogNormal(d2, s1)\n        y2 ~ Gamma(d1^2 + abs(eta) + 1e-6, s2)\n    end\nend","category":"section"},{"location":"model-building/formulas/#Example:-Helper-Functions-and-Learned-Model-Functions","page":"@formulas","title":"Example: Helper Functions and Learned Model Functions","text":"User-defined helpers and learned parameter blocks (such as neural networks) can be called directly within formula expressions.\n\nusing NoLimits\nusing Distributions\nusing Lux\n\nchain = Chain(Dense(2, 4, tanh), Dense(4, 1))\n\nmodel = @Model begin\n    @helpers begin\n        softplus(u) = log1p(exp(u))\n    end\n\n    @fixedEffects begin\n        sigma = RealNumber(0.4, scale=:log)\n        z = NNParameters(chain; function_name=:NN1, calculate_se=false)\n    end\n\n    @covariates begin\n        t = Covariate()\n        x = ConstantCovariateVector([:Age, :BMI]; constant_on=:ID)\n    end\n\n    @randomEffects begin\n        eta = RandomEffect(SkewNormal(0.0, 1.0, 0.8); column=:ID)\n    end\n\n    @formulas begin\n        lin = NN1([x.Age, x.BMI], z)[1] + softplus(eta^2)\n        y ~ Gamma(lin + 1e-6, sigma)\n    end\nend","category":"section"},{"location":"model-building/formulas/#Example:-ODE-State-and-Signal-Access","page":"@formulas","title":"Example: ODE State and Signal Access","text":"When a model includes @DifferentialEquation, states and derived signals are accessed in formulas using explicit time arguments. Time offsets such as x1(t + 0.25) enable evaluation at shifted time points.\n\nusing NoLimits\nusing Distributions\n\nmodel = @Model begin\n    @fixedEffects begin\n        a = RealNumber(0.2)\n        sigma = RealNumber(0.3, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n    end\n\n    @DifferentialEquation begin\n        s(t) = sin(t)\n        D(x1) ~ -a * x1 + s(t)\n    end\n\n    @initialDE begin\n        x1 = 1.0\n    end\n\n    @formulas begin\n        mu = x1(t) + s(t) + x1(t + 0.25)\n        y ~ Exponential(log1p(abs(mu)) + 1e-6)\n    end\nend","category":"section"},{"location":"model-building/formulas/#Example:-Hidden-Markov-Observation-Model","page":"@formulas","title":"Example: Hidden Markov Observation Model","text":"For data generated by regime-switching processes, @formulas supports hidden Markov model likelihoods via ContinuousTimeDiscreteStatesHMM. The ContinuousTransitionMatrix parameter type provides an AD-compatible rate matrix where off-diagonal entries are constrained to be non-negative and the diagonal is always derived as minus the row sum.\n\nusing NoLimits\nusing Distributions\n\nmodel = @Model begin\n    @fixedEffects begin\n        Q  = ContinuousTransitionMatrix([-0.2  0.2; 0.3  -0.3])\n        p1 = RealNumber(0.25)\n        p2 = RealNumber(0.75)\n    end\n\n    @covariates begin\n        t = Covariate()\n        delta_t = Covariate()\n    end\n\n    @formulas begin\n        outcome ~ ContinuousTimeDiscreteStatesHMM(\n            Q,\n            (Bernoulli(p1), Bernoulli(p2)),\n            Categorical([0.6, 0.4]),\n            delta_t\n        )\n    end\nend","category":"section"},{"location":"model-building/formulas/#Example:-Multivariate-Hidden-Markov-Observation-Model","page":"@formulas","title":"Example: Multivariate Hidden Markov Observation Model","text":"When each observation consists of several outcome variables that share the same hidden state, use MVDiscreteTimeDiscreteStatesHMM or MVContinuousTimeDiscreteStatesHMM. The observable column in the DataFrame should contain Vector{<:Real} values, one vector per observation time.\n\nTwo emission modes are supported:\n\nConditionally independent: the emission for state k is a Tuple of M scalar distributions, one per outcome. Missing entries in the observation vector are skipped (contribute zero log-likelihood).\nJoint MvNormal: the emission for state k is a single MvNormal. Partial missings are handled by marginalising analytically over the observed indices.","category":"section"},{"location":"model-building/formulas/#Discrete-time,-conditionally-independent-emissions","page":"@formulas","title":"Discrete time, conditionally independent emissions","text":"The transition matrix is row-stochastic (each row sums to one). The outer Tuple contains one inner Tuple of scalar distributions per state. DiscreteTransitionMatrix handles the row-stochastic constraint automatically via the logistic stick-breaking transform.\n\nusing NoLimits\nusing Distributions\n\nmodel = @Model begin\n    @fixedEffects begin\n        P     = DiscreteTransitionMatrix([0.7 0.3; 0.2 0.8])\n        mu1   = RealNumber(1.0)\n        mu2   = RealNumber(3.0)\n        sigma = RealNumber(0.5, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n    end\n\n    @formulas begin\n        outcome ~ MVDiscreteTimeDiscreteStatesHMM(\n            P,\n            (\n                (Normal(mu1, sigma), Bernoulli(0.2)),  # state 1: (continuous, binary)\n                (Normal(mu2, sigma), Bernoulli(0.8)),  # state 2: (continuous, binary)\n            ),\n            Categorical([0.5, 0.5])\n        )\n    end\nend\n\nThe outcome column in the DataFrame must hold two-element vectors (one Float64 and one 0.0/1.0) matching the M=2 outcomes declared by the emission tuples.","category":"section"},{"location":"model-building/formulas/#Continuous-time,-joint-MvNormal-emissions","page":"@formulas","title":"Continuous time, joint MvNormal emissions","text":"The transition matrix is a rate matrix (generator): off-diagonal entries ≥ 0, each row sums to zero. State propagation uses the matrix exponential exp(Q · Δt). Joint MvNormal emissions enable full cross-outcome correlation within each state. ContinuousTransitionMatrix enforces all rate-matrix constraints and exposes the off-diagonal rates as log-transformed free parameters.\n\nusing NoLimits\nusing Distributions\nusing LinearAlgebra\n\nmodel = @Model begin\n    @fixedEffects begin\n        Q   = ContinuousTransitionMatrix([-0.2  0.2; 0.3  -0.3])\n        mu1 = RealNumber(0.0)\n        mu2 = RealNumber(2.0)\n    end\n\n    @covariates begin\n        t       = Covariate()\n        delta_t = Covariate()\n    end\n\n    @formulas begin\n        outcome ~ MVContinuousTimeDiscreteStatesHMM(\n            Q,\n            (\n                MvNormal([mu1, mu1], I(2)),\n                MvNormal([mu2, mu2], I(2)),\n            ),\n            Categorical([0.5, 0.5]),\n            delta_t\n        )\n    end\nend\n\nHere delta_t is a time-varying covariate holding the elapsed time since the previous observation for each row. The outcome column must hold two-element vectors matching the dimension of the MvNormal emissions. Partially-missing vectors (e.g. [1.2, missing]) are handled by marginalising the MvNormal over the observed index.","category":"section"},{"location":"model-building/formulas/#Related-APIs","page":"@formulas","title":"Related APIs","text":"The following functions provide programmatic access to the internal representation and evaluation of formulas:\n\nget_formulas_meta(formulas)\nget_formulas_ir(formulas)\nget_formulas_builders(formulas; ...)\nget_formulas_all(formulas, ctx, sol_accessors, constant_covariates_i, varying_covariates; ...)\nget_formulas_obs(formulas, ctx, sol_accessors, constant_covariates_i, varying_covariates; ...)\nget_formulas_time_offsets(formulas, state_names, signal_names)","category":"section"},{"location":"nlme-methodology/#NLME-Methodology","page":"NLME Methodology","title":"NLME Methodology","text":"This page outlines the methodological framework underlying nonlinear mixed-effects (NLME) models as implemented in NoLimits.jl. It provides a compact mathematical reference for the model structure, likelihood formulation, and estimation targets. Package-specific algorithmic details for each estimation method are documented on their respective pages.","category":"section"},{"location":"nlme-methodology/#Notation","page":"NLME Methodology","title":"Notation","text":"Symbol Description\n(i = 1, \\dots, N) Index over individuals (or higher-level observational units)\n(j = 1, \\dots, n_i) Index over observations within individual (i)\n(t_{ij}) Observation time (or general indexing coordinate)\n(y_{ij}) Observed outcome\n(\\theta) Fixed effects (population-level parameters)\n(\\eta_i) Individual-level random effects\n(x_{ij}) Observation-level (time-varying) covariates\n(z_i) Group-level or time-invariant covariates","category":"section"},{"location":"nlme-methodology/#Hierarchical-Model-Structure","page":"NLME Methodology","title":"Hierarchical Model Structure","text":"An NLME model consists of three components: a structural model describing the underlying process, a random-effects model capturing between-individual variability, and an observation model linking latent predictions to measured data.","category":"section"},{"location":"nlme-methodology/#Structural-Model","page":"NLME Methodology","title":"Structural Model","text":"The structural process for individual (i) is defined by a nonlinear mapping\n\nf_i(t theta eta_i x_i z_i)\n\nwhich may be algebraic (a closed-form function of time and parameters) or dynamic (the solution of an ODE system). In the dynamic case, the structural component is governed by\n\nfracd u_i(t)dt = gleft(u_i(t) t theta eta_i x_i(t) z_iright) quad\nu_i(t_0) = u_i0(theta eta_i z_i)\n\nwhere predictions used in the observation model are derived from the state trajectory (u_i(t)) and optional derived signals.","category":"section"},{"location":"nlme-methodology/#Random-Effects-Model","page":"NLME Methodology","title":"Random-Effects Model","text":"Between-individual variability is represented by\n\neta_i sim p_eta(cdot mid theta z_i)\n\nwhere (p_\\eta) may be Gaussian or non-Gaussian. In NoLimits.jl, the random-effects distribution can depend on fixed effects, group-level covariates, and learned nonlinear functions, enabling flexible covariate-dependent heterogeneity.","category":"section"},{"location":"nlme-methodology/#Observation-Model","page":"NLME Methodology","title":"Observation Model","text":"Observed data are drawn from\n\ny_ij sim p_yleft(cdot mid f_i(t_ij theta eta_i x_ij z_i) thetaright)\n\nThe observation distribution (p_y) can be any distribution from the Distributions.jl ecosystem – continuous (e.g., Normal, LogNormal), discrete (e.g., Poisson, Bernoulli), or structured (e.g., hidden Markov models).","category":"section"},{"location":"nlme-methodology/#Likelihood","page":"NLME Methodology","title":"Likelihood","text":"","category":"section"},{"location":"nlme-methodology/#Individual-Contribution","page":"NLME Methodology","title":"Individual Contribution","text":"Conditioned on the random effects (\\eta_i), the contribution from individual (i) is\n\nL_i(theta eta_i) = prod_j=1^n_i p_yleft(y_ijmid f_i(t_ij theta eta_i x_ij z_i) thetaright) p_eta(eta_i mid theta z_i)","category":"section"},{"location":"nlme-methodology/#Marginal-Population-Likelihood","page":"NLME Methodology","title":"Marginal Population Likelihood","text":"The marginal likelihood integrates over the random effects:\n\nL(theta) = prod_i=1^N int L_i(theta eta_i) deta_i\n\nThis integral is generally intractable for nonlinear models and must be approximated. The estimation methods in NoLimits.jl use different strategies:\n\nLaplace approximation replaces each integral with a second-order expansion around the empirical Bayes mode.\nMCEM and SAEM use Monte Carlo samples from the conditional distribution of (\\eta_i) to approximate the E-step of an EM algorithm.\nMCMC targets the full joint posterior (p(\\theta, \\eta \\mid y)) directly.\n\nThe objective may be likelihood-based or posterior-based, depending on the inference mode selected by the user.","category":"section"},{"location":"nlme-methodology/#Multi-Outcome-and-Hidden-State-Extensions","page":"NLME Methodology","title":"Multi-Outcome and Hidden-State Extensions","text":"For models with (K) outcomes, the observation vector at each time point becomes\n\nmathbfy_ij = (y_ij^(1) dots y_ij^(K))\n\nand the observation model may factorize across outcomes or use a joint distribution. Hidden-state formulations introduce latent discrete processes with state-dependent emission distributions.","category":"section"},{"location":"nlme-methodology/#Covariate-Effects","page":"NLME Methodology","title":"Covariate Effects","text":"Covariates can enter at three levels:\n\nStructural dynamics – modifying the deterministic model or ODE right-hand side.\nObservation model – affecting distribution parameters directly.\nRandom-effect distributions – modulating the location, scale, or shape of between-individual variability.\n\nThis flexibility enables both mean-structure and variability-structure covariate effects within a single model.","category":"section"},{"location":"nlme-methodology/#Estimation-and-Inference-Targets","page":"NLME Methodology","title":"Estimation and Inference Targets","text":"The primary targets of estimation are:\n\nPoint estimates of the fixed effects (\\theta).\nEmpirical Bayes estimates or posterior distributions for individual random effects (\\eta_i).\nUncertainty quantification for (\\theta) on both transformed and natural parameter scales.\n\nDetails on each estimation method and the available uncertainty quantification backends are provided in the Estimation and Uncertainty Quantification sections.","category":"section"},{"location":"tutorials/mixed-effects-seizure-counts-poisson-nb-mcem/#Mixed-Effects-Tutorial-5:-Modeling-Seizure-Counts-with-Poisson-and-Negative-Binomial-Outcomes","page":"Mixed-Effects Tutorial 5: Seizure Counts with Poisson and NegativeBinomial Outcomes (MCEM)","title":"Mixed-Effects Tutorial 5: Modeling Seizure Counts with Poisson and Negative Binomial Outcomes","text":"Counting events over time – seizures, infections, adverse reactions – is one of the most common tasks in longitudinal clinical research. Yet count data behave very differently from continuous measurements. Counts are non-negative integers, their variance typically grows with their mean, and they are often right-skewed. Applying a Normal likelihood to such data can produce negative predicted counts, biased standard errors, and misleading inferences. The Poisson and Negative Binomial distributions are the natural statistical models for these outcomes because they respect the discrete, non-negative nature of counts and link the variance directly to the mean.\n\nIn this tutorial, you will build and compare two nonlinear mixed-effects models for repeated seizure counts, using data from the Thall and Vail (1990) epilepsy trial. This trial is one of the most widely analyzed longitudinal count datasets in biostatistics: 59 epilepsy patients were randomized to either progabide or placebo, and seizure counts were recorded across four consecutive two-week periods following a baseline observation window. The dataset has become a benchmark for count regression methods because it exhibits substantial between-subject variability and overdispersion – features that push simple Poisson models to their limits and motivate the Negative Binomial alternative.\n\nBoth models share the same fixed-effects covariates and subject-level random-effects structure; they differ only in the outcome distribution. The Poisson assumes the variance equals the mean, while the Negative Binomial introduces a dispersion parameter to accommodate extra-Poisson variation. Because the subject-specific random effect enters inside an exponential rate function, both models are nonlinear in the random effects. This nonlinearity means the marginal likelihood integral over random effects has no closed-form solution, making Monte Carlo Expectation-Maximization (MCEM) a natural estimation strategy.","category":"section"},{"location":"tutorials/mixed-effects-seizure-counts-poisson-nb-mcem/#Learning-Goals","page":"Mixed-Effects Tutorial 5: Seizure Counts with Poisson and NegativeBinomial Outcomes (MCEM)","title":"Learning Goals","text":"By the end of this tutorial, you will be able to:\n\nPrepare longitudinal count data – reshape the classic MASS epil dataset into the long format NoLimits expects, deriving subject-level covariates (baseline seizure rate, age, treatment) and time-varying design variables (period, treatment-by-period interaction).\nSpecify count regression models – define both a Poisson and a Negative Binomial mixed-effects model in NoLimits, each with a log-link linear predictor and a subject-level random intercept.\nEstimate with MCEM – fit both models using Monte Carlo Expectation-Maximization, an algorithm that alternates between sampling random effects from their conditional distribution and optimizing the fixed-effects parameters.\nVisualize model fit – generate fitted-trajectory plots and observation-level predictive distributions using plot_fits and plot_observation_distributions.\nQuantify uncertainty – compute Wald-based confidence intervals with compute_uq and produce publication-ready summary tables with NoLimits.summarize.","category":"section"},{"location":"tutorials/mixed-effects-seizure-counts-poisson-nb-mcem/#Step-1:-Data-Setup","page":"Mixed-Effects Tutorial 5: Seizure Counts with Poisson and NegativeBinomial Outcomes (MCEM)","title":"Step 1: Data Setup","text":"In this step, you will load the MASS epilepsy dataset (MASS::epil, mirrored in Rdatasets) and reshape it into the long format that NoLimits expects, where each row represents one subject-period combination. Along the way, you will derive several analysis variables: a binary treatment indicator, log-transformed baseline seizure count and age (to stabilize the scale of the linear predictor), and a centered period variable that improves interpretability of the intercept. The treatment-by-period interaction term will allow you to test whether the treatment effect changes over the course of the trial.\n\nusing NoLimits\nusing CSV\nusing DataFrames\nusing Distributions\nusing Downloads\nusing LinearAlgebra\nusing Random\nusing SciMLBase\n\ninclude(joinpath(@__DIR__, \"_data_loaders.jl\"))\n\nRandom.seed!(2026)\n\nfunction build_epilepsy_long_df(tbl::DataFrame)\n    df = DataFrame(\n        Subject=Int.(tbl.subject),\n        Period=Int.(tbl.period),\n        Trt=string.(tbl.trt),\n        Base=Int.(tbl.base),\n        Age=Int.(tbl.age),\n        seizures=Int.(tbl.y),\n    )\n\n    sort!(df, [:Subject, :Period])\n    df.trt_active = ifelse.(df.Trt .== \"progabide\", 1.0, 0.0)\n    df.base_log = log1p.(float.(df.Base))\n    df.age_log = log1p.(float.(df.Age))\n    df.period_f = float.(df.Period)\n    df.period_centered = df.period_f .- 1.0\n    df.trt_period_centered = df.trt_active .* df.period_centered\n\n    return df\nend\n\nepil_df = load_epil()\ndf = build_epilepsy_long_df(epil_df)\nfirst(df, 10)","category":"section"},{"location":"tutorials/mixed-effects-seizure-counts-poisson-nb-mcem/#Step-2:-Poisson-Mixed-Effects-Model","page":"Mixed-Effects Tutorial 5: Seizure Counts with Poisson and NegativeBinomial Outcomes (MCEM)","title":"Step 2: Poisson Mixed-Effects Model","text":"In this step, you will define the Poisson mixed-effects model. The core idea is a log-linear predictor: the log of the expected seizure rate is modeled as a linear combination of covariates plus a subject-level random intercept. The random effect eta captures unmeasured between-subject heterogeneity in baseline seizure propensity. Because eta appears inside the exponential that maps the log-rate to the Poisson intensity parameter lambda, the model is nonlinear in the random effects – a key reason MCEM is preferred over simpler estimation methods such as the Laplace approximation.\n\nNote the two helper functions defined in the @helpers block: linpred computes the dot product of the covariate vector with the coefficient vector (keeping the model specification clean), and safe_exp guards against numerical overflow in the exponential.\n\nmodel_poisson = @Model begin\n    @helpers begin\n        safe_exp(x) = exp(clamp(x, -20.0, 20.0))\n        linpred(x, β) = dot(\n            [x.base_log, x.age_log, x.trt_active, x.period_centered, x.trt_period_centered],\n            β,\n        )\n    end\n\n    @covariates begin\n        period_f = Covariate()\n        x = CovariateVector([:base_log, :age_log, :trt_active, :period_centered, :trt_period_centered])\n    end\n\n    @fixedEffects begin\n        beta0 = RealNumber(0.1, calculate_se=true)\n        beta = RealVector([0.5, -0.1, -0.2, -0.05, -0.04], calculate_se=true)\n        omega = RealNumber(0.5, scale=:log, calculate_se=true)\n    end\n\n    @randomEffects begin\n        eta = RandomEffect(Normal(0.0, omega); column=:Subject)\n    end\n\n    @formulas begin\n        log_rate = beta0 + linpred(x, beta) + eta\n        lambda = safe_exp(log_rate)\n        seizures ~ Poisson(lambda)\n    end\nend\n\nNoLimits.summarize(model_poisson)","category":"section"},{"location":"tutorials/mixed-effects-seizure-counts-poisson-nb-mcem/#Build-DataModel-and-Configure-MCEM","page":"Mixed-Effects Tutorial 5: Seizure Counts with Poisson and NegativeBinomial Outcomes (MCEM)","title":"Build DataModel and Configure MCEM","text":"Next, you will bind the model to the data by constructing a DataModel. This step validates that all required columns are present and correctly typed, groups observations by subject, and prepares the internal data structures for estimation.\n\nYou will then configure the MCEM algorithm. The sample_schedule controls how many MCMC samples are drawn from the conditional distribution of the random effects at each EM iteration – starting with fewer samples and increasing over iterations improves computational efficiency. The settings here are intentionally compact for documentation purposes; in practice, you would use more iterations and larger sample sizes for production analyses.\n\ndm_poisson = DataModel(model_poisson, df; primary_id=:Subject, time_col=:period_f)\n\nmcem_method = NoLimits.MCEM(;\n    maxiters=4,\n    sample_schedule=i -> min(20 + 10 * (i - 1), 60),\n    turing_kwargs=(n_samples=20, n_adapt=10, progress=false),\n    optim_kwargs=(maxiters=80,),\n    progress=false,\n)\n\nserialization = SciMLBase.EnsembleThreads()\n\nNoLimits.summarize(dm_poisson)","category":"section"},{"location":"tutorials/mixed-effects-seizure-counts-poisson-nb-mcem/#Fit,-Summarize,-Plot,-and-UQ-(Poisson)","page":"Mixed-Effects Tutorial 5: Seizure Counts with Poisson and NegativeBinomial Outcomes (MCEM)","title":"Fit, Summarize, Plot, and UQ (Poisson)","text":"You are now ready to fit the Poisson model. The fit_model call runs the full MCEM loop: at each iteration, it samples random effects conditional on the current fixed-effects estimates, then updates the fixed effects by maximizing the Monte Carlo approximation to the marginal likelihood. After fitting, you will inspect a summary of the estimated parameters.\n\nres_poisson = fit_model(\n    dm_poisson,\n    mcem_method;\n    serialization=serialization,\n    rng=Random.Xoshiro(21),\n)\n\nNoLimits.summarize(res_poisson)\n\nTo assess how well the model captures individual seizure trajectories, you will now plot fitted values against observed data for the first two subjects. The observation-distribution diagnostic reveals the full predictive distribution at selected time points – a particularly informative view for count data, where the shape of the distribution (not just its mean) carries clinical significance.\n\np_fit_poisson = plot_fits(\n    res_poisson;\n    observable=:seizures,\n    individuals_idx=[1, 2],\n    ncols=2,\n    shared_x_axis=true,\n    shared_y_axis=true,\n)\n\np_obs_poisson = plot_observation_distributions(\n    res_poisson;\n    observables=:seizures,\n    individuals_idx=1,\n    obs_rows=[1, 2],\n)\n\np_fit_poisson\n\nDisplay the observation-distribution plot to examine the predicted probability mass function at individual time points.\n\np_obs_poisson\n\nNext, you will compute Wald-based confidence intervals for the fixed-effects parameters. The Wald method uses the curvature of the log-likelihood at the optimum to approximate the sampling distribution of each parameter estimate, yielding standard errors and 95% confidence intervals without requiring additional model fits.\n\nuq_poisson = compute_uq(\n    res_poisson;\n    method=:wald,\n    n_draws=100,\n    level=0.95,\n)\n\nNoLimits.summarize(uq_poisson)\n\nFor a consolidated view, combine the parameter estimates and their uncertainty into a single summary table – the format most convenient for reporting in manuscripts and presentations.\n\nNoLimits.summarize(res_poisson, uq_poisson)\n\nFinally, visualize the approximate sampling distributions of the fixed-effects parameters implied by the Wald approximation.\n\nplot_uq_distributions(uq_poisson)","category":"section"},{"location":"tutorials/mixed-effects-seizure-counts-poisson-nb-mcem/#Step-3:-Negative-Binomial-Mixed-Effects-Model","page":"Mixed-Effects Tutorial 5: Seizure Counts with Poisson and NegativeBinomial Outcomes (MCEM)","title":"Step 3: Negative Binomial Mixed-Effects Model","text":"The Poisson model assumes that the conditional variance of seizure counts equals the conditional mean. In practice, count data from clinical trials frequently exhibit overdispersion – more variability than the Poisson predicts – due to unmeasured heterogeneity beyond what the random intercept alone can capture. The Negative Binomial distribution addresses this limitation by introducing an additional dispersion parameter r (sometimes called the \"size\" or \"shape\" parameter). As r grows large, the Negative Binomial converges to the Poisson; smaller values of r indicate greater overdispersion. This makes the Negative Binomial a strict generalization of the Poisson, and comparing the two reveals whether overdispersion is a meaningful feature of the data.\n\nIn this step, you will define the Negative Binomial model. It retains the same structural predictor and random-effects hierarchy as the Poisson model above. The only additions are the dispersion parameter log_r (estimated on the log scale to ensure positivity) and the formula lines that convert the mean-scale rate lambda and size r into the (r, p) parameterization expected by Distributions.NegativeBinomial.\n\nmodel_nb = @Model begin\n    @helpers begin\n        safe_exp(x) = exp(clamp(x, -20.0, 20.0))\n        linpred(x, β) = dot(\n            [x.base_log, x.age_log, x.trt_active, x.period_centered, x.trt_period_centered],\n            β,\n        )\n    end\n\n    @covariates begin\n        period_f = Covariate()\n        x = CovariateVector([:base_log, :age_log, :trt_active, :period_centered, :trt_period_centered])\n    end\n\n    @fixedEffects begin\n        beta0 = RealNumber(0.1, calculate_se=true)\n        beta = RealVector([0.5, -0.1, -0.2, -0.05, -0.04], calculate_se=true)\n        omega = RealNumber(0.5, scale=:log, calculate_se=true)\n        log_r = RealNumber(log(5.0), calculate_se=true)\n    end\n\n    @randomEffects begin\n        eta = RandomEffect(Normal(0.0, omega); column=:Subject)\n    end\n\n    @formulas begin\n        log_rate = beta0 + linpred(x, beta) + eta\n        lambda = safe_exp(log_rate)\n        r = exp(log_r) + 1e-6\n        p = clamp(r / (r + lambda), 1e-8, 1.0 - 1e-8)\n        seizures ~ NegativeBinomial(r, p)\n    end\nend\n\nNoLimits.summarize(model_nb)","category":"section"},{"location":"tutorials/mixed-effects-seizure-counts-poisson-nb-mcem/#Build-DataModel,-Fit,-Summarize,-Plot,-and-UQ-(Negative-Binomial)","page":"Mixed-Effects Tutorial 5: Seizure Counts with Poisson and NegativeBinomial Outcomes (MCEM)","title":"Build DataModel, Fit, Summarize, Plot, and UQ (Negative Binomial)","text":"You will now follow the same workflow as for the Poisson model: construct the DataModel, fit with identical MCEM settings, and inspect the results. Using the same algorithm configuration for both models ensures that any differences in fit quality reflect genuine differences in model adequacy rather than tuning artifacts.\n\ndm_nb = DataModel(model_nb, df; primary_id=:Subject, time_col=:period_f)\n\nres_nb = fit_model(\n    dm_nb,\n    mcem_method;\n    serialization=serialization,\n    rng=Random.Xoshiro(22),\n)\n\nNoLimits.summarize(dm_nb)\nNoLimits.summarize(res_nb)\n\nGenerate the same fitted-trajectory and observation-distribution diagnostics as before. Comparing these plots side-by-side with the Poisson versions will reveal whether the Negative Binomial's wider predictive intervals better capture the observed variability in seizure counts.\n\np_fit_nb = plot_fits(\n    res_nb;\n    observable=:seizures,\n    individuals_idx=[1, 2],\n    ncols=2,\n    shared_x_axis=true,\n    shared_y_axis=true,\n)\n\np_obs_nb = plot_observation_distributions(\n    res_nb;\n    observables=:seizures,\n    individuals_idx=1,\n    obs_rows=[1, 2],\n)\n\np_fit_nb\n\nDisplay the Negative Binomial observation-distribution plot for direct comparison with the Poisson version above. Pay attention to the width of the predicted probability mass – the Negative Binomial should assign more probability to extreme counts.\n\np_obs_nb\n\nCompute Wald-based uncertainty for the Negative Binomial model and produce both standalone and combined summary tables, following the same procedure as for the Poisson fit.\n\nuq_nb = compute_uq(\n    res_nb;\n    method=:wald,\n    n_draws=100,\n    level=0.95,\n)\n\nNoLimits.summarize(uq_nb)\nNoLimits.summarize(res_nb, uq_nb)\n\nThe uncertainty distribution plots for the Negative Binomial model now include the dispersion parameter log_r – a parameter with no counterpart in the Poisson model, whose magnitude directly quantifies the degree of overdispersion in the data.\n\nplot_uq_distributions(uq_nb)","category":"section"},{"location":"tutorials/mixed-effects-seizure-counts-poisson-nb-mcem/#Step-4:-Comparing-Poisson-and-Negative-Binomial-Objectives","page":"Mixed-Effects Tutorial 5: Seizure Counts with Poisson and NegativeBinomial Outcomes (MCEM)","title":"Step 4: Comparing Poisson and Negative Binomial Objectives","text":"As a final diagnostic, you will compare the objective function values (negative log-likelihoods) from the two fits. A word of caution: because the Poisson is a limiting case of the Negative Binomial (as r tends to infinity) rather than a strict parameter restriction, these values are not directly comparable via a standard likelihood ratio test. Nevertheless, a substantially lower objective for the Negative Binomial strongly suggests that the data exhibit overdispersion the Poisson cannot accommodate. Formal model comparison for this class of count models would require information criteria (AIC, BIC) or cross-validation, which are beyond the scope of this tutorial.\n\n(\n    poisson_objective = NoLimits.get_objective(res_poisson),\n    nb_objective = NoLimits.get_objective(res_nb),\n)\n\nKeep in mind that these two objective values arise from different likelihood families. They provide a useful heuristic comparison, but definitive model selection requires the formal tools mentioned above.","category":"section"},{"location":"estimation/mle/#MLE","page":"MLE","title":"MLE","text":"Maximum likelihood estimation (MLE) is the standard frequentist approach to parameter estimation. It finds the fixed-effect values that make the observed data most probable under the assumed model. In NoLimits.jl, MLE supports nonlinear models, ODE-based dynamics, and non-Gaussian outcome distributions, provided that the model contains only fixed effects (no random effects).","category":"section"},{"location":"estimation/mle/#Applicability","page":"MLE","title":"Applicability","text":"Applicable only to models without random effects.\nThe model must declare at least one fixed effect.\nAt least one fixed effect must remain free (i.e., not all parameters may be held constant via constants).\n\nIf the model includes random effects, MLE will raise an error. Use a mixed-effects method such as Laplace, SAEM, or MCEM instead.","category":"section"},{"location":"estimation/mle/#Basic-Usage","page":"MLE","title":"Basic Usage","text":"The following example defines a simple nonlinear model with an exponential mean function and fits it to a small dataset.\n\nusing NoLimits\nusing DataFrames\nusing Distributions\n\nmodel = @Model begin\n    @covariates begin\n        t = Covariate()\n        z = Covariate()\n    end\n\n    @fixedEffects begin\n        a = RealNumber(0.1)\n        b = RealNumber(0.2)\n        sigma = RealNumber(0.5, scale=:log)\n    end\n\n    @formulas begin\n        mu = exp(a + b * z)\n        y ~ Exponential(mu * sigma)\n    end\nend\n\ndf = DataFrame(\n    ID = [1, 1, 2, 2],\n    t = [0.0, 1.0, 0.0, 1.0],\n    z = [0.0, 0.5, 1.0, 1.5],\n    y = [1.0, 1.1, 1.3, 1.7],\n)\n\ndm = DataModel(model, df; primary_id=:ID, time_col=:t)\nres = fit_model(dm, NoLimits.MLE())","category":"section"},{"location":"estimation/mle/#Constructor-Options","page":"MLE","title":"Constructor Options","text":"The MLE constructor exposes options for the optimizer, automatic differentiation backend, and optional parameter bounds.\n\nusing NoLimits\nusing Optimization\nusing OptimizationOptimJL\nusing LineSearches\n\nmethod = NoLimits.MLE(;\n    optimizer=OptimizationOptimJL.LBFGS(linesearch=LineSearches.BackTracking()),\n    optim_kwargs=NamedTuple(),\n    adtype=Optimization.AutoForwardDiff(),\n    lb=nothing,\n    ub=nothing,\n)","category":"section"},{"location":"estimation/mle/#Option-Groups","page":"MLE","title":"Option Groups","text":"Group Keywords Description\nOptimization optimizer, optim_kwargs, adtype Controls fixed-effect objective optimization via Optimization.jl.\nBounds lb, ub Optional transformed-scale bounds for free fixed effects.","category":"section"},{"location":"estimation/mle/#Optimization.jl-Interface","page":"MLE","title":"Optimization.jl Interface","text":"MLE delegates numerical optimization to the SciML Optimization.jl ecosystem. Any optimizer compatible with that interface can, in principle, be used. The following have been tested:\n\nOptimizationOptimJL.LBFGS (default)\nOptim.BFGS\nOptim.NelderMead\nOptimizationOptimisers.Adam\nOptimizationBBO.BBO_adaptive_de_rand_1_bin_radiuslimited()","category":"section"},{"location":"estimation/mle/#Bounds-and-BlackBoxOptim","page":"MLE","title":"Bounds and BlackBoxOptim","text":"User-supplied bounds (lb, ub) operate on the transformed scale of the free parameters. Bound entries for parameters held fixed via constants are ignored automatically.\n\nDerivative-free optimizers from BlackBoxOptim require finite lower and upper bounds for every free parameter. The convenience function default_bounds_from_start(dm; margin=1.0) constructs default box bounds on the transformed scale, centered on the initial values.","category":"section"},{"location":"estimation/mle/#Objective-and-Penalty","page":"MLE","title":"Objective and Penalty","text":"The MLE objective is the negative log-likelihood of the data given the model parameters. An optional L2-style penalty can be added through the penalty keyword of fit_model. The penalty is applied parameter-wise and supports both scalar and vector parameter blocks.","category":"section"},{"location":"estimation/mle/#Fit-Keywords","page":"MLE","title":"Fit Keywords","text":"The following keyword arguments are accepted by fit_model(dm, NoLimits.MLE(...); ...):\n\nconstants – named tuple of fixed-effect values to hold constant during optimization.\npenalty – named tuple of per-parameter penalty weights (L2, on the natural scale).\node_args, ode_kwargs – additional positional and keyword arguments forwarded to the ODE solver.\nserialization – ensemble algorithm controlling parallelism (e.g., EnsembleThreads()).\nrng – random number generator, used where stochastic initialization is needed.\ntheta_0_untransformed – custom starting values on the natural (untransformed) scale.\nstore_data_model – whether to store the DataModel in the result (default: true).\n\nThe keyword constants_re does not apply to MLE, as this method does not involve random effects.","category":"section"},{"location":"estimation/mle/#ODE-Example","page":"MLE","title":"ODE Example","text":"Models that include ordinary differential equations are handled transparently – no special configuration is needed beyond specifying the ODE system. The example below fits a scalar ODE with a quadratic nonlinearity.\n\nusing NoLimits\nusing DataFrames\nusing Distributions\n\nmodel_ode = @Model begin\n    @fixedEffects begin\n        a = RealNumber(0.2)\n        sigma = RealNumber(0.3, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n    end\n\n    @DifferentialEquation begin\n        D(x1) ~ -a * x1^2\n    end\n\n    @initialDE begin\n        x1 = 1.0\n    end\n\n    @formulas begin\n        y ~ Erlang(3, log1p(x1(t)^2) + sigma)\n    end\nend\n\ndf_ode = DataFrame(\n    ID = [1, 1],\n    t = [0.0, 1.0],\n    y = [1.0, 1.05],\n)\n\nmodel_saveat = set_solver_config(model_ode; saveat_mode=:saveat)\ndm_ode = DataModel(model_saveat, df_ode; primary_id=:ID, time_col=:t)\nres_ode = fit_model(dm_ode, NoLimits.MLE())","category":"section"},{"location":"estimation/mle/#Non-Gaussian-Outcome-Example","page":"MLE","title":"Non-Gaussian Outcome Example","text":"NoLimits.jl supports any outcome distribution available in Distributions.jl. The following example uses a Poisson likelihood for count data.\n\nusing NoLimits\nusing DataFrames\nusing Distributions\n\nmodel_pois = @Model begin\n    @covariates begin\n        t = Covariate()\n        z = Covariate()\n    end\n\n    @fixedEffects begin\n        a = RealNumber(0.1)\n        b = RealNumber(0.2)\n    end\n\n    @formulas begin\n        lambda = exp(a + b * z)\n        y ~ Poisson(lambda)\n    end\nend\n\ndf_pois = DataFrame(\n    ID = [1, 1, 2, 2],\n    t = [0.0, 1.0, 0.0, 1.0],\n    z = [0.0, 0.5, 1.0, 1.5],\n    y = [1, 1, 2, 3],\n)\n\ndm_pois = DataModel(model_pois, df_pois; primary_id=:ID, time_col=:t)\nres_pois = fit_model(dm_pois, NoLimits.MLE())","category":"section"},{"location":"estimation/mle/#HMM-Example-(Fixed-Effects-Only)","page":"MLE","title":"HMM Example (Fixed-Effects-Only)","text":"Hidden Markov models (HMMs) with discrete states and discrete time steps can also be estimated via MLE. The dedicated parameter types ProbabilityVector and DiscreteTransitionMatrix handle all simplex and row-stochastic constraints automatically, with full AD support through the logistic stick-breaking parameterization.\n\nusing NoLimits\nusing DataFrames\nusing Distributions\n\nmodel_hmm = @Model begin\n    @covariates begin\n        t = Covariate()\n    end\n\n    @fixedEffects begin\n        pi0 = ProbabilityVector([0.6, 0.4])\n        P   = DiscreteTransitionMatrix([0.9 0.1; 0.2 0.8])\n        p1  = RealNumber(0.2, scale=:logit)\n        p2  = RealNumber(0.8, scale=:logit)\n    end\n\n    @formulas begin\n        y ~ DiscreteTimeDiscreteStatesHMM(\n            P,\n            (Bernoulli(p1), Bernoulli(p2)),\n            Categorical(pi0),\n        )\n    end\nend\n\ndf_hmm = DataFrame(\n    ID = [1, 1, 1, 2, 2, 2],\n    t = [0.0, 1.0, 2.0, 0.0, 1.0, 2.0],\n    y = [0, 1, 1, 1, 0, 1],\n)\n\ndm_hmm = DataModel(model_hmm, df_hmm; primary_id=:ID, time_col=:t)\nres_hmm = fit_model(dm_hmm, NoLimits.MLE(optim_kwargs=(iterations=5,)))","category":"section"},{"location":"estimation/mle/#Accessing-Results","page":"MLE","title":"Accessing Results","text":"After fitting, use the standard accessor functions to retrieve parameter estimates, the final objective value, convergence status, and diagnostics.\n\ntheta_u = get_params(res; scale=:untransformed)\ntheta_t = get_params(res; scale=:transformed)\nobj = get_objective(res)\nok = get_converged(res)\ndiag = get_diagnostics(res)","category":"section"},{"location":"tutorials/fixed-effects-vi/#Fixed-Effects-Tutorial-2:-Variational-Inference-(VI)","page":"Fixed-Effects Tutorial 2: Variational Inference (VI)","title":"Fixed-Effects Tutorial 2: Variational Inference (VI)","text":"This tutorial shows a complete fixed-effects Bayesian workflow with VI in NoLimits.jl: model specification, fitting, variational diagnostics, posterior sampling, chain-style UQ, and plotting.","category":"section"},{"location":"tutorials/fixed-effects-vi/#What-You-Will-Learn","page":"Fixed-Effects Tutorial 2: Variational Inference (VI)","title":"What You Will Learn","text":"How to fit a fixed-effects model with NoLimits.VI.\nHow to inspect VI-specific outputs (trace, state, posterior object).\nHow to sample posterior draws from the variational approximation.\nHow to run compute_uq(...; method=:chain) for VI.","category":"section"},{"location":"tutorials/fixed-effects-vi/#Step-1:-Build-a-Small-Fixed-Effects-Dataset","page":"Fixed-Effects Tutorial 2: Variational Inference (VI)","title":"Step 1: Build a Small Fixed-Effects Dataset","text":"using NoLimits\nusing DataFrames\nusing Distributions\nusing Random\n\nRandom.seed!(123)\n\ndf = DataFrame(\n    ID = [:A, :A, :B, :B, :C, :C, :D, :D],\n    t = [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0],\n    y = [0.10, 0.45, -0.05, 0.20, 0.00, 0.33, -0.08, 0.26],\n)","category":"section"},{"location":"tutorials/fixed-effects-vi/#Step-2:-Define-the-Model","page":"Fixed-Effects Tutorial 2: Variational Inference (VI)","title":"Step 2: Define the Model","text":"model = @Model begin\n    @covariates begin\n        t = Covariate()\n    end\n\n    @fixedEffects begin\n        a = RealNumber(0.0, prior=Normal(0.0, 1.0))\n        b = RealNumber(0.3, prior=Normal(0.0, 1.0))\n        sigma = RealNumber(0.2, scale=:log, prior=LogNormal(-1.5, 0.3))\n    end\n\n    @formulas begin\n        y ~ Normal(a + b * t, sigma)\n    end\nend\n\ndm = DataModel(model, df; primary_id=:ID, time_col=:t)","category":"section"},{"location":"tutorials/fixed-effects-vi/#Step-3:-Fit-with-VI","page":"Fixed-Effects Tutorial 2: Variational Inference (VI)","title":"Step 3: Fit with VI","text":"res_vi = fit_model(\n    dm,\n    VI(; turing_kwargs=(max_iter=350, family=:meanfield, progress=false)),\n    rng=Random.Xoshiro(10),\n)","category":"section"},{"location":"tutorials/fixed-effects-vi/#Step-4:-Inspect-VI-Outputs","page":"Fixed-Effects Tutorial 2: Variational Inference (VI)","title":"Step 4: Inspect VI Outputs","text":"objective = get_objective(res_vi)     # final ELBO\nconverged = get_converged(res_vi)\ntrace = get_vi_trace(res_vi)\nstate = get_vi_state(res_vi)\nposterior = get_variational_posterior(res_vi)\n\nfit_summary = NoLimits.summarize(res_vi)\nfit_summary","category":"section"},{"location":"tutorials/fixed-effects-vi/#Step-5:-Sample-Posterior-Draws-from-the-Variational-Posterior","page":"Fixed-Effects Tutorial 2: Variational Inference (VI)","title":"Step 5: Sample Posterior Draws from the Variational Posterior","text":"draws_named = sample_posterior(\n    res_vi;\n    n_draws=200,\n    rng=Random.Xoshiro(11),\n    return_names=true,\n)\n\nsize(draws_named.draws), first(draws_named.names, 3)","category":"section"},{"location":"tutorials/fixed-effects-vi/#Step-6:-Compute-UQ-Intervals-from-VI-Draws","page":"Fixed-Effects Tutorial 2: Variational Inference (VI)","title":"Step 6: Compute UQ Intervals from VI Draws","text":"For VI fits, use method=:chain. Internally, NoLimits samples from the variational posterior.\n\nuq_vi = compute_uq(\n    res_vi;\n    method=:chain,\n    level=0.95,\n    mcmc_draws=150,\n    rng=Random.Xoshiro(12),\n)\n\nuq_summary = NoLimits.summarize(res_vi, uq_vi)\nuq_summary","category":"section"},{"location":"tutorials/fixed-effects-vi/#Step-7:-Posterior-Based-Plotting","page":"Fixed-Effects Tutorial 2: Variational Inference (VI)","title":"Step 7: Posterior-Based Plotting","text":"The same plotting APIs used for MCMC also work for VI posterior draws.\n\np_fit_vi = plot_fits(\n    res_vi;\n    observable=:y,\n    individuals_idx=[1, 2],\n    ncols=2,\n    plot_mcmc_quantiles=true,\n    mcmc_draws=120,\n)\n\np_obs_vi = plot_observation_distributions(\n    res_vi;\n    observables=:y,\n    individuals_idx=1,\n    obs_rows=1,\n    mcmc_draws=120,\n)\n\nFit plot:\n\np_fit_vi\n\nObservation distribution plot:\n\np_obs_vi","category":"section"},{"location":"tutorials/fixed-effects-vi/#Summary","page":"Fixed-Effects Tutorial 2: Variational Inference (VI)","title":"Summary","text":"You now have a complete fixed-effects VI workflow:\n\nBayesian fitting with VI.\nAccess to optimization trace/state and posterior sampler.\nPosterior intervals through compute_uq(...; method=:chain).\nPosterior-aware predictive and observation-level plots.","category":"section"},{"location":"tutorials/mixed-effects-ode-mcem/#Mixed-Effects-Tutorial-2:-ODE-Model-with-Input-Events-(MCEM)","page":"Mixed-Effects Tutorial 2: ODE Model with Dosing Events (MCEM)","title":"Mixed-Effects Tutorial 2: ODE Model with Input Events (MCEM)","text":"Many longitudinal studies involve systems whose dynamics are governed by ordinary differential equations (ODEs) with discrete input events –- a bolus injection, a nutrient pulse, or a stimulus onset. When individuals differ in their dynamic parameters, nonlinear mixed-effects models provide a principled way to separate population-level trends from subject-level variability. In this tutorial, you will build such a model from scratch and fit it using the Monte Carlo Expectation-Maximization (MCEM) algorithm, a method well-suited to problems where random effects enter the model nonlinearly.\n\nStarting from a publicly available concentration-time dataset (Theophylline), you will learn how to prepare event-aware data, define a two-compartment ODE model with subject-level random effects, run the full estimation workflow, and generate publication-quality diagnostics. The structural model describes a two-compartment transfer system (depot, center) in which material moves from a depot compartment into a central compartment and is subsequently eliminated. Subject-level random effects on the transfer rate (ka), elimination rate (cl), and distribution volume (v) capture between-individual variability. Although this tutorial uses concentration-time data, the workflow generalizes to any domain where compartmental ODE dynamics with discrete input events arise –- tracer kinetics in imaging, nutrient uptake in ecology, or substrate processing in bioprocess engineering.","category":"section"},{"location":"tutorials/mixed-effects-ode-mcem/#What-You-Will-Learn","page":"Mixed-Effects Tutorial 2: ODE Model with Dosing Events (MCEM)","title":"What You Will Learn","text":"By the end of this tutorial, you will be able to:\n\nPrepare event-aware data –- convert a raw longitudinal table into the event format NoLimits.jl expects for ODE models with discrete inputs (bolus injections, perturbations, or similar events).\nDefine a nonlinear mixed-effects ODE model –- use the @preDifferentialEquation and @DifferentialEquation blocks to specify a model whose random effects enter nonlinearly, a setting where MCEM is especially useful.\nFit the model with MCEM –- run the estimation and inspect core diagnostics including the objective value and estimated parameters.\nVisualize and assess results –- generate fitted trajectory plots, observation-distribution diagnostics, and Wald-based uncertainty quantification summaries.","category":"section"},{"location":"tutorials/mixed-effects-ode-mcem/#Step-1:-Prepare-the-Event-Table","page":"Mixed-Effects Tutorial 2: ODE Model with Dosing Events (MCEM)","title":"Step 1: Prepare the Event-Table","text":"In this step, you will convert raw longitudinal data into the event-table format that NoLimits.jl uses for ODE models with discrete inputs. An event table interleaves input events (such as a bolus entering a compartment) with observation rows, so that the ODE solver knows when and where to apply external perturbations. The key columns are:\n\nEVID –- an integer flag distinguishing input events (1) from observation rows (0).\nAMT –- the magnitude of the input event (e.g., total amount delivered).\nCMT –- the name of the ODE compartment that receives the input.\nRATE –- the infusion rate; 0.0 indicates an instantaneous bolus.\n\nThe code below loads the Theophylline dataset –- a widely used concentration-time dataset recording twelve individuals over time –- and reshapes it into this event-table format. Each individual receives a single bolus input at time zero, followed by a series of concentration observations.\n\nusing NoLimits\nusing CSV\nusing DataFrames\nusing Distributions\nusing Downloads\nusing Random\nusing LinearAlgebra\nusing OrdinaryDiffEq\nusing SciMLBase\n\ninclude(joinpath(@__DIR__, \"_data_loaders.jl\"))\n\nRandom.seed!(123)\n\ntheoph_df = load_theoph()\n\nfunction build_theoph_event_df(tbl::DataFrame)\n    df = DataFrame(\n        id=Int[],\n        t=Float64[],\n        AMT=Float64[],\n        EVID=Int[],\n        CMT=Union{String, Missing}[],\n        RATE=Float64[],\n        y1=Union{Float64, Missing}[],\n        _event_order=Int[],\n    )\n\n    for g in groupby(tbl, :Subject)\n        id = Int(first(g.Subject))\n        amt = Float64(first(g.Wt)) * Float64(first(g.Dose))\n\n        push!(df, (id, 0.0, amt, 1, \"depot\", 0.0, missing, 0))\n\n        g_sorted = sort(DataFrame(g), :Time)\n        for row in eachrow(g_sorted)\n            push!(df, (id, Float64(row.Time), 0.0, 0, missing, 0.0, Float64(row.conc), 1))\n        end\n    end\n\n    sort!(df, [:id, :t, :_event_order])\n    select!(df, Not(:_event_order))\n    return df\nend\n\ndf = build_theoph_event_df(theoph_df)\nfirst(df, 12)","category":"section"},{"location":"tutorials/mixed-effects-ode-mcem/#Step-2:-Define-the-ODE-Mixed-Effects-Model","page":"Mixed-Effects Tutorial 2: ODE Model with Dosing Events (MCEM)","title":"Step 2: Define the ODE Mixed-Effects Model","text":"In this step, you will specify the mechanistic model. The goal is twofold: describe the ODE dynamics of the two-compartment system, and account for inter-individual variability through random effects.\n\nThe model has three system parameters –- a transfer rate ka, an elimination rate cl, and a distribution volume v –- each of which varies across individuals. To ensure positivity, each parameter is expressed as an exponential transform of a population-level fixed effect plus a subject-specific random deviation:\n\nka = exp(tka + eta[1])\ncl = exp(tcl + eta[2])\nv = exp(tv + eta[3])\n\nBecause the parameters enter the ODE nonlinearly (through the exponential transform of the random effects), standard linear mixed-effects approximations do not apply. This is precisely the setting where Monte Carlo EM methods such as MCEM offer a robust estimation strategy.\n\nThe random effects vector eta follows a multivariate normal distribution with a diagonal covariance matrix whose entries (omega1, omega2, omega3) are estimated alongside the other fixed effects. Observations are modeled as normally distributed around the predicted concentration in the central compartment, with residual standard deviation sigma_eps.\n\nAfter defining the model, you will configure the ODE solver. Tsit5() is an efficient explicit Runge–Kutta method well-suited to non-stiff systems, and the tolerances below balance numerical accuracy with computational cost.\n\nusing NoLimits\nusing Distributions\nusing LinearAlgebra\nusing OrdinaryDiffEq\n\nmodel_raw = @Model begin\n    @covariates begin\n        t = Covariate()\n    end\n\n    @fixedEffects begin\n        tka = RealNumber(0.45, prior=Uniform(0.1, 5.0), calculate_se=true)\n        tcl = RealNumber(1.0, prior=Uniform(0.1, 5.0), calculate_se=true)\n        tv = RealNumber(3.45, prior=Uniform(0.1, 5.0), calculate_se=true)\n\n        omega1 = RealNumber(1.0, scale=:log, prior=Uniform(0.0, 2.0), calculate_se=true)\n        omega2 = RealNumber(1.0, scale=:log, prior=Uniform(0.0, 2.0), calculate_se=true)\n        omega3 = RealNumber(1.0, scale=:log, prior=Uniform(0.0, 2.0), calculate_se=true)\n\n        sigma_eps = RealNumber(1.0, scale=:log, prior=Uniform(0.0, 2.0), calculate_se=true)\n    end\n\n    @randomEffects begin\n        eta = RandomEffect(\n            MvNormal([0.0, 0.0, 0.0], Diagonal([omega1, omega2, omega3]));\n            column=:id,\n        )\n    end\n\n    @preDifferentialEquation begin\n        ka = exp(tka + eta[1])\n        cl = exp(tcl + eta[2])\n        v = exp(tv + eta[3])\n    end\n\n    @DifferentialEquation begin\n        D(depot) ~ -ka * depot\n        D(center) ~ ka * depot - cl / v * center\n    end\n\n    @initialDE begin\n        depot = 0.0\n        center = 0.0\n    end\n\n    @formulas begin\n        y1 ~ Normal(center(t) / v, sigma_eps)\n    end\nend\n\nmodel = set_solver_config(\n    model_raw;\n    saveat_mode=:saveat,\n    alg=Tsit5(),\n    kwargs=(abstol=1e-6, reltol=1e-6),\n)","category":"section"},{"location":"tutorials/mixed-effects-ode-mcem/#Model-Summary","page":"Mixed-Effects Tutorial 2: ODE Model with Dosing Events (MCEM)","title":"Model Summary","text":"Before moving on, it is good practice to inspect the model structure with NoLimits.summarize. This confirms that all blocks –- fixed effects, random effects, covariates, ODE states, and formulas –- have been parsed correctly.\n\nmodel_summary = NoLimits.summarize(model)\nmodel_summary","category":"section"},{"location":"tutorials/mixed-effects-ode-mcem/#Step-3:-Build-the-DataModel","page":"Mixed-Effects Tutorial 2: ODE Model with Dosing Events (MCEM)","title":"Step 3: Build the DataModel","text":"In this step, you will connect the model to the data by constructing a DataModel. This constructor validates the dataset against the model's requirements –- checking that all declared covariates are present, that constant covariates are indeed constant within each group, and that event columns are well-formed. You must explicitly specify the event-related column names so that NoLimits.jl can correctly distinguish input events from observation rows during ODE integration.\n\ndm = DataModel(\n    model,\n    df;\n    primary_id=:id,\n    time_col=:t,\n    evid_col=:EVID,\n    amt_col=:AMT,\n    rate_col=:RATE,\n    cmt_col=:CMT,\n)\n","category":"section"},{"location":"tutorials/mixed-effects-ode-mcem/#DataModel-Summary","page":"Mixed-Effects Tutorial 2: ODE Model with Dosing Events (MCEM)","title":"DataModel Summary","text":"Summarizing the DataModel gives you an overview of the number of individuals, batches (groups of individuals linked by shared random-effect levels), observation counts, and event structure. This is a useful checkpoint before launching a potentially expensive estimation run.\n\ndm_summary = NoLimits.summarize(dm)\ndm_summary","category":"section"},{"location":"tutorials/mixed-effects-ode-mcem/#Step-4:-Configure-MCEM","page":"Mixed-Effects Tutorial 2: ODE Model with Dosing Events (MCEM)","title":"Step 4: Configure MCEM","text":"In this step, you will set up the MCEM algorithm. MCEM alternates between two stages: (1) an E-step that samples random effects from their conditional posterior given the current fixed-effect estimates, and (2) an M-step that maximizes the expected complete-data log-likelihood with respect to the fixed effects. The number of Monte Carlo samples in the E-step can grow across iterations, improving the approximation as the algorithm converges.\n\nThe configuration below is tuned for a tutorial setting –- moderate iteration counts and sample sizes that keep runtime reasonable. For production analyses, you would typically increase maxiters, raise the ceiling of the sample schedule, and draw more MCMC samples per E-step to reduce Monte Carlo noise in the gradient estimates.\n\nA few notes on the specific settings:\n\nsample_schedule controls how the number of Monte Carlo samples grows with each EM iteration, starting at 60 and increasing by 20 per iteration up to a maximum of 260.\nprogress=false suppresses progress bars to keep documentation output clean.\nEnsembleThreads() enables multithreaded ODE solving across individuals for faster execution.\n\nmcem_method = NoLimits.MCEM(;\n    maxiters=12,\n    sample_schedule=i -> min(60 + 20 * (i - 1), 260),\n    turing_kwargs=(n_samples=60, n_adapt=20, progress=false),\n    optim_kwargs=(maxiters=220,),\n    progress=false,\n)\n\nserialization = SciMLBase.EnsembleThreads()","category":"section"},{"location":"tutorials/mixed-effects-ode-mcem/#Step-5:-Fit-the-Model-and-Inspect-Core-Outputs","page":"Mixed-Effects Tutorial 2: ODE Model with Dosing Events (MCEM)","title":"Step 5: Fit the Model and Inspect Core Outputs","text":"With everything in place, you can now run the fit. The call to fit_model performs the full MCEM optimization loop and returns a result object containing parameter estimates, diagnostics, and metadata.\n\nAfter fitting, you will extract the final objective value –- the marginal log-likelihood approximation at convergence. This number is useful for comparing models or monitoring optimization progress, but model adequacy is best judged through the predictive diagnostics covered in the next steps.\n\nres_mcem = fit_model(\n    dm,\n    mcem_method;\n    serialization=serialization,\n    rng=Random.Xoshiro(33),\n)\n\nfit_summary = (\n    objective=NoLimits.get_objective(res_mcem),\n)\n\nfit_summary","category":"section"},{"location":"tutorials/mixed-effects-ode-mcem/#FitResult-Summary","page":"Mixed-Effects Tutorial 2: ODE Model with Dosing Events (MCEM)","title":"FitResult Summary","text":"The structured summary provides a convenient overview of convergence status, iteration counts, and method-specific diagnostics.\n\nfit_result_summary = NoLimits.summarize(res_mcem)\nfit_result_summary\n\nYou can also extract the estimated fixed-effect parameters on their natural (untransformed) scale. The population-level log-scale parameters (tka, tcl, tv) can be exponentiated to recover typical-individual values, and sigma_eps represents the residual observation noise.\n\nparams = NoLimits.get_params(res_mcem; scale=:untransformed)\n(\n    tka=params.tka,\n    tcl=params.tcl,\n    tv=params.tv,\n    sigma_eps=params.sigma_eps,\n)","category":"section"},{"location":"tutorials/mixed-effects-ode-mcem/#Step-6:-Visualize-Fitted-Trajectories","page":"Mixed-Effects Tutorial 2: ODE Model with Dosing Events (MCEM)","title":"Step 6: Visualize Fitted Trajectories","text":"A natural first diagnostic is to overlay the model's predicted trajectories on the observed data. In this step, the plot_fits function computes individual-level predictions (using empirical Bayes estimates of the random effects) and plots them alongside the raw observations. Showing the first two individuals provides a quick visual check that the model captures the characteristic rise-and-fall dynamics of the two-compartment system.\n\np_fit_mcem = plot_fits(\n    res_mcem;\n    observable=:y1,\n    individuals_idx=[1, 2],\n    ncols=2,\n    shared_x_axis=true,\n    shared_y_axis=true,\n)\n\np_fit_mcem","category":"section"},{"location":"tutorials/mixed-effects-ode-mcem/#Step-7:-Assess-the-Observation-Distribution","page":"Mixed-Effects Tutorial 2: ODE Model with Dosing Events (MCEM)","title":"Step 7: Assess the Observation Distribution","text":"Beyond trajectory-level checks, it is valuable to examine how well the model's predicted observation distribution matches the data at individual time points. In this step, plot_observation_distributions visualizes the predictive distribution for a given individual and observation row, letting you assess whether the assumed error model (here, a normal distribution with standard deviation sigma_eps) is well-calibrated.\n\np_obs_mcem = plot_observation_distributions(\n    res_mcem;\n    observables=:y1,\n    individuals_idx=1,\n    obs_rows=1,\n)\n\np_obs_mcem","category":"section"},{"location":"tutorials/mixed-effects-ode-mcem/#Step-8:-Quantify-Parameter-Uncertainty","page":"Mixed-Effects Tutorial 2: ODE Model with Dosing Events (MCEM)","title":"Step 8: Quantify Parameter Uncertainty","text":"Point estimates alone are insufficient for scientific conclusions –- you also need to quantify how precisely each parameter has been determined. In this step, you will use the Wald approach, which constructs approximate confidence intervals from the curvature of the log-likelihood at the optimum (the observed information matrix). For MCEM fits, computing this curvature requires an auxiliary random-effects integration step; the Laplace approximation (re_approx=:laplace) serves this purpose here.\n\nThe resulting uncertainty estimates are visualized as density plots on the natural parameter scale, providing an intuitive picture of estimation precision given the available data.\n\nuq_mcem = compute_uq(\n    res_mcem;\n    method=:wald,\n    vcov=:hessian,\n    re_approx=:laplace,\n    pseudo_inverse=true,\n    serialization=serialization,\n    n_draws=400,\n    rng=Random.Xoshiro(44),\n)\n\np_uq_mcem = plot_uq_distributions(\n    uq_mcem;\n    scale=:natural,\n    plot_type=:density,\n    show_legend=false,\n)\n\np_uq_mcem","category":"section"},{"location":"tutorials/mixed-effects-ode-mcem/#UQ-Summaries","page":"Mixed-Effects Tutorial 2: ODE Model with Dosing Events (MCEM)","title":"UQ Summaries","text":"Finally, the numerical summaries consolidate point estimates and confidence intervals into a single table –- a format suitable for reporting results in publications or for systematic comparison across candidate models.\n\nuq_summary_mcem = NoLimits.summarize(uq_mcem)\nfit_uq_summary_mcem = NoLimits.summarize(res_mcem, uq_mcem)\n\nfit_uq_summary_mcem","category":"section"},{"location":"tutorials/mixed-effects-ode-mcem/#Practical-Guidance","page":"Mixed-Effects Tutorial 2: ODE Model with Dosing Events (MCEM)","title":"Practical Guidance","text":"Objective values are optimization diagnostics, not model quality scores. A lower objective indicates better optimization convergence, but model adequacy should be judged through predictive checks –- trajectory plots and observation-distribution diagnostics provide complementary views of fit quality.\nInspect trajectory and distributional diagnostics together. plot_fits reveals whether the model captures the overall shape of the response, while plot_observation_distributions tests whether the local predictive distribution is well-calibrated. Discrepancies in either diagnostic suggest different kinds of model misspecification.\nScale up before changing model structure. If the fit appears unstable or the objective has not plateaued, first try increasing maxiters and the sample schedule ceiling. MCEM convergence can be slow when Monte Carlo noise dominates the gradient signal, and increasing sample sizes is often more productive than modifying the structural model.","category":"section"},{"location":"installation/#Installation","page":"Installation","title":"Installation","text":"NoLimits.jl requires Julia 1.12 or later. The package is currently installed directly from GitHub.","category":"section"},{"location":"installation/#Installing-the-Package","page":"Installation","title":"Installing the Package","text":"In a Julia session, run:\n\nusing Pkg\nPkg.add(url=\"https://github.com/manuhuth/NoLimits.jl\")\n\nThen verify the installation by loading the package:\n\nusing NoLimits\n\nIf this runs without errors, the installation is complete and you are ready to proceed to the Tutorials.","category":"section"},{"location":"installation/#Registry-Status","page":"Installation","title":"Registry Status","text":"Registry-based installation (Pkg.add(\"NoLimits\")) will be available once the package is published in a Julia package registry.","category":"section"},{"location":"uncertainty-quantification/wald/#Wald","page":"Wald","title":"Wald","text":"The Wald method is the most widely used approach to uncertainty quantification for maximum likelihood and related estimators. It approximates the log-likelihood surface as locally quadratic at the optimum, yielding a Gaussian approximation to the sampling distribution of the parameter estimates. This approximation is computationally inexpensive and accurate when the model is well-identified and the sample size is adequate.\n\nIn NoLimits.jl, Wald UQ is accessed via compute_uq(...; method=:wald). It constructs an approximate variance-covariance matrix for eligible fixed-effect coordinates, then draws from the implied Gaussian distribution to form confidence intervals.\n\nFor visualization of Wald-based densities, closed-form curves are used when available:\n\nTransformed scale: Normal distribution (closed form).\nNatural scale: Normal for identity-scale coordinates; LogNormal for log-scale coordinates.\nOther cases: draw-based kernel density estimation (KDE) is used as a fallback.","category":"section"},{"location":"uncertainty-quantification/wald/#Applicability","page":"Wald","title":"Applicability","text":"Wald UQ is supported for results from the following estimation methods:\n\nFixed-effects fits: MLE and MAP\nMixed-effects fits: Laplace, LaplaceMAP, MCEM, and SAEM","category":"section"},{"location":"uncertainty-quantification/wald/#Minimal-Usage","page":"Wald","title":"Minimal Usage","text":"using NoLimits\n\nuq = compute_uq(\n    res;\n    method=:wald,\n    level=0.95,\n    vcov=:hessian,\n    n_draws=2000,\n)","category":"section"},{"location":"uncertainty-quantification/wald/#Covariance-Choice","page":"Wald","title":"Covariance Choice","text":"The vcov argument selects the variance-covariance estimator.\n\nvcov=:hessian: computes the covariance as the inverse of the observed information (Hessian) matrix. This is the standard choice when the model is correctly specified.\nvcov=:sandwich: computes the sandwich (robust) covariance estimator, which provides consistent standard errors even under mild model misspecification.\n\nuq_hessian = compute_uq(res; method=:wald, vcov=:hessian, n_draws=1000)\nuq_sandwich = compute_uq(res; method=:wald, vcov=:sandwich, n_draws=1000)","category":"section"},{"location":"uncertainty-quantification/wald/#Numerical-Robustness-Controls","page":"Wald","title":"Numerical Robustness Controls","text":"When the Hessian is poorly conditioned – for example, because the log-likelihood surface is locally flat along certain parameter directions – the following controls can improve numerical stability:\n\npseudo_inverse: use the Moore-Penrose pseudo-inverse when direct matrix inversion is numerically unstable.\nhessian_backend: choose the differentiation strategy – :auto, :forwarddiff, or :fd_gradient (finite-difference based).\nfd_abs_step: absolute step size for finite-difference computation.\nfd_rel_step: relative step size for finite-difference computation.\nfd_max_tries: maximum number of retry attempts for finite-difference gradient and Hessian evaluations.\n\nuq_robust = compute_uq(\n    res;\n    method=:wald,\n    pseudo_inverse=true,\n    hessian_backend=:fd_gradient,\n    fd_abs_step=1e-4,\n    fd_rel_step=1e-3,\n    fd_max_tries=8,\n    n_draws=1500,\n)\n\nWhen the covariance matrix requires projection to restore positive semi-definiteness, this is reported in the diagnostics (via vcov_projected and eigenvalue-related fields).","category":"section"},{"location":"uncertainty-quantification/wald/#MCEM/SAEM-Approximation-Controls","page":"Wald","title":"MCEM/SAEM Approximation Controls","text":"For fits obtained with MCEM or SAEM, the marginal likelihood is not available in closed form, so the Hessian cannot be computed directly from the EM objective. In these cases, the Wald backend uses a random-effects approximation method (typically Laplace) during the Hessian evaluation.\n\nre_approx=:auto (the default) selects a Laplace-style approximation automatically.\nre_approx_method allows passing an explicit approximation method instance for finer control.\n\nif @isdefined(res_saem) && res_saem !== nothing\n    uq_saem = compute_uq(\n        res_saem;\n        method=:wald,\n        re_approx=:laplace,\n        n_draws=800,\n    )\nend\n\nif @isdefined(res_mcem) && res_mcem !== nothing\n    uq_mcem_custom = compute_uq(\n        res_mcem;\n        method=:wald,\n        re_approx_method=NoLimits.Laplace(; multistart_n=0, multistart_k=0),\n        n_draws=800,\n    )\nend","category":"section"},{"location":"uncertainty-quantification/wald/#Choosing-n_draws","page":"Wald","title":"Choosing n_draws","text":"The n_draws parameter controls the number of Monte Carlo samples drawn from the Wald Gaussian approximation. These draws are used to compute draw-based summaries (accessible via get_uq_draws), interval estimates, and the natural-scale covariance matrix.\n\nImportantly, n_draws does not affect the transformed-scale covariance matrix itself (get_uq_vcov(...; scale=:transformed)), which is derived directly from the Hessian or sandwich calculation.\n\nGuidelines for setting n_draws:\n\nLower values (e.g., a few hundred): faster computation, but with greater Monte Carlo variability in the resulting intervals.\nHigher values (e.g., several thousand): more stable interval estimates at the cost of additional computation.\n\nThe value n_draws=800 used in examples above represents a practical middle ground for interactive exploration. For final reporting, it is advisable to increase n_draws and verify stability by rerunning with a different rng seed.\n\nFor density visualization via plot_uq_distributions, the distinction between closed-form and draw-based densities becomes relevant:\n\nIdentity-scale and log-scale coordinates use closed-form Normal or LogNormal density curves, independent of n_draws.\nCoordinates requiring nontrivial inverse transforms (e.g., PSD matrix elements parameterized via :cholesky or :expm) rely on draw-based KDE, making the draw count important for plot quality.","category":"section"},{"location":"uncertainty-quantification/wald/#Parameter-Inclusion-Rules","page":"Wald","title":"Parameter Inclusion Rules","text":"Wald UQ is computed only on the subset of free fixed-effect coordinates that are eligible for uncertainty calculation.\n\nA coordinate is excluded when:\n\nits fixed effect is held constant via constants, or\nits parameter block has calculate_se=false.\n\nIf no eligible coordinates remain after exclusion, Wald UQ raises an error.\n\nfe = @fixedEffects begin\n    a = RealNumber(0.2, calculate_se=true)                 # included\n    b = RealNumber(0.1, calculate_se=false)                # excluded\n    sigma = RealNumber(0.3, scale=:log, calculate_se=true) # included\nend\n\nuq = compute_uq(res; method=:wald, constants=(; a=0.2))\n\nIn this example, a is also excluded because it is fixed by constants, leaving only sigma in the UQ computation.","category":"section"},{"location":"uncertainty-quantification/wald/#Fit-Kwarg-Forwarding","page":"Wald","title":"Fit-Kwarg Forwarding","text":"compute_uq(...; method=:wald) accepts overrides for fit-related settings that may differ from those used during the original estimation:\n\nconstants\nconstants_re\npenalty\node_args, ode_kwargs\nserialization\nrng\n\nWhen these are omitted, the values stored from the original fit are used.","category":"section"},{"location":"uncertainty-quantification/wald/#Returned-Quantities","page":"Wald","title":"Returned Quantities","text":"Wald UQ returns a UQResult with backend :wald. The full set of accessor functions is shown below.\n\nbackend = get_uq_backend(uq)                 # :wald\nsource = get_uq_source_method(uq)            # original fit method symbol\nnames = get_uq_parameter_names(uq)\n\nest_nat = get_uq_estimates(uq; scale=:natural)\nest_tr = get_uq_estimates(uq; scale=:transformed)\n\nints_nat = get_uq_intervals(uq; scale=:natural)\nints_tr = get_uq_intervals(uq; scale=:transformed)\n\nV_nat = get_uq_vcov(uq; scale=:natural)\nV_tr = get_uq_vcov(uq; scale=:transformed)\n\ndraws_nat = get_uq_draws(uq; scale=:natural)\ndraws_tr = get_uq_draws(uq; scale=:transformed)\n\ndiag = get_uq_diagnostics(uq)","category":"section"},{"location":"tutorials/mixed-effects-vi/#Mixed-Effects-Tutorial-7:-Variational-Inference-(VI)-for-Longitudinal-Growth","page":"Mixed-Effects Tutorial 7: Variational Inference (VI)","title":"Mixed-Effects Tutorial 7: Variational Inference (VI) for Longitudinal Growth","text":"Longitudinal growth studies measure the same individuals repeatedly over time, capturing how a biological quantity evolves under different conditions. The classic ChickWeight dataset from R records the body weights of 50 newly hatched chicks at up to twelve time points over 21 days, with each chick assigned to one of four dietary formulations. The scientific questions are natural: how quickly do chicks grow on average, does diet tier systematically shift the weight trajectory, and how much individual-to-individual variability persists after accounting for diet? A linear mixed-effects model addresses all three questions simultaneously – a population-level growth rate and diet effect captured by fixed effects, and a subject-specific random intercept that absorbs unexplained individual differences.\n\nEstimating this model requires integrating over the random effects. In this tutorial, you will use Variational Inference (VI), a deterministic alternative to Markov Chain Monte Carlo (MCMC). Rather than drawing exact samples from the posterior, VI optimizes a parameterized family of distributions (here, a full-rank multivariate Normal) to minimize the Kullback–Leibler divergence to the true posterior. When it converges well, VI produces a tractable posterior approximation that can be sampled, summarized, and used for uncertainty quantification at a fraction of the computational cost of MCMC.\n\nFor a mixed-effects model, the VI posterior covers both the fixed-effects parameters and the individual-level random effect values – jointly. This makes VI an especially convenient method when you want approximate Bayesian uncertainty for both population parameters and subject-specific estimates in a single optimization pass.","category":"section"},{"location":"tutorials/mixed-effects-vi/#Learning-Goals","page":"Mixed-Effects Tutorial 7: Variational Inference (VI)","title":"Learning Goals","text":"By the end of this tutorial, you will know how to:\n\nPrepare longitudinal growth data. Load and restructure the ChickWeight dataset, encoding diet as an ordinal numeric covariate and enforcing the types and sort order that NoLimits requires.\nSpecify a mixed-effects model with a constant covariate. Define fixed effects for baseline weight, growth slope, and diet effect, plus a Normal random intercept for each chick.\nFit with VI. Run fit_model with a VI method configuration and inspect the ELBO value and convergence status.\nInspect the joint variational posterior. Use sample_posterior to draw from the variational approximation and examine which posterior coordinates correspond to fixed effects and which to random effects.\nQuantify uncertainty with chain-style UQ. Use compute_uq(...; method=:chain) to propagate variational posterior samples into credible intervals for all fixed-effects parameters.\nProduce posterior-aware diagnostic plots. Generate fitted-trajectory plots, random-effect distribution plots, VPC plots, and residual QQ plots using the same APIs available for MCMC.","category":"section"},{"location":"tutorials/mixed-effects-vi/#Step-1:-Data-Setup","page":"Mixed-Effects Tutorial 7: Variational Inference (VI)","title":"Step 1: Data Setup","text":"In this step, you will load the ChickWeight dataset from the Rdatasets mirror and prepare it for modelling. The preprocessing operations are: converting the chick identifier to a string (NoLimits requires grouping IDs to be comparable with ==); enforcing Float64 types on the time and weight columns; constructing a numeric diet covariate diet_num by subtracting 1 from the integer diet label so that Diet 1 maps to 0.0, Diet 2 to 1.0, and so on; and sorting by chick and time. The summary printed at the end gives a quick overview of the dataset's scale before modelling.\n\nusing NoLimits\nusing CSV\nusing DataFrames\nusing Distributions\nusing Downloads\nusing Random\n\ninclude(joinpath(@__DIR__, \"_data_loaders.jl\"))\n\nRandom.seed!(2026)\n\ndf = load_chickweight()\nselect!(df, [:Chick, :Time, :weight, :Diet])\ndf.Chick    = string.(df.Chick)\ndf.Time     = Float64.(df.Time)\ndf.weight   = Float64.(df.weight)\ndf.diet_num = Float64.(df.Diet .- 1)   # 0.0 = Diet 1, 1.0 = Diet 2, 2.0 = Diet 3, 3.0 = Diet 4\nsort!(df, [:Chick, :Time])\n\n(\n    n_rows       = nrow(df),\n    n_chicks     = length(unique(df.Chick)),\n    n_diets      = length(unique(df.Diet)),\n    time_range   = extrema(df.Time),\n    weight_range = extrema(df.weight),\n)","category":"section"},{"location":"tutorials/mixed-effects-vi/#Step-2:-Define-the-Mixed-Effects-Model","page":"Mixed-Effects Tutorial 7: Variational Inference (VI)","title":"Step 2: Define the Mixed-Effects Model","text":"In this step, you will specify a linear mixed-effects model for chick growth. The population-level mean weight for chick i at time t is:\n\nmu_it = alpha + beta cdot t + gamma cdot d_i + eta_i\n\nwhere alpha is the population intercept (expected weight at day 0 for a Diet-1 chick), beta is the daily growth rate shared across all individuals, gamma is the additive diet effect per diet tier above tier 1, d_i in 0 1 2 3 is the constant diet-level covariate for chick i, and eta_i sim textNormal(0 omega) is a subject-specific random intercept that captures unexplained baseline weight variation.\n\nThe observation model is textweight_it sim textNormal(mu_it sigma), where sigma is the residual standard deviation.\n\nAll five fixed-effects parameters are assigned weakly informative priors consistent with the observed weight range (roughly 35–380 g over 21 days). The positive variance parameters omega and sigma are estimated on the log scale and given LogNormal priors; the regression coefficients alpha, beta, and gamma are estimated on the identity scale with Normal priors wide enough to let the data dominate.\n\nmodel = @Model begin\n    @covariates begin\n        Time     = Covariate()\n        diet_num = ConstantCovariate(; constant_on=:Chick)\n    end\n\n    @fixedEffects begin\n        alpha = RealNumber(45.0, prior=Normal(45.0, 20.0))\n        beta  = RealNumber(8.0,  prior=Normal(8.0,  4.0))\n        gamma = RealNumber(8.0,  prior=Normal(0.0,  15.0))\n        omega = RealNumber(20.0, scale=:log, prior=LogNormal(3.0, 0.5))\n        sigma = RealNumber(15.0, scale=:log, prior=LogNormal(2.7, 0.5))\n    end\n\n    @randomEffects begin\n        eta = RandomEffect(Normal(0.0, omega); column=:Chick)\n    end\n\n    @formulas begin\n        mu = alpha + beta * Time + gamma * diet_num + eta\n        weight ~ Normal(mu, sigma)\n    end\nend\n\nNoLimits.summarize(model)","category":"section"},{"location":"tutorials/mixed-effects-vi/#Step-3:-Build-DataModel-and-Configure-VI","page":"Mixed-Effects Tutorial 7: Variational Inference (VI)","title":"Step 3: Build DataModel and Configure VI","text":"In this step, you will bind the model to the dataset and configure the variational inference method.\n\nThe DataModel constructor validates the schema (checking that Chick, Time, weight, and diet_num are all present and correctly typed), groups observations by chick, and assembles the internal data structures needed for estimation. Calling NoLimits.summarize on the constructed data model is a useful sanity check: it reports the number of individuals, the RE grouping structure, and the covariate types.\n\nVariational Inference in NoLimits is powered by Turing.jl's vi engine. The two key configuration choices are the variational family and max_iter. The :fullrank family uses a full-covariance multivariate Normal approximation over all latent variables, capturing posterior correlations between fixed effects and random effects that are typical in hierarchical models. The :meanfield family uses a factorized approximation that is faster but ignores these correlations and can underestimate posterior uncertainty. For a mixed-effects model with 50 chick-level random effects and 5 fixed-effects parameters, :fullrank is the more faithful choice.\n\ndm = DataModel(model, df; primary_id=:Chick, time_col=:Time)\n\nvi_method = VI(; turing_kwargs=(max_iter=600, family=:fullrank, progress=false))\n\nNoLimits.summarize(dm)","category":"section"},{"location":"tutorials/mixed-effects-vi/#Step-4:-Fit-and-Inspect-Core-Summary","page":"Mixed-Effects Tutorial 7: Variational Inference (VI)","title":"Step 4: Fit and Inspect Core Summary","text":"Running fit_model optimizes the variational parameters (the mean and covariance of the multivariate Normal approximation) by maximizing the Evidence Lower BOund (ELBO) with stochastic gradient ascent. The ELBO is a lower bound on the log marginal likelihood; maximizing it pushes the variational distribution to be as close as possible to the true posterior while keeping it analytically tractable. The reported objective is the final ELBO value.\n\nres_vi = fit_model(\n    dm,\n    vi_method;\n    rng=Random.Xoshiro(20),\n)\n\nNoLimits.summarize(res_vi)\n\nYou can also directly inspect the convergence flag and ELBO trace to confirm that optimization did not terminate prematurely:\n\nget_converged(res_vi)\nget_vi_trace(res_vi)\n\nA rising ELBO trace that flattens toward the final iteration is the expected signature of well-behaved convergence. If the trace is still noticeably increasing at the last step, consider increasing max_iter.","category":"section"},{"location":"tutorials/mixed-effects-vi/#Step-5:-Posterior-Coordinates-and-Random-Effects","page":"Mixed-Effects Tutorial 7: Variational Inference (VI)","title":"Step 5: Posterior Coordinates and Random Effects","text":"The variational posterior is a joint distribution over all latent variables: the five fixed-effects parameters and the 50 chick-level random effects. sample_posterior draws from this approximation and, when return_names=true, returns the coordinate names alongside the draws so you can identify which dimensions correspond to which quantities.\n\ndraws_named = sample_posterior(\n    res_vi;\n    n_draws=200,\n    rng=Random.Xoshiro(21),\n    return_names=true,\n)\n\nsize(draws_named.draws), first(draws_named.names, 8)\n\nThe first five coordinates in draws_named.names are the transformed fixed-effects parameters (alpha, beta, gamma, omega, sigma). The remaining 50 coordinates are the chick-level random-effect values eta_vals[1], ..., eta_vals[50], one per chick in the order they appear in the dataset. Examining these joint draws directly – or computing per-chick posterior means and standard deviations – reveals both population-level uncertainty and subject-specific uncertainty in a single posterior object.","category":"section"},{"location":"tutorials/mixed-effects-vi/#Step-6:-Chain-Style-UQ","page":"Mixed-Effects Tutorial 7: Variational Inference (VI)","title":"Step 6: Chain-Style UQ","text":"For VI fits, credible intervals are constructed by treating posterior samples as a pseudo-chain. The method=:chain option draws mcmc_draws samples from the variational posterior and uses their empirical quantiles to form credible intervals. This approach is API-compatible with the MCMC-based UQ workflow, so the same summarization and plotting functions work for both sampling-based and variational methods.\n\nuq_vi = compute_uq(\n    res_vi;\n    method=:chain,\n    level=0.95,\n    mcmc_draws=200,\n    rng=Random.Xoshiro(22),\n)\n\nNoLimits.summarize(res_vi, uq_vi)\n\nThe combined summary table shows point estimates alongside 95% credible interval bounds for each fixed-effects parameter. A narrow interval for beta confirms that the population growth rate is well-identified by the data. A positive gamma with an interval clearly above zero provides evidence that higher diet tiers are associated with heavier weights, after accounting for individual variation.","category":"section"},{"location":"tutorials/mixed-effects-vi/#Step-7:-Posterior-Based-Diagnostic-Plots","page":"Mixed-Effects Tutorial 7: Variational Inference (VI)","title":"Step 7: Posterior-Based Diagnostic Plots","text":"A key advantage of VI over point-estimate methods (MLE, MAP) is that the posterior approximation can propagate uncertainty into all downstream visualizations. The following four plots each draw mcmc_draws posterior samples from the variational posterior and use them to construct credible bands, predictive distributions, and calibration diagnostics.\n\np_fit_vi = plot_fits(\n    res_vi;\n    observable=:weight,\n    individuals_idx=[1, 2, 3, 4, 5, 6],\n    ncols=3,\n    plot_mcmc_quantiles=true,\n    mcmc_draws=150,\n)\n\np_re_vi = plot_random_effect_distributions(\n    res_vi;\n    mcmc_draws=150,\n)\n\np_vpc_vi = plot_vpc(\n    res_vi;\n    n_simulations=50,\n    n_bins=4,\n    mcmc_draws=150,\n    rng=Random.Xoshiro(23),\n)\n\np_qq_vi = plot_residual_qq(\n    res_vi;\n    residual=:quantile,\n    mcmc_draws=150,\n)\n\nFitted trajectories with posterior credible bands (6 chicks):\n\np_fit_vi\n\nMarginal posterior distribution of the random effects across all 50 chicks:\n\np_re_vi\n\nVisual Predictive Check – do model-simulated trajectories bracket the observed data in the expected proportions?\n\np_vpc_vi\n\nResidual quantile-quantile plot for overall calibration assessment:\n\np_qq_vi","category":"section"},{"location":"tutorials/mixed-effects-vi/#Step-8:-Optional-–-Condition-on-Fixed-Effect-Estimates","page":"Mixed-Effects Tutorial 7: Variational Inference (VI)","title":"Step 8: Optional – Condition on Fixed-Effect Estimates","text":"When fixed-effect values have been established from a prior analysis (for example, a MAP fit), you can hold them constant and run VI exclusively over the random effects. The constants argument pins specified fixed effects at given values on the transformed scale, removing them from the optimizer's parameter space. For identity-scale parameters (alpha, beta, gamma) the transformed value equals the natural value; for log-scale parameters (omega, sigma) the transformed value is the logarithm.\n\nres_re_only = fit_model(\n    dm,\n    VI(; turing_kwargs=(max_iter=300, progress=false)),\n    constants=(alpha=45.0, beta=8.0, gamma=8.0, omega=log(20.0), sigma=log(15.0)),\n    rng=Random.Xoshiro(24),\n)\n\nThis conditional RE-only fit is useful for sensitivity analyses or plug-in empirical Bayes workflows where you trust the point estimates of the population parameters but still want approximate posterior uncertainty over individual-level effects.","category":"section"},{"location":"tutorials/mixed-effects-vi/#Interpretation-Notes","page":"Mixed-Effects Tutorial 7: Variational Inference (VI)","title":"Interpretation Notes","text":"Why VI for mixed effects? MCMC scales poorly with the number of random effects because each subject introduces additional latent dimensions that the sampler must explore. VI collapses this exploration into an optimization problem, making it faster and more predictable while still providing an approximate posterior. For datasets with tens or hundreds of subjects, VI is often the most practical approximate Bayesian method.\nFullrank vs. meanfield. The full-rank variational family captures correlations between fixed and random effects – for example, the negative correlation between the population intercept alpha and individual random effects eta_i that arises because a higher alpha requires smaller eta_i values to explain the same observations. Meanfield VI ignores these correlations and can underestimate posterior variance or misrepresent marginal parameter distributions. For hierarchical models, fullrank is generally preferred when the additional computational cost is acceptable.\nVI tends to underestimate posterior variance. Because VI minimizes the reverse KL divergence (which is mass-seeking rather than mass-covering), the variational approximation often produces credible intervals that are somewhat narrower than those from MCMC. If you observe suspiciously narrow intervals, consider running an MCMC sampler on the same model as a reference comparison.\nLinear model as a starting point. The linear growth model used here is a deliberate simplification: chick growth is not strictly linear over 21 days, and residual plots may show some curvature at later time points. Nonlinear alternatives – logistic growth, Gompertz curves – would better capture the decelerating growth phase but require a @DifferentialEquation or a closed-form nonlinear @formulas block. The VI workflow demonstrated here extends naturally to those more complex specifications.","category":"section"},{"location":"api/#API-Reference","page":"API","title":"API Reference","text":"This page documents the complete public API of NoLimits.jl. Each entry is rendered from the docstring attached to the corresponding function, type, or macro.","category":"section"},{"location":"api/#Model-Building","page":"API","title":"Model Building","text":"","category":"section"},{"location":"api/#Macros","page":"API","title":"Macros","text":"","category":"section"},{"location":"api/#Parameter-Types","page":"API","title":"Parameter Types","text":"","category":"section"},{"location":"api/#Covariate-Types","page":"API","title":"Covariate Types","text":"","category":"section"},{"location":"api/#Random-Effects","page":"API","title":"Random Effects","text":"","category":"section"},{"location":"api/#Model-Struct-and-Solver-Configuration","page":"API","title":"Model Struct and Solver Configuration","text":"","category":"section"},{"location":"api/#Model-Component-Structs","page":"API","title":"Model Component Structs","text":"These structs hold the parsed, compiled form of each model block. They are constructed automatically by the block macros and stored inside Model.","category":"section"},{"location":"api/#Data-Binding","page":"API","title":"Data Binding","text":"","category":"section"},{"location":"api/#DataModel","page":"API","title":"DataModel","text":"","category":"section"},{"location":"api/#DataModel-Accessors","page":"API","title":"DataModel Accessors","text":"","category":"section"},{"location":"api/#Summaries","page":"API","title":"Summaries","text":"","category":"section"},{"location":"api/#Estimation","page":"API","title":"Estimation","text":"","category":"section"},{"location":"api/#Base-Types","page":"API","title":"Base Types","text":"","category":"section"},{"location":"api/#Fitting-Interface","page":"API","title":"Fitting Interface","text":"","category":"section"},{"location":"api/#Methods","page":"API","title":"Methods","text":"","category":"section"},{"location":"api/#Result-Types","page":"API","title":"Result Types","text":"","category":"section"},{"location":"api/#Fit-Result-Accessors","page":"API","title":"Fit Result Accessors","text":"","category":"section"},{"location":"api/#Multistart-Accessors","page":"API","title":"Multistart Accessors","text":"","category":"section"},{"location":"api/#Fit-Summaries","page":"API","title":"Fit Summaries","text":"","category":"section"},{"location":"api/#Utilities","page":"API","title":"Utilities","text":"","category":"section"},{"location":"api/#Uncertainty-Quantification","page":"API","title":"Uncertainty Quantification","text":"","category":"section"},{"location":"api/#Data-Simulation","page":"API","title":"Data Simulation","text":"","category":"section"},{"location":"api/#Identifiability-Analysis","page":"API","title":"Identifiability Analysis","text":"","category":"section"},{"location":"api/#Plotting-and-Diagnostics","page":"API","title":"Plotting and Diagnostics","text":"","category":"section"},{"location":"api/#Core-Plots","page":"API","title":"Core Plots","text":"","category":"section"},{"location":"api/#Visual-Predictive-Checks","page":"API","title":"Visual Predictive Checks","text":"","category":"section"},{"location":"api/#Residual-Diagnostics","page":"API","title":"Residual Diagnostics","text":"","category":"section"},{"location":"api/#Random-Effects-Diagnostics","page":"API","title":"Random-Effects Diagnostics","text":"","category":"section"},{"location":"api/#Observation-Distributions","page":"API","title":"Observation Distributions","text":"","category":"section"},{"location":"api/#Uncertainty-Quantification-Plots","page":"API","title":"Uncertainty Quantification Plots","text":"","category":"section"},{"location":"api/#Multistart-Plots","page":"API","title":"Multistart Plots","text":"","category":"section"},{"location":"api/#Distributions","page":"API","title":"Distributions","text":"","category":"section"},{"location":"api/#Hidden-Markov-Models","page":"API","title":"Hidden Markov Models","text":"","category":"section"},{"location":"api/#Normalizing-Flows","page":"API","title":"Normalizing Flows","text":"","category":"section"},{"location":"api/#Utilities-2","page":"API","title":"Utilities","text":"","category":"section"},{"location":"api/#Soft-Decision-Trees","page":"API","title":"Soft Decision Trees","text":"","category":"section"},{"location":"api/#B-Splines","page":"API","title":"B-Splines","text":"","category":"section"},{"location":"api/#NoLimits.@Model","page":"API","title":"NoLimits.@Model","text":"@Model begin\n    @helpers begin ... end           # optional\n    @fixedEffects begin ... end      # optional if @randomEffects present\n    @covariates begin ... end        # optional\n    @randomEffects begin ... end     # optional if @fixedEffects present\n    @preDifferentialEquation begin ... end  # optional\n    @DifferentialEquation begin ... end     # optional; requires @initialDE\n    @initialDE begin ... end                # optional; requires @DifferentialEquation\n    @formulas begin ... end          # required\nend\n\nCompose all model blocks into a Model struct.\n\nEach block is optional except @formulas. At least one of @fixedEffects or @randomEffects must be non-empty. @DifferentialEquation and @initialDE must appear together.\n\nAfter assembling the blocks, @Model:\n\nCalls finalize_covariates to resolve constant_on defaults.\nValidates that DE covariates are used correctly (no varying covariates, dynamic covariates must be called as w(t)).\nCompiles formula builder functions and validates state/signal usage.\nReturns a fully constructed Model ready for use with DataModel.\n\n\n\n\n\n","category":"macro"},{"location":"api/#NoLimits.@helpers","page":"API","title":"NoLimits.@helpers","text":"@helpers begin\n    f(x) = ...\n    g(x, y) = ...\nend\n\nDefine user-provided helper functions that are available inside @randomEffects, @preDifferentialEquation, @DifferentialEquation, @initialDE, and @formulas blocks.\n\nEach statement must be a function definition using short (f(x) = expr) or long (function f(x) ... end) form. Helper names must be unique within the block.\n\nReturns a NamedTuple mapping each function name to its compiled anonymous function. The helpers NamedTuple is stored in the Model and passed automatically at evaluation time via get_helper_funs.\n\nMutating operations (calls ending in !, indexed assignment) trigger a warning since they may break Zygote-based automatic differentiation.\n\n\n\n\n\n","category":"macro"},{"location":"api/#NoLimits.@fixedEffects","page":"API","title":"NoLimits.@fixedEffects","text":"@fixedEffects begin\n    name = ParameterBlockType(...)\n    ...\nend\n\nCompile a block of fixed-effect parameter declarations into a FixedEffects struct.\n\nEach statement must be an assignment name = constructor(...) where the right-hand side is one of the parameter block constructors: RealNumber, RealVector, RealPSDMatrix, RealDiagonalMatrix, NNParameters, SoftTreeParameters, SplineParameters, or NPFParameter.\n\nThe LHS symbol becomes the parameter name and is automatically injected as the name keyword argument into each constructor.\n\nThe @fixedEffects block is typically used inside @Model. It can also be used standalone to construct a FixedEffects object directly.\n\n\n\n\n\n","category":"macro"},{"location":"api/#NoLimits.@covariates","page":"API","title":"NoLimits.@covariates","text":"@covariates begin\n    name = CovariateType(...)\n    ...\nend\n\nCompile covariate declarations into a Covariates struct.\n\nEach statement must be an assignment name = constructor(...) where the constructor is one of: Covariate, CovariateVector, ConstantCovariate, ConstantCovariateVector, DynamicCovariate, or DynamicCovariateVector.\n\nFor scalar types (Covariate, ConstantCovariate, DynamicCovariate), the LHS symbol determines the data-frame column name — do not pass an explicit column argument.\n\nThe @covariates block is typically used inside @Model. It can also be used standalone to construct a Covariates object directly.\n\n\n\n\n\n","category":"macro"},{"location":"api/#NoLimits.@randomEffects","page":"API","title":"NoLimits.@randomEffects","text":"@randomEffects begin\n    name = RandomEffect(dist; column=:GroupCol)\n    ...\nend\n\nCompile random-effect declarations into a RandomEffects struct.\n\nEach statement must be an assignment name = RandomEffect(dist; column=:Col). The distribution expression dist may reference fixed effects, constant covariates, helper functions, and model functions (NNs, splines, soft trees). The symbols t and ξ are forbidden.\n\nWhen using NormalizingPlanarFlow(ψ) in a distribution, it is automatically rewritten to call model_funs.NPF_ψ(ψ), where the NPF callable is registered automatically from the corresponding NPFParameter in @fixedEffects.\n\nThe @randomEffects block is typically used inside @Model.\n\n\n\n\n\n","category":"macro"},{"location":"api/#NoLimits.@preDifferentialEquation","page":"API","title":"NoLimits.@preDifferentialEquation","text":"@preDifferentialEquation begin\n    name = expr\n    ...\nend\n\nCompile time-constant derived quantities into a PreDifferentialEquation struct.\n\nEach statement must be an assignment name = expr. The right-hand side may reference:\n\nfixed effects and random effects by name,\nconstant covariates (including vector fields x.field),\nhelper functions,\nmodel functions (NNs, splines, soft trees).\n\nThe symbols t and ξ are forbidden (pre-DE variables are time-constant).\n\nPre-DE variables are computed once per individual before the ODE is integrated and are available inside @DifferentialEquation and @initialDE.\n\nMutating operations trigger a warning since they may break Zygote-based AD.\n\n\n\n\n\n","category":"macro"},{"location":"api/#NoLimits.@DifferentialEquation","page":"API","title":"NoLimits.@DifferentialEquation","text":"@DifferentialEquation begin\n    D(state) ~ rhs_expr\n    signal(t) = signal_expr\n    ...\nend\n\nCompile an ODE system into a DifferentialEquation struct.\n\nTwo statement forms are supported:\n\nD(state) ~ rhs: defines a state variable whose time derivative equals rhs.\nsignal(t) = expr: defines a derived signal computed from states and parameters.\n\nSymbols in right-hand sides are resolved from (in order): pre-DE variables, random effects, fixed effects, constant covariates, dynamic covariates (called as w(t)), model functions, and helper functions. Varying (non-dynamic) covariates are not allowed inside the DE.\n\nSmall vector literals (up to length 8) are automatically replaced with StaticArrays.SVector for allocation-free ODE evaluation.\n\nMust be paired with @initialDE when used inside @Model.\n\n\n\n\n\n","category":"macro"},{"location":"api/#NoLimits.@initialDE","page":"API","title":"NoLimits.@initialDE","text":"@initialDE begin\n    state = expr\n    ...\nend\n\nCompile initial-condition declarations for an ODE system into an InitialDE struct.\n\nEach statement must assign a scalar expression to a state name matching one declared with D(state) ~ ... in the paired @DifferentialEquation block. Every state must have exactly one initial condition. The symbols t and ξ are forbidden.\n\nRight-hand sides may reference fixed effects, random effects, constant covariates, pre-DE variables, model functions, and helper functions.\n\n\n\n\n\n","category":"macro"},{"location":"api/#NoLimits.@formulas","page":"API","title":"NoLimits.@formulas","text":"@formulas begin\n    name = expr          # deterministic node\n    outcome ~ dist(...)  # observation node\n    ...\nend\n\nCompile the observation model into a Formulas struct.\n\nTwo statement forms are supported:\n\nname = expr — a deterministic intermediate variable. May reference any previously defined deterministic name or any model symbol.\noutcome ~ dist(...) — an observation distribution. The right-hand side must be a Distributions.Distribution constructor.\n\nSymbols are resolved from (in order): fixed effects, random effects, pre-DE variables, constant covariates, varying covariates, helper functions, model functions, and DE state/signal accessors. State and signal names must be called with a time argument: x1(t) or x1(t - offset).\n\nDynamic covariates used without an explicit (t) call are evaluated implicitly at the current time t.\n\nThe @formulas block is required in every @Model.\n\n\n\n\n\n","category":"macro"},{"location":"api/#NoLimits.RealNumber","page":"API","title":"NoLimits.RealNumber","text":"RealNumber(value; name, scale, lower, upper, prior, calculate_se) -> RealNumber\n\nA scalar real-valued fixed-effect parameter block.\n\nArguments\n\nvalue::Real: initial value on the natural (untransformed) scale.\n\nKeyword Arguments\n\nname::Symbol = :unnamed: parameter name (injected automatically by @fixedEffects).\nscale::Symbol = :identity: reparameterisation applied during optimisation. Must be one of REAL_SCALES (:identity, :log).\nlower::Real = -Inf: lower bound on the natural scale (defaults to EPSILON when scale=:log).\nupper::Real = Inf: upper bound on the natural scale.\nprior = Priorless(): prior distribution (Distributions.Distribution) or Priorless().\ncalculate_se::Bool = false: whether to include this parameter in standard-error calculations.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.RealVector","page":"API","title":"NoLimits.RealVector","text":"RealVector(value; name, scale, lower, upper, prior, calculate_se) -> RealVector\n\nA vector of real-valued fixed-effect parameters with per-element scale options.\n\nArguments\n\nvalue::AbstractVector{<:Real}: initial values on the natural scale.\n\nKeyword Arguments\n\nname::Symbol = :unnamed: parameter name (injected automatically by @fixedEffects).\nscale: per-element scale symbols. A single Symbol or a Vector{Symbol} of the same length as value. Each element must be in REAL_SCALES (:identity, :log). Defaults to all :identity.\nlower: lower bounds per element. Defaults to -Inf (or EPSILON for :log elements).\nupper: upper bounds per element. Defaults to Inf.\nprior = Priorless(): a Distributions.Distribution, a Vector{Distribution} of matching length, or Priorless().\ncalculate_se::Bool = false: whether to include this parameter in standard-error calculations.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.RealPSDMatrix","page":"API","title":"NoLimits.RealPSDMatrix","text":"RealPSDMatrix(value; name, scale, prior, calculate_se) -> RealPSDMatrix\n\nA symmetric positive semi-definite (PSD) matrix parameter block, typically used to parameterise covariance matrices of random-effect distributions.\n\nThe matrix is reparameterised during optimisation to ensure PSD constraints are automatically satisfied.\n\nArguments\n\nvalue::AbstractMatrix{<:Real}: initial symmetric PSD matrix.\n\nKeyword Arguments\n\nname::Symbol = :unnamed: parameter name (injected automatically by @fixedEffects).\nscale::Symbol = :cholesky: reparameterisation. Must be one of PSD_SCALES (:cholesky, :expm).\nprior = Priorless(): a Distributions.Distribution (e.g. Wishart) or Priorless().\ncalculate_se::Bool = false: whether to include this parameter in standard-error calculations.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.RealDiagonalMatrix","page":"API","title":"NoLimits.RealDiagonalMatrix","text":"RealDiagonalMatrix(value; name, scale, prior, calculate_se) -> RealDiagonalMatrix\n\nA diagonal positive-definite matrix parameter block, stored as a vector of the diagonal entries. Useful for diagonal covariance matrices.\n\nAll diagonal entries must be strictly positive. They are stored and optimised on the log scale.\n\nArguments\n\nvalue: initial diagonal entries as an AbstractVector{<:Real} or a diagonal AbstractMatrix. If a matrix is provided, off-diagonal entries are ignored with a warning.\n\nKeyword Arguments\n\nname::Symbol = :unnamed: parameter name (injected automatically by @fixedEffects).\nscale::Symbol = :log: reparameterisation. Must be in DIAGONAL_SCALES (:log).\nprior = Priorless(): a Distributions.Distribution or Priorless().\ncalculate_se::Bool = false: whether to include this parameter in standard-error calculations.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.ProbabilityVector","page":"API","title":"NoLimits.ProbabilityVector","text":"ProbabilityVector(value; name, scale, prior, calculate_se) -> ProbabilityVector\n\nA probability vector parameter block: a vector of k ≥ 2 non-negative entries summing to 1. Optimised via the logistic stick-breaking reparameterisation, which maps the simplex to k-1 unconstrained reals.\n\nArguments\n\nvalue::AbstractVector{<:Real}: initial probability vector. All entries must be non-negative and sum to 1 (within tolerance); if the sum differs by less than atol=1e-6, the vector is silently normalised.\n\nKeyword Arguments\n\nname::Symbol = :unnamed: parameter name (injected automatically by @fixedEffects).\nscale::Symbol = :stickbreak: reparameterisation. Must be in PROBABILITY_SCALES.\nprior = Priorless(): a Distributions.Distribution or Priorless().\ncalculate_se::Bool = false: whether to include this parameter in standard-error calculations.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.DiscreteTransitionMatrix","page":"API","title":"NoLimits.DiscreteTransitionMatrix","text":"DiscreteTransitionMatrix(value; name, scale, prior, calculate_se) -> DiscreteTransitionMatrix\n\nA square row-stochastic matrix parameter block of size n×n (n ≥ 2). Each row is a probability vector and is independently reparameterised via the logistic stick-breaking transform, yielding n*(n-1) unconstrained reals.\n\nArguments\n\nvalue::AbstractMatrix{<:Real}: initial row-stochastic matrix. Each row must be non-negative and sum to 1 (within tolerance); rows are silently normalised if needed.\n\nKeyword Arguments\n\nname::Symbol = :unnamed: parameter name (injected automatically by @fixedEffects).\nscale::Symbol = :stickbreakrows: reparameterisation. Must be in TRANSITION_SCALES.\nprior = Priorless(): a Distributions.Distribution or Priorless().\ncalculate_se::Bool = false: whether to include this parameter in standard-error calculations.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.ContinuousTransitionMatrix","page":"API","title":"NoLimits.ContinuousTransitionMatrix","text":"ContinuousTransitionMatrix(value; name, scale, prior, calculate_se) -> ContinuousTransitionMatrix\n\nAn n×n rate matrix (Q-matrix) parameter block for continuous-time Markov chains (n ≥ 2).\n\nThe Q-matrix has:\n\nOff-diagonal entries Q[i,j] ≥ 0 (transition rates from state i to state j, i ≠ j).\nDiagonal entries Q[i,i] = -∑_{j≠i} Q[i,j] (rows sum to zero).\n\nThe n*(n-1) off-diagonal rates are optimised on the log scale (:lograterows), mapping each rate to an unconstrained real via log. The diagonal is recomputed from the off-diagonals and is not an independent free parameter.\n\nArguments\n\nvalue::AbstractMatrix{<:Real}: initial n×n Q-matrix. Off-diagonal entries must be non-negative. The diagonal is always silently recomputed as -rowsum of the off-diagonals, so any diagonal values provided in value are ignored.\n\nKeyword Arguments\n\nname::Symbol = :unnamed: parameter name (injected automatically by @fixedEffects).\nscale::Symbol = :lograterows: reparameterisation. Must be in RATE_MATRIX_SCALES.\nprior = Priorless(): a Distributions.Distribution or Priorless().\ncalculate_se::Bool = false: whether to include this parameter in standard-error calculations.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.NNParameters","page":"API","title":"NoLimits.NNParameters","text":"NNParameters(chain; name, function_name, seed, prior, calculate_se) -> NNParameters\n\nA parameter block that wraps the flattened parameters of a Lux.jl neural-network chain.\n\nThe resulting parameter is optimised as a flat real vector. Inside model blocks (@randomEffects, @preDifferentialEquation, @formulas) the network is called as function_name(input, θ_slice), where θ_slice is the corresponding slice of the fixed-effects ComponentArray.\n\nArguments\n\nchain: a Lux.Chain defining the neural-network architecture.\n\nKeyword Arguments\n\nname::Symbol = :unnamed: parameter name (injected automatically by @fixedEffects).\nfunction_name::Symbol: the name used to call the network in model blocks.\nseed::Integer = 0: random seed for initialising the Lux parameters.\nprior = Priorless(): Priorless(), a Vector{Distribution} of length equal to the number of parameters, or a multivariate Distribution with matching length.\ncalculate_se::Bool = false: whether to include this parameter in standard-error calculations.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.NPFParameter","page":"API","title":"NoLimits.NPFParameter","text":"NPFParameter(n_input, n_layers; name, seed, init, prior, calculate_se) -> NPFParameter\n\nA parameter block for a Normalizing Planar Flow (NPF), enabling flexible non-Gaussian distributions in @randomEffects via RandomEffect(NormalizingPlanarFlow(ψ); column=:ID).\n\nThe flow is composed of n_layers planar transformations on an n_input-dimensional base Gaussian. Parameters are stored as a flat real vector.\n\nArguments\n\nn_input::Integer: dimensionality of the latent space (typically 1 for scalar random effects).\nn_layers::Integer: number of planar flow layers.\n\nKeyword Arguments\n\nname::Symbol = :unnamed: parameter name (injected automatically by @fixedEffects).\nseed::Integer = 0: random seed for initialisation.\ninit::Function: weight initialisation function; defaults to x -> sqrt(1/n_input) .* x.\nprior = Priorless(): Priorless(), a Vector{Distribution}, or a multivariate Distribution.\ncalculate_se::Bool = false: whether to include this parameter in standard-error calculations.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.SoftTreeParameters","page":"API","title":"NoLimits.SoftTreeParameters","text":"SoftTreeParameters(input_dim, depth; name, function_name, n_output, seed, prior, calculate_se) -> SoftTreeParameters\n\nA parameter block for a soft decision tree whose parameters are optimised as fixed effects.\n\nThe tree takes a real-valued vector of length input_dim and produces a vector of length n_output. Parameters are stored as a flat real vector. Inside model blocks the tree is called as function_name(x, θ_slice).\n\nArguments\n\ninput_dim::Integer: number of input features.\ndepth::Integer: depth of the tree (number of internal split levels).\n\nKeyword Arguments\n\nname::Symbol = :unnamed: parameter name (injected automatically by @fixedEffects).\nfunction_name::Symbol: the name used to call the tree in model blocks.\nn_output::Integer = 1: number of output values.\nseed::Integer = 0: random seed for initialisation.\nprior = Priorless(): Priorless(), a Vector{Distribution}, or a multivariate Distribution.\ncalculate_se::Bool = false: whether to include this parameter in standard-error calculations.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.SplineParameters","page":"API","title":"NoLimits.SplineParameters","text":"SplineParameters(knots; name, function_name, degree, prior, calculate_se) -> SplineParameters\n\nA parameter block for a B-spline function whose coefficients are optimised as fixed effects.\n\nThe number of coefficients is determined by length(knots) - degree - 1. All coefficients are initialised to zero. Inside model blocks the spline is evaluated as function_name(x, θ_slice).\n\nArguments\n\nknots::AbstractVector{<:Real}: B-spline knot vector (including boundary knots).\n\nKeyword Arguments\n\nname::Symbol = :unnamed: parameter name (injected automatically by @fixedEffects).\nfunction_name::Symbol: the name used to call the spline in model blocks.\ndegree::Integer = 3: polynomial degree of the B-spline (e.g. 2 for quadratic, 3 for cubic).\nprior = Priorless(): Priorless(), a Vector{Distribution}, or a multivariate Distribution.\ncalculate_se::Bool = false: whether to include this parameter in standard-error calculations.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.Priorless","page":"API","title":"NoLimits.Priorless","text":"Priorless()\n\nSentinel type indicating that no prior distribution is assigned to a parameter. Used as the default prior value in all parameter block constructors.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.Covariate","page":"API","title":"NoLimits.Covariate","text":"Covariate() -> Covariate\n\nA time-varying scalar covariate read row-by-row from the data frame.\n\nIn @covariates, the LHS name determines the data column and must refer to a column present in the data frame. This type is also used to declare the time column:\n\n@covariates begin\n    t = Covariate()\n    z = Covariate()\nend\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.CovariateVector","page":"API","title":"NoLimits.CovariateVector","text":"CovariateVector(columns::Vector{Symbol}) -> CovariateVector\n\nA vector of time-varying scalar covariates read row-by-row.\n\n@covariates begin\n    z = CovariateVector([:z1, :z2])\nend\n\nAccessed in model blocks as z.z1, z.z2.\n\nArguments\n\ncolumns::Vector{Symbol}: names of the data-frame columns to collect.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.ConstantCovariate","page":"API","title":"NoLimits.ConstantCovariate","text":"ConstantCovariate(; constant_on) -> ConstantCovariate\n\nA scalar covariate that is constant within a grouping (e.g. a subject-level baseline).\n\nIn @covariates, the LHS name determines the data column:\n\n@covariates begin\n    Age = ConstantCovariate(; constant_on=:ID)\nend\n\nKeyword Arguments\n\nconstant_on: a Symbol or vector of Symbols naming the grouping column(s) within which the covariate is constant. When only one random-effect group exists, this defaults to that group's column.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.ConstantCovariateVector","page":"API","title":"NoLimits.ConstantCovariateVector","text":"ConstantCovariateVector(columns::Vector{Symbol}; constant_on) -> ConstantCovariateVector\n\nA vector of scalar covariates, each constant within a grouping.\n\nThe LHS name in @covariates becomes the accessor name; columns specifies which data-frame columns to read:\n\n@covariates begin\n    x = ConstantCovariateVector([:Age, :BMI]; constant_on=:ID)\nend\n\nAccessed in model blocks as x.Age, x.BMI.\n\nKeyword Arguments\n\nconstant_on: a Symbol or vector of Symbols naming the grouping column(s).\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.DynamicCovariate","page":"API","title":"NoLimits.DynamicCovariate","text":"DynamicCovariate(; interpolation=LinearInterpolation) -> DynamicCovariate\n\nA time-varying covariate represented as a DataInterpolations.jl interpolant, callable as w(t) inside @DifferentialEquation and @formulas.\n\nIn @covariates, the LHS name provides both the accessor name and the data-frame column.\n\nKeyword Arguments\n\ninterpolation: a DataInterpolations.jl interpolation type (not instance). Must be one of ConstantInterpolation, SmoothedConstantInterpolation, LinearInterpolation, QuadraticInterpolation, LagrangeInterpolation, QuadraticSpline, CubicSpline, or AkimaInterpolation. Defaults to LinearInterpolation.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.DynamicCovariateVector","page":"API","title":"NoLimits.DynamicCovariateVector","text":"DynamicCovariateVector(columns::Vector{Symbol}; interpolations) -> DynamicCovariateVector\n\nA vector of time-varying covariates, each represented as a separate interpolant.\n\n@covariates begin\n    inputs = DynamicCovariateVector([:i1, :i2]; interpolations=[LinearInterpolation, CubicSpline])\nend\n\nArguments\n\ncolumns::Vector{Symbol}: data-frame column names.\n\nKeyword Arguments\n\ninterpolations: a Vector of DataInterpolations.jl types, one per column. Defaults to LinearInterpolation for all columns.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.RandomEffect","page":"API","title":"NoLimits.RandomEffect","text":"RandomEffect(dist; column::Symbol) -> RandomEffectDecl\n\nDeclare a random effect with the given distribution and grouping column.\n\nUsed exclusively inside @randomEffects:\n\n@randomEffects begin\n    η = RandomEffect(Normal(0.0, σ); column=:ID)\nend\n\nArguments\n\ndist: a distribution expression (evaluated at model construction time). May reference fixed effects, constant covariates, helper functions, and model functions. The symbols t and ξ are forbidden (random effects are time-constant).\n\nKeyword Arguments\n\ncolumn::Symbol: the data-frame column that defines the grouping for this random effect.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.Model","page":"API","title":"NoLimits.Model","text":"Model{F, R, C, D, H, O}\n\nTop-level model struct produced by the @Model macro. Bundles all model components: fixed effects, random effects, covariates, ODE system, helper functions, and observation formulas.\n\nFields\n\nfixed::FixedBundle: fixed-effects data (parameters, transforms, model functions).\nrandom::RandomBundle: random-effects data (distributions, logpdf).\ncovariates::CovariatesBundle: covariate metadata.\nde::DEBundle: ODE system components (may hold nothing when no DE is defined).\nhelpers::HelpersBundle: user-defined helper functions.\nformulas::FormulasBundle: observation model (deterministic + observation nodes).\n\nUse accessor functions get_model_funs, get_helper_funs, get_solver_config, etc. rather than accessing fields directly.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.ODESolverConfig","page":"API","title":"NoLimits.ODESolverConfig","text":"ODESolverConfig{A, K, T}\n\nConfiguration for the ODE solver used when integrating the @DifferentialEquation block.\n\nFields\n\nalg: ODE algorithm (e.g. Tsit5(), CVODE_BDF()). nothing uses the SciML default.\nkwargs::NamedTuple: keyword arguments forwarded to solve (e.g. abstol, reltol).\nargs::Tuple: positional arguments forwarded to solve.\nsaveat_mode::Symbol: one of :dense, :saveat, or :auto.\n:dense — full dense solution (required for non-constant time offsets in formulas).\n:saveat — save only at observation + event + formula-offset times.\n:auto — resolves to :saveat unless non-constant time offsets require :dense.\n\nConstructed by the @Model macro with defaults and updated via set_solver_config.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.set_solver_config","page":"API","title":"NoLimits.set_solver_config","text":"set_solver_config(m::Model, cfg::ODESolverConfig) -> Model\nset_solver_config(m::Model; alg, kwargs, args, saveat_mode) -> Model\n\nReturn a new Model with the ODE solver configuration replaced by cfg. The keyword form constructs a new ODESolverConfig from the given keyword arguments and replaces the existing configuration.\n\nKeyword Arguments\n\nalg: ODE algorithm (e.g. Tsit5()). nothing uses the SciML default.\nkwargs = NamedTuple(): keyword arguments forwarded to solve.\nargs = (): positional arguments forwarded to solve.\nsaveat_mode::Symbol = :dense: save-time mode (:dense, :saveat, or :auto).\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_model_funs","page":"API","title":"NoLimits.get_model_funs","text":"get_model_funs(fe::FixedEffects) -> NamedTuple\n\nReturn a NamedTuple of callable model functions derived from NNParameters, SoftTreeParameters, SplineParameters, and NPFParameter blocks. Each function has the signature matching its block type's function_name.\n\n\n\n\n\nget_model_funs(m::Model) -> NamedTuple\n\nReturn the NamedTuple of callable model functions derived from NNParameters, SoftTreeParameters, SplineParameters, and NPFParameter blocks in @fixedEffects.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_helper_funs","page":"API","title":"NoLimits.get_helper_funs","text":"get_helper_funs(m::Model) -> NamedTuple\n\nReturn the NamedTuple of user-defined helper functions from the @helpers block.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_solver_config","page":"API","title":"NoLimits.get_solver_config","text":"get_solver_config(m::Model) -> ODESolverConfig\n\nReturn the ODESolverConfig controlling how the ODE is solved.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.FixedEffects","page":"API","title":"NoLimits.FixedEffects","text":"FixedEffects\n\nCompiled representation of a @fixedEffects block. Contains initial parameter values, bounds, forward/inverse transforms, priors, SE masks, and model functions (NNs, splines, soft trees, NPFs).\n\nUse accessor functions rather than accessing fields directly.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.Covariates","page":"API","title":"NoLimits.Covariates","text":"Covariates\n\nCompiled representation of a @covariates block. Stores the covariate names categorised as constant, varying, or dynamic, along with interpolation types and the raw covariate parameter structs.\n\nUse the model.covariates.covariates field (or the CovariatesBundle) to inspect this struct. Typically accessed indirectly via DataModel construction.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.finalize_covariates","page":"API","title":"NoLimits.finalize_covariates","text":"finalize_covariates(covariates::Covariates, random_effects::RandomEffects) -> Covariates\n\nResolve the constant_on grouping column for each ConstantCovariate / ConstantCovariateVector that did not specify one explicitly.\n\nWhen there is exactly one random-effect grouping column, constant_on defaults to it.\nWhen there are multiple grouping columns, constant_on must be explicit.\nValidates that covariates used inside random-effect distributions are declared constant_on for the correct grouping column.\n\nThis function is called automatically by @Model after @covariates and @randomEffects are evaluated.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.RandomEffects","page":"API","title":"NoLimits.RandomEffects","text":"RandomEffects\n\nCompiled representation of a @randomEffects block. Stores metadata (names, grouping columns, distribution types, symbol dependencies) and runtime builder functions for constructing distributions and evaluating log-densities.\n\nUse accessor functions rather than accessing fields directly.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.PreDifferentialEquation","page":"API","title":"NoLimits.PreDifferentialEquation","text":"PreDifferentialEquation\n\nCompiled representation of a @preDifferentialEquation block. Stores derived variable names, symbol dependencies, raw expression lines, and a runtime builder function.\n\nPre-DE variables are time-constant quantities computed from fixed effects, random effects, and constant covariates before the ODE is solved. They are available inside @DifferentialEquation and @initialDE.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.DifferentialEquation","page":"API","title":"NoLimits.DifferentialEquation","text":"DifferentialEquation\n\nCompiled representation of a @DifferentialEquation block. Stores state and signal names, in-place/out-of-place ODE functions, a parameter compiler, and an accessor builder for retrieving state/signal values from a solution object.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.InitialDE","page":"API","title":"NoLimits.InitialDE","text":"InitialDE\n\nCompiled representation of an @initialDE block. Stores state names and the intermediate representation (IR) of initial-condition expressions. A runtime builder function is produced lazily via get_initialde_builder.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.get_initialde_builder","page":"API","title":"NoLimits.get_initialde_builder","text":"get_initialde_builder(i::InitialDE, state_names::Vector{Symbol}; static=false) -> Function\n\nBuild and return the initial-condition function for the ODE system.\n\nThe returned function has signature:\n\n(θ::ComponentArray, η::ComponentArray, const_cov::NamedTuple,\n model_funs::NamedTuple, helpers::NamedTuple, preDE::NamedTuple) -> Vector\n\nIt returns the initial state vector ordered to match state_names.\n\nArguments\n\ni::InitialDE: the compiled initial-condition block.\nstate_names::Vector{Symbol}: ordered state names from the @DifferentialEquation block.\n\nKeyword Arguments\n\nstatic::Bool = false: if true, returns a StaticArrays.SVector for allocation-free ODE solving with small state dimensions.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.Formulas","page":"API","title":"NoLimits.Formulas","text":"Formulas\n\nCompiled representation of a @formulas block. Stores deterministic-node and observation-node names and expressions in an intermediate representation (FormulasIR).\n\nBuilder functions are produced via get_formulas_builders.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.get_formulas_builders","page":"API","title":"NoLimits.get_formulas_builders","text":"get_formulas_builders(f::Formulas; fixed_names, random_names, prede_names,\n                      const_cov_names, varying_cov_names, helper_names,\n                      model_fun_names, state_names, signal_names,\n                      index_sym) -> (all_fn, obs_fn, req_states, req_signals)\n\nCompile the formula expressions into two runtime-generated functions and return them together with lists of required DE states and signals.\n\nReturns\n\nall_fn: function (ctx, sol_accessors, const_cov_i, vary_cov) -> NamedTuple evaluating all deterministic and observation nodes.\nobs_fn: function (ctx, sol_accessors, const_cov_i, vary_cov) -> NamedTuple evaluating observation nodes only.\nreq_states::Vector{Symbol}: DE state names that are accessed in the formulas.\nreq_signals::Vector{Symbol}: derived signal names that are accessed in the formulas.\n\nKeyword Arguments\n\nfixed_names, random_names, prede_names, const_cov_names, varying_cov_names: symbol lists from each model namespace.\nhelper_names, model_fun_names: callable symbol lists.\nstate_names, signal_names: DE state and signal names for time-call rewriting.\nindex_sym::Symbol = :t: the varying-covariates key used to extract the current time.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.DataModel","page":"API","title":"NoLimits.DataModel","text":"DataModel{M, D, I, P, C, K, G, R}\n\nTop-level struct pairing a Model with a dataset. Produced by the DataModel constructor and passed to fit_model and plotting functions.\n\nUse accessor functions rather than accessing fields directly: get_model, get_df, get_individuals, get_individual, get_batches, get_batch_ids, get_primary_id, get_row_groups, get_re_group_info, get_re_indices.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.get_individuals","page":"API","title":"NoLimits.get_individuals","text":"get_individuals(dm::DataModel) -> Vector{Individual}\n\nReturn the vector of Individual structs (one per unique primary-id value).\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_individual","page":"API","title":"NoLimits.get_individual","text":"get_individual(dm::DataModel, id) -> Individual\n\nReturn the Individual struct for the given primary-id value. Raises an error if the id is not found.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_batches","page":"API","title":"NoLimits.get_batches","text":"get_batches(dm::DataModel) -> Vector{Vector{Int}}\n\nReturn the list of batches, where each batch is a vector of individual indices. Individuals in the same batch share at least one random-effect level and must be estimated jointly.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_batch_ids","page":"API","title":"NoLimits.get_batch_ids","text":"get_batch_ids(dm::DataModel) -> Vector{Int}\n\nReturn the batch index for each individual (length equals number of individuals).\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_primary_id","page":"API","title":"NoLimits.get_primary_id","text":"get_primary_id(dm::DataModel) -> Symbol\n\nReturn the primary individual-grouping column name.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_df","page":"API","title":"NoLimits.get_df","text":"get_df(dm::DataModel) -> DataFrame\n\nReturn the original DataFrame used to construct the DataModel.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_model","page":"API","title":"NoLimits.get_model","text":"get_model(dm::DataModel) -> Model\n\nReturn the Model stored in the DataModel.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_row_groups","page":"API","title":"NoLimits.get_row_groups","text":"get_row_groups(dm::DataModel) -> RowGroups\n\nReturn the RowGroups struct mapping each individual index to its data-frame row indices (all rows and observation-only rows).\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_re_group_info","page":"API","title":"NoLimits.get_re_group_info","text":"get_re_group_info(dm::DataModel) -> REGroupInfo\n\nReturn the REGroupInfo struct containing random-effect level values and per-row level indices.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_re_indices","page":"API","title":"NoLimits.get_re_indices","text":"get_re_indices(dm::DataModel, id_or_ind_or_idx; obs_only=true) -> NamedTuple\n\nReturn a NamedTuple mapping each random-effect name to a vector of level indices for the specified individual.\n\nThe individual can be identified by its primary-id value, an Individual object, or its integer position in get_individuals(dm).\n\nKeyword Arguments\n\nobs_only::Bool = true: if true, only return indices for observation rows; if false, include all rows (including event rows).\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.ModelSummary","page":"API","title":"NoLimits.ModelSummary","text":"ModelSummary\n\nStructured summary of a Model. Created by summarize(model).\n\nProvides counts and lists of all model components: fixed effects, random effects, covariates, deterministic formulas, outcome distributions, and differential equation states/signals. Displayed via Base.show.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.DataModelSummary","page":"API","title":"NoLimits.DataModelSummary","text":"DataModelSummary\n\nStructured summary of a DataModel. Created by summarize(dm).\n\nProvides individual-level, covariate, outcome, and random-effects statistics, as well as data-quality checks (duplicate times, non-monotonic time, missing values). Displayed via Base.show.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.DescriptiveStats","page":"API","title":"NoLimits.DescriptiveStats","text":"DescriptiveStats\n\nDescriptive statistics for a numeric variable. Contains n, mean, sd, min, q25, median, q75, and max.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.summarize","page":"API","title":"NoLimits.summarize","text":"summarize(m::Model) -> ModelSummary\nsummarize(dm::DataModel) -> DataModelSummary\nsummarize(res::FitResult; scale, include_non_se, constants_re) -> FitResultSummary\nsummarize(uq::UQResult; scale) -> UQResultSummary\nsummarize(res::FitResult, uq::UQResult; scale, include_non_se, constants_re) -> FitResultSummary\n\nCompute a structured summary of a model, data model, fit result, or UQ result.\n\nEach overload returns a specialised summary struct that has a pretty-printed show method for interactive inspection.\n\nKeyword Arguments (for fit/UQ overloads)\n\nscale::Symbol = :natural: parameter scale to report (:natural or :transformed).\ninclude_non_se::Bool = false: include parameters marked calculate_se=false.\nconstants_re::NamedTuple = NamedTuple(): constants for random-effects reporting.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.FittingMethod","page":"API","title":"NoLimits.FittingMethod","text":"FittingMethod\n\nAbstract base type for all estimation methods. Concrete subtypes include MLE, MAP, MCMC, Laplace, LaplaceMAP, SAEM, MCEM, and Multistart.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.MethodResult","page":"API","title":"NoLimits.MethodResult","text":"MethodResult\n\nAbstract base type for the method-specific result structs stored inside FitResult. Each FittingMethod subtype has a corresponding MethodResult subtype.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.FitResult","page":"API","title":"NoLimits.FitResult","text":"FitResult{M, R, S, D, DM, A, K}\n\nUnified result wrapper returned by fit_model. Contains the fitting method, method-specific result, summary, diagnostics, and optionally the DataModel.\n\nUse accessor functions rather than accessing fields directly: get_summary, get_diagnostics, get_result, get_method, get_objective, get_converged, get_params, get_data_model.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.FitSummary","page":"API","title":"NoLimits.FitSummary","text":"FitSummary{O, C, P, N}\n\nHigh-level summary of a fitting result.\n\nFields\n\nobjective::O: the final objective value (negative log-likelihood, negative log-posterior, etc.).\nconverged::C: convergence flag (true / false / nothing for MCMC).\nparams::P: a FitParameters struct with parameter estimates.\nnotes::N: method-specific string notes or nothing.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.FitDiagnostics","page":"API","title":"NoLimits.FitDiagnostics","text":"FitDiagnostics{T, O, X, N}\n\nDiagnostic information for a fitting run.\n\nFields\n\ntiming::T: elapsed time in seconds.\noptimizer::O: optimizer-specific diagnostic (e.g. Optim.jl result).\nconvergence::X: convergence-related metadata.\nnotes::N: additional string notes.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.FitParameters","page":"API","title":"NoLimits.FitParameters","text":"FitParameters{T, U}\n\nStores parameter estimates on both the transformed (optimisation) and untransformed (natural) scales as ComponentArrays.\n\nFields\n\ntransformed::T: parameter vector on the optimisation scale.\nuntransformed::U: parameter vector on the natural scale.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.fit_model","page":"API","title":"NoLimits.fit_model","text":"fit_model(dm::DataModel, method::FittingMethod; constants, penalty,\n          ode_args, ode_kwargs, serialization, rng,\n          theta_0_untransformed, store_data_model) -> FitResult\n\nFit a model to data using the specified estimation method.\n\nArguments\n\ndm::DataModel: the data model.\nmethod::FittingMethod: estimation method (e.g. MLE(), Laplace(), MCMC(...)).\n\nKeyword Arguments\n\nconstants::NamedTuple = NamedTuple(): fix named parameters at given values on the natural scale. Fixed parameters are removed from the optimiser state.\npenalty::NamedTuple = NamedTuple(): add per-parameter quadratic penalties on the natural scale (not available for MCMC).\node_args::Tuple = (): extra positional arguments forwarded to the ODE solver.\node_kwargs::NamedTuple = NamedTuple(): extra keyword arguments forwarded to the ODE solver.\nserialization = EnsembleSerial(): parallelisation strategy.\nrng = Random.default_rng(): random number generator (used by MCMC/SAEM/MCEM).\ntheta_0_untransformed::Union{Nothing, ComponentArray} = nothing: custom starting point on the natural scale; defaults to the model's declared initial values.\nstore_data_model::Bool = true: whether to store a reference to dm in the result.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.MLE","page":"API","title":"NoLimits.MLE","text":"MLE(; optimizer, optim_kwargs, adtype, lb, ub) <: FittingMethod\n\nMaximum Likelihood Estimation for models without random effects.\n\nKeyword Arguments\n\noptimizer: Optimization.jl-compatible optimiser. Defaults to LBFGS with backtracking line search.\noptim_kwargs::NamedTuple = NamedTuple(): keyword arguments forwarded to Optimization.solve (e.g. maxiters, reltol).\nadtype: automatic-differentiation backend. Defaults to AutoForwardDiff().\nlb: lower bounds on the transformed parameter scale, or nothing to use the model-declared bounds.\nub: upper bounds on the transformed parameter scale, or nothing.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.MAP","page":"API","title":"NoLimits.MAP","text":"MAP(; optimizer, optim_kwargs, adtype, lb, ub) <: FittingMethod\n\nMaximum A Posteriori estimation for models without random effects. Requires prior distributions on at least one free fixed effect.\n\nKeyword Arguments\n\noptimizer: Optimization.jl-compatible optimiser. Defaults to LBFGS with backtracking line search.\noptim_kwargs::NamedTuple = NamedTuple(): keyword arguments forwarded to Optimization.solve (e.g. maxiters, reltol).\nadtype: automatic-differentiation backend. Defaults to AutoForwardDiff().\nlb: lower bounds on the transformed parameter scale, or nothing to use the model-declared bounds.\nub: upper bounds on the transformed parameter scale, or nothing.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.Laplace","page":"API","title":"NoLimits.Laplace","text":"Laplace(; optimizer, optim_kwargs, adtype, inner_options, hessian_options,\n          cache_options, multistart_options, inner_optimizer, inner_kwargs,\n          inner_adtype, inner_grad_tol, multistart_n, multistart_k,\n          multistart_grad_tol, multistart_max_rounds, multistart_sampling,\n          jitter, max_tries, jitter_growth, adaptive_jitter, jitter_scale,\n          use_trace_logdet_grad, use_hutchinson, hutchinson_n, theta_tol,\n          lb, ub) <: FittingMethod\n\nLaplace approximation with Empirical Bayes Estimates (EBE) for random-effects models. The outer optimiser maximises the Laplace-approximated marginal likelihood over the fixed effects, while the inner optimiser computes per-individual MAP estimates of the random effects.\n\nKeyword Arguments\n\noptimizer: outer Optimization.jl optimiser. Defaults to LBFGS with backtracking.\noptim_kwargs::NamedTuple = NamedTuple(): keyword arguments for the outer solve call.\nadtype: AD backend for the outer optimiser. Defaults to AutoForwardDiff().\ninner_optimizer: inner optimiser for computing EBE modes. Defaults to LBFGS.\ninner_kwargs::NamedTuple = NamedTuple(): keyword arguments for the inner solve call.\ninner_adtype: AD backend for the inner optimiser. Defaults to AutoForwardDiff().\ninner_grad_tol: gradient tolerance for inner convergence (:auto chooses automatically).\nmultistart_n::Int = 50: number of random starts for the inner EBE multistart.\nmultistart_k::Int = 10: number of best starts to refine in the inner multistart.\nmultistart_grad_tol: gradient tolerance for multistart refinement.\nmultistart_max_rounds::Int = 1: maximum multistart refinement rounds.\nmultistart_sampling::Symbol = :lhs: inner multistart sampling strategy (:lhs or :random).\njitter::Float64 = 1e-6: initial diagonal jitter added to ensure Hessian PD.\nmax_tries::Int = 6: maximum attempts to regularise the Hessian.\njitter_growth::Float64 = 10.0: multiplicative growth factor for jitter on each retry.\nadaptive_jitter::Bool = true: whether to adapt jitter magnitude based on scale.\njitter_scale::Float64 = 1e-6: scale for the adaptive jitter.\nuse_trace_logdet_grad::Bool = true: use trace estimator for log-determinant gradient.\nuse_hutchinson::Bool = false: use Hutchinson estimator instead of Cholesky for log-det.\nhutchinson_n::Int = 8: number of Rademacher vectors for the Hutchinson estimator.\ntheta_tol::Float64 = 0.0: fixed-effect change tolerance for EBE caching.\nlb, ub: bounds on the transformed fixed-effect scale, or nothing.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.LaplaceMAP","page":"API","title":"NoLimits.LaplaceMAP","text":"LaplaceMAP(; optimizer, optim_kwargs, adtype, inner_options, hessian_options,\n             cache_options, multistart_options, inner_optimizer, inner_kwargs,\n             inner_adtype, inner_grad_tol, multistart_n, multistart_k,\n             multistart_grad_tol, multistart_max_rounds, multistart_sampling,\n             jitter, max_tries, jitter_growth, adaptive_jitter, jitter_scale,\n             use_trace_logdet_grad, use_hutchinson, hutchinson_n, theta_tol,\n             lb, ub) <: FittingMethod\n\nLaplace approximation with MAP-regularised fixed effects for random-effects models. Identical to Laplace but adds the log-prior of the fixed effects to the outer objective, giving a MAP estimate of the fixed effects rather than MLE. Requires prior distributions on at least one free fixed effect.\n\nSee Laplace for a description of all keyword arguments. The only difference in defaults is multistart_max_rounds = 5.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.MCEM","page":"API","title":"NoLimits.MCEM","text":"MCEM(; optimizer, optim_kwargs, adtype, sampler, turing_kwargs, sample_schedule,\n       warm_start, verbose, progress, maxiters, rtol_theta, atol_theta, rtol_Q,\n       atol_Q, consecutive_params, ebe_optimizer, ebe_optim_kwargs, ebe_adtype,\n       ebe_grad_tol, ebe_multistart_n, ebe_multistart_k, ebe_multistart_max_rounds,\n       ebe_multistart_sampling, ebe_rescue_on_high_grad, ebe_rescue_multistart_n,\n       ebe_rescue_multistart_k, ebe_rescue_max_rounds, ebe_rescue_grad_tol,\n       ebe_rescue_multistart_sampling, lb, ub) <: FittingMethod\n\nMonte Carlo Expectation-Maximisation for random-effects models. At each EM iteration the E-step draws random effects from the posterior using a Turing.jl sampler; the M-step maximises the Monte Carlo Q-function over the fixed effects.\n\nKeyword Arguments\n\noptimizer: M-step Optimization.jl optimiser. Defaults to LBFGS with backtracking.\noptim_kwargs::NamedTuple = NamedTuple(): keyword arguments for the M-step solve.\nadtype: AD backend for the M-step. Defaults to AutoForwardDiff().\nsampler: Turing-compatible sampler for the E-step. Defaults to NUTS(0.75).\nturing_kwargs::NamedTuple = NamedTuple(): keyword arguments forwarded to Turing.sample.\nsample_schedule::Int = 250: number of MCMC samples per E-step iteration.\nwarm_start::Bool = true: initialise sampler from the previous iteration's modes.\nverbose::Bool = false: print per-iteration diagnostics.\nprogress::Bool = true: show a progress bar.\nmaxiters::Int = 100: maximum number of EM iterations.\nrtol_theta, atol_theta: relative/absolute convergence tolerance on fixed effects.\nrtol_Q, atol_Q: relative/absolute convergence tolerance on the Q-function.\nconsecutive_params::Int = 3: number of consecutive iterations satisfying the tolerance required to declare convergence.\nebe_optimizer, ebe_optim_kwargs, ebe_adtype, ebe_grad_tol: EBE inner optimiser settings used to compute mode starting points.\nebe_multistart_n, ebe_multistart_k, ebe_multistart_max_rounds, ebe_multistart_sampling: multistart settings for EBE mode computation.\nebe_rescue_on_high_grad, ebe_rescue_multistart_n, ebe_rescue_multistart_k, ebe_rescue_max_rounds, ebe_rescue_grad_tol, ebe_rescue_multistart_sampling: rescue multistart settings when an EBE mode has a high gradient norm.\nlb, ub: bounds on the transformed fixed-effect scale, or nothing.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.SAEM","page":"API","title":"NoLimits.SAEM","text":"SAEM(; optimizer, optim_kwargs, adtype, sampler, turing_kwargs, update_schedule,\n       warm_start, verbose, progress, mcmc_steps, max_store, t0, kappa, maxiters,\n       rtol_theta, atol_theta, rtol_Q, atol_Q, consecutive_params, suffstats,\n       q_from_stats, mstep_closed_form, builtin_stats, builtin_mean,\n       resid_var_param, re_cov_params, re_mean_params, ebe_optimizer,\n       ebe_optim_kwargs, ebe_adtype, ebe_grad_tol, ebe_multistart_n,\n       ebe_multistart_k, ebe_multistart_max_rounds, ebe_multistart_sampling,\n       ebe_rescue_on_high_grad, ebe_rescue_multistart_n, ebe_rescue_multistart_k,\n       ebe_rescue_max_rounds, ebe_rescue_grad_tol, ebe_rescue_multistart_sampling,\n       lb, ub) <: FittingMethod\n\nStochastic Approximation Expectation-Maximisation for random-effects models. SAEM maintains a stochastic approximation of the sufficient statistics using a decreasing step-size sequence; the M-step updates the fixed effects via gradient-based optimisation or closed-form updates (when builtin_stats is enabled).\n\nKeyword Arguments\n\noptimizer: M-step Optimization.jl optimiser. Defaults to LBFGS with backtracking.\noptim_kwargs::NamedTuple = NamedTuple(): keyword arguments for the M-step solve.\nadtype: AD backend for the M-step. Defaults to AutoForwardDiff().\nsampler: Turing-compatible sampler. Defaults to NUTS(0.75).\nturing_kwargs::NamedTuple = NamedTuple(): keyword arguments for Turing.sample.\nupdate_schedule: which parameters to update per iteration (:all or a Vector{Symbol}).\nwarm_start::Bool = true: initialise the sampler from the previous iteration's modes.\nverbose::Bool = false: print per-iteration diagnostics.\nprogress::Bool = true: show a progress bar.\nmcmc_steps::Int = 80: number of MCMC steps per E-step.\nmax_store::Int = 50: size of the sufficient statistic history window.\nt0::Int = 20: burn-in iterations before stochastic approximation averaging begins.\nkappa::Float64 = 0.65: step-size decay exponent for the Robbins-Monro schedule.\nmaxiters::Int = 300: maximum number of SAEM iterations.\nrtol_theta, atol_theta: relative/absolute convergence tolerance on fixed effects.\nrtol_Q, atol_Q: relative/absolute convergence tolerance on the Q-function.\nconsecutive_params::Int = 4: consecutive iterations satisfying tolerance to converge.\nsuffstats: custom sufficient statistics function, or nothing to use the built-in.\nq_from_stats: custom Q-function from sufficient statistics, or nothing.\nmstep_closed_form: custom closed-form M-step function, or nothing.\nbuiltin_stats: :auto, :on, or :off; controls use of built-in Gaussian statistics.\nbuiltin_mean: :none, :additive, or :all; controls built-in mean parameterisation.\nresid_var_param::Symbol = :σ: fixed-effect name for the residual standard deviation.\nre_cov_params::NamedTuple = NamedTuple(): mapping of RE name to covariance parameter.\nre_mean_params::NamedTuple = NamedTuple(): mapping of RE name to mean parameter.\nebe_optimizer, ebe_optim_kwargs, ebe_adtype, ebe_grad_tol: EBE inner optimiser.\nebe_multistart_n, ebe_multistart_k, ebe_multistart_max_rounds, ebe_multistart_sampling: multistart settings for EBE mode computation.\nebe_rescue_*: rescue multistart settings when an EBE mode has a high gradient norm.\nlb, ub: bounds on the transformed fixed-effect scale, or nothing.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.MCMC","page":"API","title":"NoLimits.MCMC","text":"MCMC(; sampler, turing_kwargs, adtype, progress) <: FittingMethod\n\nBayesian sampling via Turing.jl for models with or without random effects. All free fixed effects and random effects must have prior distributions.\n\nKeyword Arguments\n\nsampler: Turing-compatible sampler. Defaults to NUTS(0.75).\nturing_kwargs::NamedTuple = NamedTuple(): keyword arguments forwarded to Turing.sample (e.g. n_samples, n_adapt).\nadtype: automatic-differentiation backend. Defaults to AutoForwardDiff().\nprogress::Bool = false: whether to display a progress bar during sampling.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.Multistart","page":"API","title":"NoLimits.Multistart","text":"Multistart(; dists, n_draws_requested, n_draws_used, sampling, serialization, rng)\n\nMultistart wrapper that runs any optimization-based fitting method from multiple initial parameter vectors and returns the best result.\n\nStarting points are drawn either from the fixed-effect priors or from user-supplied dists; the top-n_draws_used candidates (by a cheap objective evaluation) are then fully optimised.\n\nKeyword Arguments\n\ndists::NamedTuple = NamedTuple(): per-parameter sampling distributions, keyed by fixed-effect name. Parameters without an entry use their prior, if available.\nn_draws_requested::Int = 100: number of candidate starting points to sample.\nn_draws_used::Int = 50: number of candidates to fully optimise after screening.\nsampling::Symbol = :random: sampling strategy for starting points: :random or :lhs (Latin hypercube sampling).\nserialization::SciMLBase.EnsembleAlgorithm = EnsembleSerial(): parallelisation strategy for running multiple starts.\nrng::AbstractRNG = Random.default_rng(): random-number generator.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.MLEResult","page":"API","title":"NoLimits.MLEResult","text":"MLEResult{S, O, I, R, N} <: MethodResult\n\nMethod-specific result from an MLE fit. Stores the solution, objective value, iteration count, raw Optimization.jl result, and optional notes.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.MAPResult","page":"API","title":"NoLimits.MAPResult","text":"MAPResult{S, O, I, R, N} <: MethodResult\n\nMethod-specific result from a MAP fit. Stores the solution, objective value, iteration count, raw Optimization.jl result, and optional notes.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.LaplaceResult","page":"API","title":"NoLimits.LaplaceResult","text":"LaplaceResult{S, O, I, R, N, B} <: MethodResult\n\nMethod-specific result from a Laplace fit. Stores the solution, objective value, iteration count, raw solver result, optional notes, and empirical-Bayes mode estimates for each individual.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.LaplaceMAPResult","page":"API","title":"NoLimits.LaplaceMAPResult","text":"LaplaceMAPResult{S, O, I, R, N, B} <: MethodResult\n\nMethod-specific result from a LaplaceMAP fit. Stores the solution, objective value, iteration count, raw solver result, optional notes, and empirical-Bayes mode estimates for each individual.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.MCEMResult","page":"API","title":"NoLimits.MCEMResult","text":"MCEMResult{S, O, I, R, N, B} <: MethodResult\n\nMethod-specific result from a MCEM fit. Stores the solution, objective value, iteration count, raw solver result, optional notes, and final empirical-Bayes mode estimates for each individual.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.SAEMResult","page":"API","title":"NoLimits.SAEMResult","text":"SAEMResult{S, O, I, R, N, B} <: MethodResult\n\nMethod-specific result from a SAEM fit. Stores the solution, objective value, iteration count, raw solver result, optional notes, and final empirical-Bayes mode estimates for each individual.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.MCMCResult","page":"API","title":"NoLimits.MCMCResult","text":"MCMCResult{C, S, A, N, O} <: MethodResult\n\nMethod-specific result from a MCMC fit. Stores the MCMCChains chain, sampler, number of samples, optional notes, and observed data columns.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.MultistartFitResult","page":"API","title":"NoLimits.MultistartFitResult","text":"MultistartFitResult{M, R, RE, S, E, B}\n\nResult from a Multistart run. Stores all successful and failed per-start results together with the index of the best (lowest objective) successful start.\n\nUse the accessor functions to retrieve individual components: get_multistart_results, get_multistart_errors, get_multistart_starts, get_multistart_failed_results, get_multistart_failed_starts, get_multistart_best_index, get_multistart_best.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.get_params","page":"API","title":"NoLimits.get_params","text":"get_params(fe::FixedEffects) -> NamedTuple\n\nReturn the raw parameter block structs as a NamedTuple keyed by parameter name.\n\n\n\n\n\nget_params(res::FitResult; scale=:both) -> FitParameters or ComponentArray\n\nReturn the estimated parameter vector.\n\nKeyword Arguments\n\nscale::Symbol = :both: which scale to return.\n:both — a FitParameters struct with both scales.\n:transformed — the optimisation-scale ComponentArray.\n:untransformed — the natural-scale ComponentArray.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_objective","page":"API","title":"NoLimits.get_objective","text":"get_objective(res::FitResult) -> Real\n\nReturn the final objective value (e.g. negative log-likelihood for MLE).\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_converged","page":"API","title":"NoLimits.get_converged","text":"get_converged(res::FitResult) -> Bool or Nothing\n\nReturn the convergence flag. true indicates successful convergence, false indicates failure, and nothing is returned for methods that do not track convergence (e.g. MCMC).\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_diagnostics","page":"API","title":"NoLimits.get_diagnostics","text":"get_diagnostics(res::FitResult) -> FitDiagnostics\n\nReturn the FitDiagnostics with timing, optimizer, and convergence details.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_summary","page":"API","title":"NoLimits.get_summary","text":"get_summary(res::FitResult) -> FitSummary\n\nReturn the FitSummary containing objective, convergence flag, and parameters.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_method","page":"API","title":"NoLimits.get_method","text":"get_method(res::FitResult) -> FittingMethod\n\nReturn the FittingMethod used to produce this result.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_result","page":"API","title":"NoLimits.get_result","text":"get_result(res::FitResult) -> MethodResult\n\nReturn the method-specific MethodResult subtype (e.g. MLEResult, MCMCResult).\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_data_model","page":"API","title":"NoLimits.get_data_model","text":"get_data_model(res::FitResult) -> DataModel or Nothing\n\nReturn the DataModel stored in the fit result, or nothing if the result was created with store_data_model=false.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_iterations","page":"API","title":"NoLimits.get_iterations","text":"get_iterations(res::FitResult) -> Int\n\nReturn the number of optimiser iterations. Valid for optimisation-based methods (MLE, MAP, Laplace, MCEM, SAEM).\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_raw","page":"API","title":"NoLimits.get_raw","text":"get_raw(res::FitResult)\n\nReturn the raw method-specific result object (e.g. the Optim.jl result for MLE/MAP).\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_notes","page":"API","title":"NoLimits.get_notes","text":"get_notes(res::FitResult) -> String or Nothing\n\nReturn any method-specific string notes attached to the result.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_chain","page":"API","title":"NoLimits.get_chain","text":"get_chain(res::FitResult) -> MCMCChains.Chains\n\nReturn the MCMC chain. Only valid for results produced by MCMC.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_observed","page":"API","title":"NoLimits.get_observed","text":"get_observed(res::FitResult)\n\nReturn the observed data used during MCMC sampling. Only valid for MCMC results.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_sampler","page":"API","title":"NoLimits.get_sampler","text":"get_sampler(res::FitResult)\n\nReturn the sampler object (e.g. NUTS) used for MCMC. Only valid for MCMC results.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_random_effects","page":"API","title":"NoLimits.get_random_effects","text":"get_random_effects(dm::DataModel, res::FitResult; constants_re, flatten,\n                   include_constants) -> NamedTuple\nget_random_effects(res::FitResult; constants_re, flatten, include_constants) -> NamedTuple\n\nReturn empirical Bayes (EB) random-effect estimates as a NamedTuple of DataFrames, one per random effect.\n\nSupported methods: Laplace, LaplaceMAP, FOCEI, FOCEIMAP, MCEM, SAEM.\n\nKeyword Arguments\n\nconstants_re::NamedTuple = NamedTuple(): fix random effects at given values (natural scale).\nflatten::Bool = true: if true, expand vector random effects to individual columns.\ninclude_constants::Bool = true: if true, include constant random effects in the output.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_loglikelihood","page":"API","title":"NoLimits.get_loglikelihood","text":"get_loglikelihood(dm::DataModel, res::FitResult; constants_re, ode_args,\n                  ode_kwargs, serialization) -> Real\nget_loglikelihood(res::FitResult; constants_re, ode_args, ode_kwargs,\n                  serialization) -> Real\n\nCompute the marginal log-likelihood at the estimated parameter values.\n\nFor MLE/MAP results, evaluates the population log-likelihood. For Laplace/FOCEI results, evaluates using the EB modes stored in the result.\n\nKeyword Arguments\n\nconstants_re::NamedTuple = NamedTuple(): random effects fixed at given values.\node_args::Tuple = (): additional positional arguments for the ODE solver.\node_kwargs::NamedTuple = NamedTuple(): additional keyword arguments for the ODE solver.\nserialization = EnsembleSerial(): parallelisation strategy.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_multistart_results","page":"API","title":"NoLimits.get_multistart_results","text":"get_multistart_results(res::MultistartFitResult) -> Vector{FitResult}\n\nReturn the FitResult objects for all successful multistart runs.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_multistart_errors","page":"API","title":"NoLimits.get_multistart_errors","text":"get_multistart_errors(res::MultistartFitResult) -> Vector\n\nReturn the error objects thrown by failed multistart runs.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_multistart_starts","page":"API","title":"NoLimits.get_multistart_starts","text":"get_multistart_starts(res::MultistartFitResult) -> Vector\n\nReturn the starting parameter vectors (on the untransformed scale) for all successful multistart runs.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_multistart_failed_results","page":"API","title":"NoLimits.get_multistart_failed_results","text":"get_multistart_failed_results(res::MultistartFitResult) -> Vector\n\nReturn any partially-computed FitResult objects from failed multistart runs.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_multistart_failed_starts","page":"API","title":"NoLimits.get_multistart_failed_starts","text":"get_multistart_failed_starts(res::MultistartFitResult) -> Vector\n\nReturn the starting parameter vectors for all failed multistart runs.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_multistart_best_index","page":"API","title":"NoLimits.get_multistart_best_index","text":"get_multistart_best_index(res::MultistartFitResult) -> Int\n\nReturn the index (into get_multistart_results) of the run with the lowest objective value.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_multistart_best","page":"API","title":"NoLimits.get_multistart_best","text":"get_multistart_best(res::MultistartFitResult) -> FitResult\n\nReturn the FitResult with the lowest objective value across all successful multistart runs.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.FitResultSummary","page":"API","title":"NoLimits.FitResultSummary","text":"FitResultSummary\n\nStructured summary of a FitResult. Created by summarize(res) or summarize(res, uq). Contains per-parameter rows with estimates and optional standard errors, outcome coverage statistics, and random-effects summaries. Displayed via Base.show.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.UQResultSummary","page":"API","title":"NoLimits.UQResultSummary","text":"UQResultSummary\n\nStructured summary of a UQResult. Created by summarize(uq). Contains per-parameter rows with point estimates, standard errors, and confidence/credible intervals. Displayed via Base.show.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.default_bounds_from_start","page":"API","title":"NoLimits.default_bounds_from_start","text":"default_bounds_from_start(dm::DataModel; margin=1.0) -> (lower, upper)\n\nGenerate symmetric box bounds on the transformed parameter scale centred at the initial parameter values, with half-width margin.\n\nUseful for passing to MLE(lb=lower, ub=upper) when the model-declared bounds are too wide or absent.\n\nKeyword Arguments\n\nmargin::Real = 1.0: half-width of the symmetric box on the transformed scale.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.compute_uq","page":"API","title":"NoLimits.compute_uq","text":"compute_uq(res::FitResult; method, interval, vcov, re_approx, re_approx_method,\n           level, pseudo_inverse, hessian_backend, fd_abs_step, fd_rel_step,\n           fd_max_tries, n_draws, mcmc_warmup, mcmc_draws, constants,\n           constants_re, penalty, ode_args, ode_kwargs, serialization,\n           profile_method, profile_scan_width, profile_scan_tol, profile_loss_tol,\n           profile_local_alg, profile_max_iter, profile_ftol_abs, profile_kwargs,\n           mcmc_method, mcmc_sampler, mcmc_turing_kwargs, mcmc_adtype,\n           mcmc_fit_kwargs, rng) -> UQResult\n\nCompute uncertainty quantification for the fixed-effect parameters of a fitted model.\n\nThree backends are supported:\n\n:wald – Wald intervals derived from the inverse Hessian of the objective.\n:chain – Posterior intervals from posterior draws (MCMC chains or VI posterior samples).\n:profile – Profile-likelihood intervals computed by NLopt.\n\nKeyword Arguments\n\nmethod::Symbol = :auto: UQ backend. :auto selects :chain for MCMC/VI fits and :wald otherwise; can also be :wald, :chain, :profile, or :mcmc_refit.\ninterval::Symbol = :auto: interval type. :auto picks a sensible default per backend. For Wald: :wald or :normal; for chain: :equaltail or :chain; for profile: :profile.\nvcov::Symbol = :hessian: covariance source for Wald UQ (:hessian only).\nre_approx::Symbol = :auto: random-effects approximation for Laplace/FOCEI Hessians.\nre_approx_method: fitting method used for the RE approximation, or nothing.\nlevel::Real = 0.95: nominal coverage level for the intervals.\npseudo_inverse::Bool = false: use the Moore-Penrose pseudo-inverse for singular Hessians (Wald only).\nhessian_backend::Symbol = :auto: Hessian computation backend.\nfd_abs_step, fd_rel_step, fd_max_tries: finite-difference Hessian settings.\nn_draws::Int = 2000: number of draws to generate (for the chain and MCMC backends).\nmcmc_warmup, mcmc_draws: chain-draw settings. For MCMC, warm-up and draw count; for VI, mcmc_draws is the posterior sample count (mcmc_warmup ignored).\nconstants, constants_re, penalty, ode_args, ode_kwargs, serialization: forwarded to objective evaluations (default: inherit from the source fit result).\nprofile_method, profile_scan_width, profile_scan_tol, profile_loss_tol, profile_local_alg, profile_max_iter, profile_ftol_abs, profile_kwargs: NLopt profile-likelihood settings.\nmcmc_method, mcmc_sampler, mcmc_turing_kwargs, mcmc_adtype, mcmc_fit_kwargs: MCMC backend settings.\nrng::AbstractRNG = Random.default_rng(): random-number generator.\n\nReturns\n\nA UQResult with point estimates, intervals, covariance matrices, and draws.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.UQResult","page":"API","title":"NoLimits.UQResult","text":"UQResult\n\nResult from compute_uq. Stores parameter uncertainty quantification on both the natural and transformed scales.\n\nUse the accessor functions to retrieve individual components: get_uq_backend, get_uq_source_method, get_uq_parameter_names, get_uq_estimates, get_uq_intervals, get_uq_vcov, get_uq_draws, get_uq_diagnostics.\n\nFields:\n\nbackend::Symbol: UQ backend used (:wald, :chain, or :profile).\nsource_method::Symbol: estimation method of the source fit result.\nparameter_names::Vector{Symbol}: names on the transformed scale.\nparameter_names_natural::Union{Nothing, Vector{Symbol}}: names on the natural scale, or nothing if identical to parameter_names. For ProbabilityVector and DiscreteTransitionMatrix parameters the Wald backend extends the natural scale with the derived last probability / last-column entries, giving more names than the transformed scale.\nestimates_transformed, estimates_natural: point estimates on each scale.\nintervals_transformed, intervals_natural: UQIntervals or nothing.\nvcov_transformed, vcov_natural: variance-covariance matrices or nothing.\ndraws_transformed, draws_natural: posterior/bootstrap draws (nparams × ndraws) or nothing.\ndiagnostics::NamedTuple: backend-specific diagnostic information.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.UQIntervals","page":"API","title":"NoLimits.UQIntervals","text":"UQIntervals\n\nConfidence or credible intervals at a given coverage level for a set of parameters.\n\nFields:\n\nlevel::Float64: nominal coverage level (e.g. 0.95 for 95% intervals).\nlower::Vector{Float64}: lower bounds in the order given by the parent UQResult.\nupper::Vector{Float64}: upper bounds in the order given by the parent UQResult.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.get_uq_backend","page":"API","title":"NoLimits.get_uq_backend","text":"get_uq_backend(uq::UQResult) -> Symbol\n\nReturn the UQ backend used (:wald, :chain, or :profile).\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_uq_source_method","page":"API","title":"NoLimits.get_uq_source_method","text":"get_uq_source_method(uq::UQResult) -> Symbol\n\nReturn the symbol identifying the estimation method of the source fit result.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_uq_parameter_names","page":"API","title":"NoLimits.get_uq_parameter_names","text":"get_uq_parameter_names(uq::UQResult; scale=:transformed) -> Vector{Symbol}\n\nReturn the names of the free fixed-effect parameters covered by this result.\n\nKeyword Arguments\n\nscale::Symbol = :transformed: :transformed (default) or :natural. For the Wald backend with ProbabilityVector or DiscreteTransitionMatrix parameters, the natural scale includes the derived last probability / last-column entries and may have more names than the transformed scale.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_uq_estimates","page":"API","title":"NoLimits.get_uq_estimates","text":"get_uq_estimates(uq::UQResult; scale=:natural, as_component=true)\n\nReturn point estimates from a UQResult.\n\nKeyword Arguments\n\nscale::Symbol = :natural: :natural for the untransformed scale, :transformed for the optimisation scale.\nas_component::Bool = true: if true, return a ComponentArray keyed by parameter name; otherwise return a plain Vector{Float64}.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_uq_intervals","page":"API","title":"NoLimits.get_uq_intervals","text":"get_uq_intervals(uq::UQResult; scale=:natural, as_component=true)\n-> NamedTuple{(:level, :lower, :upper)} or nothing\n\nReturn confidence/credible intervals from a UQResult, or nothing if not available.\n\nKeyword Arguments\n\nscale::Symbol = :natural: :natural or :transformed.\nas_component::Bool = true: if true, lower and upper are ComponentArrays; otherwise plain Vector{Float64}.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_uq_vcov","page":"API","title":"NoLimits.get_uq_vcov","text":"get_uq_vcov(uq::UQResult; scale=:natural) -> Matrix{Float64} or nothing\n\nReturn the variance-covariance matrix from a UQResult, or nothing if not available.\n\nKeyword Arguments\n\nscale::Symbol = :natural: :natural or :transformed.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_uq_draws","page":"API","title":"NoLimits.get_uq_draws","text":"get_uq_draws(uq::UQResult; scale=:natural) -> Matrix{Float64} or nothing\n\nReturn the posterior or bootstrap draws (nparams × ndraws) from a UQResult, or nothing if not available.\n\nKeyword Arguments\n\nscale::Symbol = :natural: :natural or :transformed.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_uq_diagnostics","page":"API","title":"NoLimits.get_uq_diagnostics","text":"get_uq_diagnostics(uq::UQResult) -> NamedTuple\n\nReturn backend-specific diagnostic information from the UQ computation.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.simulate_data","page":"API","title":"NoLimits.simulate_data","text":"simulate_data(dm::DataModel; rng, replace_missings, serialization) -> DataFrame\n\nSimulate observations from a DataModel using the model's initial parameter values.\n\nRandom effects are drawn from their prior distributions and observation columns are replaced with draws from the model's observation distributions. Non-observation columns are left unchanged.\n\nKeyword Arguments\n\nrng::AbstractRNG = Random.default_rng(): random-number generator.\nreplace_missings::Bool = false: if true, fill missing observation entries with simulated values; otherwise leave them as missing.\nserialization::SciMLBase.EnsembleAlgorithm = EnsembleSerial(): parallelisation strategy (e.g. EnsembleThreads()).\n\nReturns\n\nA copy of dm.df with simulated observation values.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.simulate_data_model","page":"API","title":"NoLimits.simulate_data_model","text":"simulate_data_model(dm::DataModel; rng, replace_missings, serialization) -> DataModel\n\nSimulate observations from a DataModel and return a new DataModel wrapping the simulated data.\n\nCalls simulate_data and constructs a fresh DataModel from the resulting DataFrame, preserving the original model, id columns, and serialization settings.\n\nKeyword Arguments\n\nrng::AbstractRNG = Random.default_rng(): random-number generator.\nreplace_missings::Bool = false: forwarded to simulate_data.\nserialization::SciMLBase.EnsembleAlgorithm: parallelisation strategy; defaults to the strategy stored in dm.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.identifiability_report","page":"API","title":"NoLimits.identifiability_report","text":"identifiability_report(dm::DataModel; method, at, constants, constants_re, penalty,\n                       ode_args, ode_kwargs, serialization, rng, rng_seed, atol, rtol,\n                       hessian_backend, fd_abs_step, fd_rel_step, fd_max_tries)\n                       -> IdentifiabilityReport\n\nidentifiability_report(res::FitResult; method, at, constants, constants_re, penalty,\n                       ode_args, ode_kwargs, serialization, rng, rng_seed, atol, rtol,\n                       hessian_backend, fd_abs_step, fd_rel_step, fd_max_tries)\n                       -> IdentifiabilityReport\n\nCompute a local identifiability report by evaluating the Hessian of the chosen objective at a specified parameter point and checking its rank.\n\nWhen called with a DataModel, the starting values from the model definition are used by default (at=:start). When called with a FitResult, the fitted parameter estimates are used by default (at=:fit).\n\nKeyword Arguments\n\nmethod::Union{Symbol, FittingMethod} = :auto: estimation method whose objective is used. :auto selects MLE for models without random effects and Laplace otherwise. Supported symbols: :mle, :map, :laplace, :laplace_map, :focei, :focei_map.\nat::Union{Symbol, ComponentArray} = :start: evaluation point. :start uses the model initial values, :fit uses the fitted estimates (only for the FitResult method), or a ComponentArray of untransformed parameter values.\nconstants, constants_re, penalty, ode_args, ode_kwargs, serialization, rng: forwarded to the objective; see fit_model for descriptions.\nrng_seed::Union{Nothing, UInt64} = nothing: optional fixed seed for reproducibility.\natol::Real = 1e-8: absolute tolerance for Hessian rank determination.\nrtol::Real = sqrt(eps(Float64)): relative tolerance for Hessian rank determination.\nhessian_backend::Symbol = :auto: Hessian computation backend. :auto tries ForwardDiff then finite differences.\nfd_abs_step::Real = 1e-4: absolute finite-difference step size.\nfd_rel_step::Real = 1e-3: relative finite-difference step size.\nfd_max_tries::Int = 8: maximum step-size retry attempts for finite differences.\n\nReturns\n\nAn IdentifiabilityReport with the Hessian, its spectral decomposition, a local identifiability verdict, and any null directions.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.IdentifiabilityReport","page":"API","title":"NoLimits.IdentifiabilityReport","text":"IdentifiabilityReport{U, T, S}\n\nResult of identifiability_report. Contains the Hessian of the objective at the evaluation point, its spectral decomposition, and a local identifiability verdict.\n\nFields:\n\nmethod::Symbol: estimation method used (e.g. :mle, :laplace).\nobjective::Symbol: objective evaluated (:nll, :map, or :laplace_nll).\nat::Symbol: where the Hessian was evaluated (:start or :fit).\npoint_untransformed: parameter values on the natural scale.\npoint_transformed: parameter values on the transformed scale.\nfree_parameters::Vector{Symbol}: names of the free (non-constant) parameters.\nhessian::Matrix{Float64}: Hessian of the objective on the transformed scale.\nsingular_values, eigenvalues: spectral decomposition of the Hessian.\nrank::Int, nullity::Int: numerical rank and null-space dimension.\ntolerance::Float64: tolerance used for rank determination.\ncondition_number::Float64: ratio of the largest to smallest singular value.\nlocally_identifiable::Bool: true if the Hessian has full rank (nullity = 0).\nnull_directions::Vector{NullDirection}: directions of non-identifiability.\nrandom_effect_information::Vector{RandomEffectInformation}: per-batch RE information.\nsettings::S: NamedTuple of the tolerances and backend settings used.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.NullDirection","page":"API","title":"NoLimits.NullDirection","text":"NullDirection{L}\n\nA direction in the fixed-effect parameter space along which the objective function is (numerically) flat, indicating a potential non-identifiability.\n\nFields:\n\nsingular_value::Float64: the singular value associated with this direction.\nvector::Vector{Float64}: the null-space direction on the transformed scale.\nloadings::L: per-parameter loadings showing which parameters contribute to the direction.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.RandomEffectInformation","page":"API","title":"NoLimits.RandomEffectInformation","text":"RandomEffectInformation\n\nFisher information analysis of the random-effects contribution for a single batch of individuals, as computed within identifiability_report.\n\nFields:\n\nbatch::Int: batch index.\nn_latent::Int: number of latent (random-effect) dimensions in the batch.\nlabels::Vector{String}: human-readable labels for each latent dimension.\nsingular_values::Vector{Float64}: singular values of the RE information matrix.\neigenvalues::Vector{Float64}: eigenvalues of the RE information matrix.\nrank::Int: numerical rank.\nnullity::Int: dimension of the null space.\ntolerance::Float64: tolerance used for rank determination.\ncondition_number::Float64: ratio of the largest to smallest eigenvalue.\npositive_definite::Bool: whether the information matrix is numerically PD.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.PlotStyle","page":"API","title":"NoLimits.PlotStyle","text":"PlotStyle(; color_primary, color_secondary, color_accent, color_dark,\n           color_density, color_reference, font_family, font_size_title,\n           font_size_label, font_size_tick, font_size_legend,\n           font_size_annotation, line_width_primary, line_width_secondary,\n           comparison_default_linestyle, comparison_line_styles, marker_size,\n           marker_size_small, marker_alpha, marker_stroke_width,\n           marker_size_pmf, marker_stroke_width_pmf,\n           base_subplot_width, base_subplot_height)\n\nVisual style configuration for all NoLimits plotting functions.\n\nAll plotting functions accept a style::PlotStyle keyword argument. Construct a PlotStyle() with the defaults and override individual fields as needed.\n\nKeyword Arguments\n\ncolor_primary::String: main series colour (default: \"#0072B2\" — blue).\ncolor_secondary::String: secondary series colour (default: \"#E69F00\" — orange).\ncolor_accent::String: accent colour (default: \"#009E73\" — green).\ncolor_dark::String: dark foreground colour (default: \"#2C3E50\").\ncolor_density::String: colour for density bands (default: \"#E69F00\").\ncolor_reference::String: reference line colour (default: \"#2C3E50\").\nfont_family::String: font family for all text (default: \"Helvetica\").\nfont_size_title, font_size_label, font_size_tick, font_size_legend, font_size_annotation: font sizes in points.\nline_width_primary, line_width_secondary: line widths in pixels.\ncomparison_default_linestyle, comparison_line_styles: line style overrides for plot_fits_comparison.\nmarker_size, marker_size_small, marker_alpha, marker_stroke_width: marker appearance for continuous outcomes.\nmarker_size_pmf, marker_stroke_width_pmf: marker appearance for discrete outcomes.\nbase_subplot_width, base_subplot_height: base pixel dimensions per subplot panel.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.PlotCache","page":"API","title":"NoLimits.PlotCache","text":"PlotCache{S, O, C, P, R, M}\n\nPre-computed plotting cache for efficient repeated rendering of model predictions.\n\nCreate via build_plot_cache and pass to plot_fits via the cache keyword argument to avoid re-solving the ODE or re-evaluating formulas on each call.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.build_plot_cache","page":"API","title":"NoLimits.build_plot_cache","text":"build_plot_cache(res::FitResult; dm, params, constants_re, cache_obs_dists,\n                 ode_args, ode_kwargs, mcmc_draws, mcmc_warmup, rng) -> PlotCache\n\nbuild_plot_cache(res::MultistartFitResult; kwargs...) -> PlotCache\n\nbuild_plot_cache(dm::DataModel; params, constants_re, cache_obs_dists,\n                 ode_args, ode_kwargs, rng) -> PlotCache\n\nPre-compute ODE solutions and (optionally) observation distributions for fast repeated plotting. Pass the returned PlotCache to plot_fits via the cache keyword.\n\nKeyword Arguments\n\ndm::Union{Nothing, DataModel} = nothing: data model (inferred from res by default).\nparams::NamedTuple = NamedTuple(): fixed-effect overrides applied before caching.\nconstants_re::NamedTuple = NamedTuple(): random-effect constants.\ncache_obs_dists::Bool = false: also pre-compute observation distributions.\node_args::Tuple = (), ode_kwargs::NamedTuple = NamedTuple(): forwarded to the ODE solver.\nmcmc_draws::Int = 1000: number of MCMC draws to use for chain-based fits.\nmcmc_warmup::Union{Nothing, Int} = nothing: warm-up count override for MCMC.\nrng::AbstractRNG = Random.default_rng(): random-number generator.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.plot_data","page":"API","title":"NoLimits.plot_data","text":"plot_data(res::FitResult; dm, x_axis_feature, individuals_idx, shared_x_axis,\n          shared_y_axis, ncols, style, kwargs_subplot, kwargs_layout,\n          save_path, plot_path, marginal_layout) -> Plots.Plot | Vector{Plots.Plot}\n\nplot_data(dm::DataModel; x_axis_feature, individuals_idx, shared_x_axis,\n          shared_y_axis, ncols, style, kwargs_subplot, kwargs_layout,\n          save_path, plot_path, marginal_layout) -> Plots.Plot | Vector{Plots.Plot}\n\nPlot raw observed data for each individual as a multi-panel figure.\n\nKeyword Arguments\n\ndm::Union{Nothing, DataModel} = nothing: data model (inferred from res by default).\nx_axis_feature::Union{Symbol, Nothing} = nothing: covariate to use as the x-axis; defaults to the time column.\nindividuals_idx: indices or IDs of individuals to include, or nothing for all.\nshared_x_axis::Bool = true: share the x-axis range across panels.\nshared_y_axis::Bool = true: share the y-axis range across panels.\nncols::Int = 3: number of subplot columns.\nmarginal_layout::Symbol = :single: :single keeps one figure with every marginal overlaid per individual; :vector returns a figure per marginal (only valid for vector-valued observables and requires save_path/plot_path to be nothing).\nstyle::PlotStyle = PlotStyle(): visual style configuration.\nmarginal_layout::Symbol = :single: :single produces one figure with subplots per individual showing every marginal; :vector returns a figure per marginal (only valid when the observable is vector-valued and requires save_path and plot_path to be nothing).\nkwargs_subplot, kwargs_layout: extra keyword arguments for subplots and layout.\nsave_path::Union{Nothing, String} = nothing: file path to save the plot (ignored for :vector mode).\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.plot_fits","page":"API","title":"NoLimits.plot_fits","text":"plot_fits(res::FitResult; dm, plot_density, plot_func, plot_data_points, observable,\n          individuals_idx, x_axis_feature, shared_x_axis, shared_y_axis, ncols,\n          style, kwargs_subplot, kwargs_layout, save_path, cache, params,\n          constants_re, cache_obs_dists, plot_mcmc_quantiles, mcmc_quantiles,\n          mcmc_quantiles_alpha, mcmc_draws, mcmc_warmup, rng) -> Plots.Plot\n\nplot_fits(dm::DataModel; params, constants_re, observable, individuals_idx,\n          x_axis_feature, shared_x_axis, shared_y_axis, ncols, plot_data_points,\n          style, kwargs_subplot, kwargs_layout, save_path, cache) -> Plots.Plot\n\nPlot model predictions against observed data for each individual as a multi-panel figure.\n\nKeyword Arguments\n\ndm::Union{Nothing, DataModel} = nothing: data model (inferred from res by default).\nplot_density::Bool = false: overlay the predictive distribution density.\nplot_func = mean: function applied to the predictive distribution to obtain the prediction line (e.g. mean, median).\nplot_data_points::Bool = true: overlay the observed data points.\nobservable: name of the outcome variable to plot, or nothing to use the first.\nindividuals_idx: indices or IDs of individuals to include, or nothing for all.\nx_axis_feature::Union{Nothing, Symbol} = nothing: covariate for the x-axis.\nshared_x_axis::Bool = true, shared_y_axis::Bool = true: share axis ranges.\nncols::Int = 3: number of subplot columns.\nstyle::PlotStyle = PlotStyle(): visual style configuration.\nkwargs_subplot, kwargs_layout: extra keyword arguments for subplots and layout.\nsave_path::Union{Nothing, String} = nothing: file path to save the plot.\ncache::Union{Nothing, PlotCache} = nothing: pre-computed plot cache.\nparams::NamedTuple = NamedTuple(): fixed-effect overrides.\nconstants_re::NamedTuple = NamedTuple(): random-effect constants.\ncache_obs_dists::Bool = false: pre-compute observation distributions when building cache.\nplot_mcmc_quantiles::Bool = false: plot posterior predictive quantile bands (MCMC).\nmcmc_quantiles::Vector = [5, 95]: quantile percentages for posterior bands.\nmcmc_quantiles_alpha::Float64 = 0.8: opacity of the quantile band.\nmcmc_draws::Int = 1000: number of MCMC draws for posterior predictive plotting.\nmcmc_warmup::Union{Nothing, Int} = nothing: warm-up count override for MCMC.\nrng::AbstractRNG = Random.default_rng(): random-number generator.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.plot_fits_comparison","page":"API","title":"NoLimits.plot_fits_comparison","text":"plot_fits_comparison(res::Union{FitResult, MultistartFitResult}; kwargs...)\n                     -> Plots.Plot\n\nplot_fits_comparison(results::AbstractVector; kwargs...) -> Plots.Plot\n\nplot_fits_comparison(results::NamedTuple; kwargs...) -> Plots.Plot\n\nplot_fits_comparison(results::AbstractDict; kwargs...) -> Plots.Plot\n\nPlot predictions from one or more fitted models side-by-side for visual comparison.\n\nWhen called with a single FitResult or MultistartFitResult, behaves like plot_fits. When called with a collection, overlays predictions from each model on the same panel, labelled by vector index, NamedTuple key, or Dict key.\n\nAll keyword arguments are forwarded to the underlying plot_fits implementation.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.plot_vpc","page":"API","title":"NoLimits.plot_vpc","text":"plot_vpc(res::FitResult; dm, n_simulations, n_sim, percentiles, show_obs_points,\n         show_obs_percentiles, n_bins, seed, observables, x_axis_feature, ncols,\n         kwargs_plot, save_path, obs_percentiles_mode, bandwidth,\n         obs_percentiles_method, constants_re, mcmc_draws, mcmc_warmup, style)\n         -> Plots.Plot\n\nVisual Predictive Check (VPC): compares observed percentile bands to simulated predictive percentile bands stratified by x-axis bins.\n\nKeyword Arguments\n\ndm::Union{Nothing, DataModel} = nothing: data model (inferred from res by default).\nn_simulations::Int = 100: number of simulated datasets for the VPC envelopes.\npercentiles::Vector = [5, 50, 95]: percentiles to display (in [0, 100]).\nshow_obs_points::Bool = true: overlay observed data points.\nshow_obs_percentiles::Bool = true: overlay observed percentile lines.\nn_bins::Union{Nothing, Int} = nothing: number of x-axis bins; nothing for auto.\nseed::Int = 12345: random seed for reproducible simulations.\nobservables: outcome name(s) to plot, or nothing for all.\nx_axis_feature::Union{Nothing, Symbol} = nothing: covariate for the x-axis; defaults to time.\nncols::Int = 3: number of subplot columns.\nkwargs_plot: extra keyword arguments forwarded to the plot.\nsave_path::Union{Nothing, String} = nothing: file path to save the plot.\nobs_percentiles_mode::Symbol = :pooled: :pooled or :individual percentile computation.\nbandwidth::Union{Nothing, Float64} = nothing: smoothing bandwidth for percentile curves, or nothing for no smoothing.\nobs_percentiles_method::Symbol = :quantile: :quantile or :weighted.\nconstants_re::NamedTuple = NamedTuple(): random-effect constants.\nmcmc_draws::Int = 1000, mcmc_warmup: MCMC draw settings.\nstyle::PlotStyle = PlotStyle(): visual style configuration.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.get_residuals","page":"API","title":"NoLimits.get_residuals","text":"get_residuals(res::FitResult; dm, cache, observables, individuals_idx, obs_rows,\n              x_axis_feature, params, constants_re, cache_obs_dists, residuals,\n              fitted_stat, randomize_discrete, cdf_fallback_mc, ode_args,\n              ode_kwargs, mcmc_draws, mcmc_warmup, mcmc_quantiles, rng,\n              return_draw_level) -> DataFrame\n\nget_residuals(dm::DataModel; params, constants_re, observables, individuals_idx,\n              obs_rows, x_axis_feature, cache, cache_obs_dists, residuals,\n              fitted_stat, randomize_discrete, cdf_fallback_mc, ode_args,\n              ode_kwargs, rng) -> DataFrame\n\nCompute residuals for each observation and return a DataFrame.\n\nKeyword Arguments\n\ndm::Union{Nothing, DataModel} = nothing: data model (inferred from res by default).\ncache::Union{Nothing, PlotCache} = nothing: pre-computed plot cache.\nobservables: outcome name(s) to include, or nothing for all.\nindividuals_idx: individuals to include, or nothing for all.\nobs_rows: specific observation row indices to include, or nothing for all.\nx_axis_feature::Union{Nothing, Symbol} = nothing: covariate for the x column.\nparams::NamedTuple = NamedTuple(): fixed-effect overrides.\nconstants_re::NamedTuple = NamedTuple(): random-effect constants.\ncache_obs_dists::Bool = true: cache observation distributions.\nresiduals: residual metrics to compute. Allowed: :pit, :quantile, :raw, :pearson, :logscore.\nfitted_stat = mean: statistic applied to the predictive distribution for raw residuals.\nrandomize_discrete::Bool = true: randomise PIT values for discrete outcomes.\ncdf_fallback_mc::Int = 0: MC samples for CDF approximation with non-analytic distributions.\node_args::Tuple = (), ode_kwargs::NamedTuple = NamedTuple(): forwarded to ODE solver.\nmcmc_draws::Int = 1000, mcmc_warmup: MCMC draw settings.\nmcmc_quantiles::Vector = [5, 95]: percentiles for MCMC residual uncertainty bands.\nrng::AbstractRNG = Random.default_rng(): random-number generator.\nreturn_draw_level::Bool = false: if true, return draw-level residuals for MCMC.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.plot_residuals","page":"API","title":"NoLimits.plot_residuals","text":"plot_residuals(res::FitResult; dm, cache, residual, observables, individuals_idx,\n               obs_rows, x_axis_feature, shared_x_axis, shared_y_axis, ncols,\n               style, params, constants_re, cache_obs_dists, fitted_stat,\n               randomize_discrete, cdf_fallback_mc, ode_args, ode_kwargs,\n               mcmc_draws, mcmc_warmup, mcmc_quantiles, rng, save_path,\n               kwargs_subplot, kwargs_layout) -> Plots.Plot\n\nplot_residuals(dm::DataModel; ...) -> Plots.Plot\n\nPlot residuals versus time (or another x-axis feature) for each individual.\n\nKeyword Arguments\n\nresidual::Symbol = :quantile: residual metric to plot. One of :pit, :quantile, :raw, :pearson, :logscore.\nAll other arguments are forwarded to get_residuals; see that function for descriptions.\nshared_x_axis::Bool = true, shared_y_axis::Bool = true: share axis ranges.\nncols::Int = 3: number of subplot columns.\nstyle::PlotStyle = PlotStyle(): visual style configuration.\nkwargs_subplot, kwargs_layout: extra keyword arguments for subplots and layout.\nsave_path::Union{Nothing, String} = nothing: file path to save the plot.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.plot_residual_distribution","page":"API","title":"NoLimits.plot_residual_distribution","text":"plot_residual_distribution(res::FitResult; dm, cache, residual, observables,\n                           individuals_idx, obs_rows, x_axis_feature,\n                           shared_x_axis, shared_y_axis, ncols, style, bins,\n                           params, constants_re, cache_obs_dists, fitted_stat,\n                           randomize_discrete, cdf_fallback_mc, ode_args,\n                           ode_kwargs, mcmc_draws, mcmc_warmup, mcmc_quantiles,\n                           rng, save_path, kwargs_subplot, kwargs_layout)\n                           -> Plots.Plot\n\nplot_residual_distribution(dm::DataModel; ...) -> Plots.Plot\n\nPlot the marginal distribution of residuals as histograms with optional density overlays.\n\nKeyword Arguments\n\nresidual::Symbol = :quantile: residual metric (:pit, :quantile, :raw, :pearson, :logscore).\nbins::Int = 20: number of histogram bins.\nAll other arguments are forwarded to get_residuals.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.plot_residual_qq","page":"API","title":"NoLimits.plot_residual_qq","text":"plot_residual_qq(res::FitResult; dm, cache, residual, observables, individuals_idx,\n                 obs_rows, x_axis_feature, ncols, style, params, constants_re,\n                 cache_obs_dists, fitted_stat, randomize_discrete, cdf_fallback_mc,\n                 ode_args, ode_kwargs, mcmc_draws, mcmc_warmup, mcmc_quantiles,\n                 rng, save_path, kwargs_subplot, kwargs_layout) -> Plots.Plot\n\nplot_residual_qq(dm::DataModel; ...) -> Plots.Plot\n\nQuantile-quantile plot of residuals against the theoretical distribution (Uniform for :pit, Normal for other metrics).\n\nKeyword Arguments\n\nresidual::Symbol = :quantile: residual metric (:pit, :quantile, :raw, :pearson, :logscore).\nAll other arguments are forwarded to get_residuals.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.plot_residual_pit","page":"API","title":"NoLimits.plot_residual_pit","text":"plot_residual_pit(res::FitResult; dm, cache, observables, individuals_idx, obs_rows,\n                  x_axis_feature, show_hist, show_kde, show_qq, ncols, style,\n                  kde_bandwidth, params, constants_re, cache_obs_dists,\n                  randomize_discrete, cdf_fallback_mc, ode_args, ode_kwargs,\n                  mcmc_draws, mcmc_warmup, rng, save_path, kwargs_subplot,\n                  kwargs_layout) -> Plots.Plot\n\nplot_residual_pit(dm::DataModel; ...) -> Plots.Plot\n\nPlot the probability integral transform (PIT) values as histograms and/or KDE curves. Uniform PIT values indicate a well-calibrated model.\n\nKeyword Arguments\n\nshow_hist::Bool = true: show a histogram of PIT values.\nshow_kde::Bool = false: overlay a kernel density estimate.\nshow_qq::Bool = false: add a uniform QQ reference line.\nkde_bandwidth::Union{Nothing, Float64} = nothing: KDE bandwidth, or nothing for auto.\nAll other arguments are forwarded to get_residuals.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.plot_residual_acf","page":"API","title":"NoLimits.plot_residual_acf","text":"plot_residual_acf(res::FitResult; dm, cache, residual, observables, individuals_idx,\n                  obs_rows, x_axis_feature, max_lag, ncols, style, params,\n                  constants_re, cache_obs_dists, fitted_stat, randomize_discrete,\n                  cdf_fallback_mc, ode_args, ode_kwargs, mcmc_draws, mcmc_warmup,\n                  mcmc_quantiles, rng, save_path, kwargs_subplot, kwargs_layout)\n                  -> Plots.Plot\n\nplot_residual_acf(dm::DataModel; ...) -> Plots.Plot\n\nPlot the autocorrelation function (ACF) of residuals across time lags for each outcome.\n\nKeyword Arguments\n\nresidual::Symbol = :quantile: residual metric (:pit, :quantile, :raw, :pearson, :logscore).\nmax_lag::Int = 5: maximum lag to compute and display.\nAll other arguments are forwarded to get_residuals.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.plot_random_effects_pdf","page":"API","title":"NoLimits.plot_random_effects_pdf","text":"plot_random_effects_pdf(res::FitResult; dm, re_names, levels, individuals_idx,\n                        shared_x_axis, shared_y_axis, ncols, style, mcmc_draws,\n                        mcmc_warmup, mcmc_quantiles, mcmc_quantiles_alpha,\n                        flow_samples, flow_plot, flow_bins, flow_bandwidth, rng,\n                        save_path, kwargs_subplot, kwargs_layout) -> Plots.Plot\n\nPlot the fitted marginal PDF of each random effect alongside the posterior EBE histogram, showing how well the parametric distribution fits the estimated random-effect values.\n\nKeyword Arguments\n\ndm::Union{Nothing, DataModel} = nothing: data model (inferred from res by default).\nre_names: random-effect name(s) to include, or nothing for all.\nlevels, individuals_idx: grouping level or individual filters.\nshared_x_axis::Bool = true, shared_y_axis::Bool = true: share axes.\nncols::Int = 3: number of subplot columns.\nstyle::PlotStyle = PlotStyle(): visual style configuration.\nmcmc_draws, mcmc_warmup, mcmc_quantiles, mcmc_quantiles_alpha: MCMC settings.\nflow_samples::Int = 500: number of samples for normalizing-flow distributions.\nflow_plot::Symbol = :kde: :kde or :hist for flow-based distributions.\nflow_bins::Int = 20, flow_bandwidth: histogram bins / KDE bandwidth for flows.\nrng::AbstractRNG = Random.default_rng(): random-number generator.\nsave_path::Union{Nothing, String} = nothing: file path to save the plot.\nkwargs_subplot, kwargs_layout: extra keyword arguments for subplots and layout.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.plot_random_effects_scatter","page":"API","title":"NoLimits.plot_random_effects_scatter","text":"plot_random_effects_scatter(res::FitResult; dm, re_names, levels, individuals_idx,\n                            x_covariate, mcmc_draws, ncols, style, save_path,\n                            kwargs_subplot, kwargs_layout) -> Plots.Plot\n\nScatter plot of empirical-Bayes estimates for each random effect against a constant covariate or group level index, useful for detecting covariate relationships.\n\nKeyword Arguments\n\ndm::Union{Nothing, DataModel} = nothing: data model (inferred from res by default).\nre_names: random-effect name(s) to include, or nothing for all.\nlevels, individuals_idx: grouping level or individual filters.\nx_covariate::Union{Nothing, Symbol} = nothing: constant covariate for the x-axis; defaults to the group level index.\nmcmc_draws::Int = 1000: MCMC draws for posterior mean EBE.\nncols::Int = 3: number of subplot columns.\nstyle::PlotStyle = PlotStyle(): visual style configuration.\nsave_path::Union{Nothing, String} = nothing: file path to save the plot.\nkwargs_subplot, kwargs_layout: extra keyword arguments.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.plot_random_effect_pairplot","page":"API","title":"NoLimits.plot_random_effect_pairplot","text":"plot_random_effect_pairplot(res::FitResult; dm, re_names, levels, individuals_idx,\n                            ncols, style, kde_bandwidth, mcmc_draws, rng, save_path,\n                            kwargs_subplot, kwargs_layout) -> Plots.Plot\n\nPairplot (scatter matrix) of empirical-Bayes estimates across all pairs of random effects, useful for visualising correlations and joint structure.\n\nKeyword Arguments\n\ndm::Union{Nothing, DataModel} = nothing: data model (inferred from res by default).\nre_names: random-effect names to include, or nothing for all.\nlevels, individuals_idx: grouping level or individual filters.\nncols::Int = 3: number of subplot columns.\nstyle::PlotStyle = PlotStyle(): visual style configuration.\nkde_bandwidth::Union{Nothing, Float64} = nothing: KDE bandwidth for diagonal panels.\nmcmc_draws::Int = 1000: MCMC draws for posterior mean EBE.\nrng::AbstractRNG = Random.default_rng(): random-number generator.\nsave_path::Union{Nothing, String} = nothing: file path to save the plot.\nkwargs_subplot, kwargs_layout: extra keyword arguments.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.plot_random_effect_distributions","page":"API","title":"NoLimits.plot_random_effect_distributions","text":"plot_random_effect_distributions(res::FitResult; dm, re_names, levels,\n                                 individuals_idx, shared_x_axis, shared_y_axis,\n                                 ncols, style, mcmc_draws, mcmc_warmup,\n                                 mcmc_quantiles, mcmc_quantiles_alpha, flow_samples,\n                                 flow_plot, flow_bins, flow_bandwidth, rng,\n                                 save_path, kwargs_subplot, kwargs_layout)\n                                 -> Plots.Plot\n\nPlot empirical and fitted distributions for each random effect side-by-side, combining the EBE histogram with the parametric prior PDF.\n\nKeyword Arguments\n\nAll arguments are identical to plot_random_effects_pdf.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.plot_random_effect_pit","page":"API","title":"NoLimits.plot_random_effect_pit","text":"plot_random_effect_pit(res::FitResult; dm, re_names, levels, individuals_idx,\n                       show_hist, show_kde, show_qq, shared_x_axis, shared_y_axis,\n                       ncols, style, kde_bandwidth, mcmc_draws, mcmc_warmup,\n                       mcmc_quantiles, mcmc_quantiles_alpha, flow_samples, rng,\n                       save_path, kwargs_subplot, kwargs_layout) -> Plots.Plot\n\nPlot the probability integral transform (PIT) of empirical-Bayes estimates under their fitted prior distributions, providing a calibration check for the random-effects model.\n\nKeyword Arguments\n\ndm::Union{Nothing, DataModel} = nothing: data model (inferred from res by default).\nre_names: random-effect names to include, or nothing for all.\nlevels, individuals_idx: grouping level or individual filters.\nshow_hist::Bool = true: show a PIT histogram.\nshow_kde::Bool = false: overlay a KDE curve.\nshow_qq::Bool = true: add a Uniform QQ reference line.\nshared_x_axis::Bool = true, shared_y_axis::Bool = true: share axes.\nncols::Int = 3: number of subplot columns.\nstyle::PlotStyle = PlotStyle(): visual style configuration.\nkde_bandwidth::Union{Nothing, Float64} = nothing: KDE bandwidth.\nmcmc_draws, mcmc_warmup, mcmc_quantiles, mcmc_quantiles_alpha: MCMC settings.\nflow_samples::Int = 500: samples for normalizing-flow distributions.\nrng::AbstractRNG = Random.default_rng(): random-number generator.\nsave_path::Union{Nothing, String} = nothing: file path to save the plot.\nkwargs_subplot, kwargs_layout: extra keyword arguments.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.plot_random_effect_standardized","page":"API","title":"NoLimits.plot_random_effect_standardized","text":"plot_random_effect_standardized(res::FitResult; dm, re_names, levels,\n                                individuals_idx, show_hist, show_kde, kde_bandwidth,\n                                mcmc_draws, flow_samples, ncols, style, save_path,\n                                kwargs_subplot, kwargs_layout) -> Plots.Plot\n\nPlot standardised (z-score) empirical-Bayes estimates for each random effect as a histogram and/or KDE, with a standard Normal reference. Values far from zero indicate outliers or misspecification.\n\nKeyword Arguments\n\ndm::Union{Nothing, DataModel} = nothing: data model (inferred from res by default).\nre_names: random-effect names to include, or nothing for all.\nlevels, individuals_idx: grouping level or individual filters.\nshow_hist::Bool = true: show a histogram.\nshow_kde::Bool = false: overlay a KDE curve.\nkde_bandwidth::Union{Nothing, Float64} = nothing: KDE bandwidth.\nmcmc_draws::Int = 1000: MCMC draws for posterior mean EBE.\nflow_samples::Int = 500: samples for normalizing-flow distributions.\nncols::Int = 3: number of subplot columns.\nstyle::PlotStyle = PlotStyle(): visual style configuration.\nsave_path::Union{Nothing, String} = nothing: file path to save the plot.\nkwargs_subplot, kwargs_layout: extra keyword arguments.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.plot_random_effect_standardized_scatter","page":"API","title":"NoLimits.plot_random_effect_standardized_scatter","text":"plot_random_effect_standardized_scatter(res::FitResult; dm, re_names, levels,\n                                        individuals_idx, x_covariate, mcmc_draws,\n                                        flow_samples, ncols, style, save_path,\n                                        kwargs_subplot, kwargs_layout) -> Plots.Plot\n\nScatter plot of standardised (z-score) empirical-Bayes estimates against a covariate or group level index. Useful for detecting systematic patterns in the residual structure of the random-effects model.\n\nKeyword Arguments\n\ndm::Union{Nothing, DataModel} = nothing: data model (inferred from res by default).\nre_names: random-effect names to include, or nothing for all.\nlevels, individuals_idx: grouping level or individual filters.\nx_covariate::Union{Nothing, Symbol} = nothing: constant covariate for the x-axis.\nmcmc_draws::Int = 1000: MCMC draws for posterior mean EBE.\nflow_samples::Int = 500: samples for normalizing-flow distributions.\nncols::Int = 3: number of subplot columns.\nstyle::PlotStyle = PlotStyle(): visual style configuration.\nsave_path::Union{Nothing, String} = nothing: file path to save the plot.\nkwargs_subplot, kwargs_layout: extra keyword arguments.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.plot_observation_distributions","page":"API","title":"NoLimits.plot_observation_distributions","text":"plot_observation_distributions(res::FitResult; dm, individuals_idx, obs_rows,\n                               observables, x_axis_feature, shared_x_axis,\n                               shared_y_axis, ncols, style, cache, cache_obs_dists,\n                               constants_re, mcmc_quantiles, mcmc_quantiles_alpha,\n                               mcmc_draws, mcmc_warmup, rng, save_path,\n                               kwargs_subplot, kwargs_layout) -> Plots.Plot\n\nPlot the predictive observation distributions at each time point as density or PMF curves overlaid on the observed data, providing a detailed look at model calibration.\n\nKeyword Arguments\n\ndm::Union{Nothing, DataModel} = nothing: data model (inferred from res by default).\nindividuals_idx: individuals to include (default: first individual only).\nobs_rows: specific observation row indices, or nothing for all.\nobservables: outcome name(s) to include, or nothing for first.\nx_axis_feature::Union{Nothing, Symbol} = nothing: covariate for the x-axis.\nshared_x_axis::Bool = true, shared_y_axis::Bool = true: share axes.\nncols::Int = 3: number of subplot columns.\nstyle::PlotStyle = PlotStyle(): visual style configuration.\ncache::Union{Nothing, PlotCache} = nothing: pre-computed plot cache.\ncache_obs_dists::Bool = false: pre-compute observation distributions when building cache.\nconstants_re::NamedTuple = NamedTuple(): random-effect constants.\nmcmc_quantiles, mcmc_quantiles_alpha, mcmc_draws, mcmc_warmup: MCMC settings.\nrng::AbstractRNG = Random.default_rng(): random-number generator.\nsave_path::Union{Nothing, String} = nothing: file path to save the plot.\nkwargs_subplot, kwargs_layout: extra keyword arguments.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.plot_uq_distributions","page":"API","title":"NoLimits.plot_uq_distributions","text":"plot_uq_distributions(uq::UQResult;\n                      scale=:natural,\n                      parameters=nothing,\n                      interval_alpha=0.22,\n                      histogram_alpha=0.45,\n                      show_estimate=true,\n                      show_interval=true,\n                      show_legend=false,\n                      bins=:auto,\n                      plot_type=:density,\n                      kde_bandwidth=nothing,\n                      ncols=3,\n                      style=PlotStyle(),\n                      kwargs_subplot=NamedTuple(),\n                      kwargs_layout=NamedTuple(),\n                      save_path=nothing)\n\nPlot marginal parameter distributions from a UQResult.\n\nFor :chain and :mcmc_refit backends, draws are shown as a KDE or histogram. For the :wald backend, analytic Gaussian approximations are plotted where the parameter is on a transformed scale; otherwise KDE is used. Point estimates and credible/confidence intervals are overlaid as vertical lines and shaded regions.\n\nArguments\n\nuq::UQResult - Uncertainty quantification result from compute_uq.\n\nKeyword Arguments\n\nscale::Symbol - Parameter scale for display: :natural (default) or :transformed.\nparameters - Symbol, vector of Symbols, or nothing (all parameters, default).\ninterval_alpha::Float64 - Opacity of the shaded interval region (default: 0.22).\nhistogram_alpha::Float64 - Opacity of histogram bars (default: 0.45).\nshow_estimate::Bool - Show point estimate as a vertical line (default: true).\nshow_interval::Bool - Shade the credible/confidence interval (default: true).\nshow_legend::Bool - Show plot legend (default: false).\nbins - Histogram bin count or :auto (default).\nplot_type::Symbol - :density (KDE, default) or :histogram.\nkde_bandwidth::Union{Nothing, Float64} - KDE bandwidth; nothing uses automatic selection (default).\nncols::Int - Number of subplot columns (default: 3).\nstyle::PlotStyle - Visual style configuration.\nkwargs_subplot - Extra keyword arguments forwarded to each subplot.\nkwargs_layout - Extra keyword arguments forwarded to the layout call.\nsave_path::Union{Nothing, String} - File path to save the plot, or nothing.\n\nReturns\n\nA Plots.jl plot object showing one panel per selected parameter.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.plot_multistart_waterfall","page":"API","title":"NoLimits.plot_multistart_waterfall","text":"plot_multistart_waterfall(res::MultistartFitResult; style, kwargs_subplot, save_path)\n-> Plots.Plot\n\nPlot the objective values of all successful multistart runs in ascending order (waterfall plot), highlighting the best run.\n\nKeyword Arguments\n\nstyle::PlotStyle = PlotStyle(): visual style configuration.\nkwargs_subplot: additional keyword arguments forwarded to the subplot.\nsave_path::Union{Nothing, String} = nothing: file path to save the plot, or nothing.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.plot_multistart_fixed_effect_variability","page":"API","title":"NoLimits.plot_multistart_fixed_effect_variability","text":"plot_multistart_fixed_effect_variability(res::MultistartFitResult; dm, k_best, mode,\n                                         quantiles, scale, include_parameters,\n                                         exclude_parameters, style, kwargs_subplot,\n                                         save_path) -> Plots.Plot\n\nPlot the variation of fixed-effect estimates across the k_best multistart runs with the lowest objective values.\n\nKeyword Arguments\n\ndm::Union{Nothing, DataModel} = nothing: data model (inferred from res by default).\nk_best::Int = 20: number of best runs to include.\nmode::Symbol = :points: :points to show individual estimates; :quantiles to show quantile bands.\nquantiles::AbstractVector = [0.1, 0.5, 0.9]: quantile levels for :quantiles mode.\nscale::Symbol = :untransformed: :untransformed or :transformed.\ninclude_parameters, exclude_parameters: parameter name filters.\nstyle::PlotStyle = PlotStyle(): visual style configuration.\nkwargs_subplot: additional keyword arguments forwarded to each subplot.\nsave_path::Union{Nothing, String} = nothing: file path to save the plot.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.DiscreteTimeDiscreteStatesHMM","page":"API","title":"NoLimits.DiscreteTimeDiscreteStatesHMM","text":"DiscreteTimeDiscreteStatesHMM(transition_matrix, emission_dists, initial_dist)\n<: Distribution{Univariate, Continuous}\n\nA discrete-time Hidden Markov Model (HMM) with a finite number of hidden states and continuous or discrete emission distributions.\n\nImplements the Distributions.jl interface (pdf, logpdf, rand, mean, var). Used as an observation distribution in @formulas blocks to model outcomes with latent state dynamics.\n\nArguments\n\ntransition_matrix::AbstractMatrix{<:Real}: row-stochastic transition matrix of shape (n_states, n_states). Entry [i, j] is P(State_t = j | State_{t-1} = i).\nemission_dists::Tuple: tuple of n_states emission distributions, one per state.\ninitial_dist::Distributions.Categorical: prior over hidden states at the current time step. Propagated one step via transition_matrix before computing the emission likelihood.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.probabilities_hidden_states","page":"API","title":"NoLimits.probabilities_hidden_states","text":"probabilities_hidden_states(hmm::DiscreteTimeDiscreteStatesHMM) -> Vector{Float64}\nprobabilities_hidden_states(hmm::ContinuousTimeDiscreteStatesHMM) -> Vector{Float64}\n\nCompute the marginal prior probabilities of the hidden states at the current observation time, propagated from hmm.initial_dist through the transition dynamics.\n\nReturns a normalised probability vector of length n_states.\n\n\n\n\n\nprobabilities_hidden_states(hmm::MVDiscreteTimeDiscreteStatesHMM) -> Vector\n\nMarginal prior probabilities of the hidden states at the current observation time, propagated one step from hmm.initial_dist via hmm.transition_matrix.\n\n\n\n\n\nprobabilities_hidden_states(hmm::MVContinuousTimeDiscreteStatesHMM) -> Vector\n\nMarginal prior probabilities of the hidden states at the current observation time, propagated from hmm.initial_dist via exp(Q · Δt).\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.posterior_hidden_states","page":"API","title":"NoLimits.posterior_hidden_states","text":"posterior_hidden_states(hmm::ContinuousTimeDiscreteStatesHMM, y::Real)\n\nCompute the posterior probability distribution of hidden states given observation y.\n\nReturns a vector of probabilities p where p[s] is P(State = s | Y = y).\n\nUses Bayes' rule: P(S | Y) ∝ P(Y | S) * P(S)\n\n\n\n\n\nposterior_hidden_states(hmm::DiscreteTimeDiscreteStatesHMM, y::Real)\n\nCompute posterior probabilities of hidden states given observation y.\n\n\n\n\n\nposterior_hidden_states(hmm::MVDiscreteTimeDiscreteStatesHMM, y::AbstractVector)\n\nPosterior probabilities of hidden states given the length-M observation vector y (which may contain missing entries). Uses all non-missing outcomes jointly.\n\n\n\n\n\nposterior_hidden_states(hmm::MVContinuousTimeDiscreteStatesHMM, y::AbstractVector)\n\nPosterior probabilities of hidden states given the length-M observation vector y (which may contain missing entries). Uses all non-missing outcomes jointly.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.ContinuousTimeDiscreteStatesHMM","page":"API","title":"NoLimits.ContinuousTimeDiscreteStatesHMM","text":"ContinuousTimeDiscreteStatesHMM(transition_matrix, emission_dists, initial_dist, Δt)\n<: Distribution{Univariate, Continuous}\n\nA continuous-time Hidden Markov Model (HMM) with a finite number of hidden states and continuous or discrete emission distributions.\n\nState propagation is performed via the matrix exponential exp(Q·Δt) where Q is the rate matrix (transition_matrix). Implements the Distributions.jl interface.\n\nArguments\n\ntransition_matrix::AbstractMatrix{<:Real}: rate matrix (generator) of shape (n_states, n_states). Off-diagonal entries must be non-negative; each row must sum to zero.\nemission_dists::Tuple: tuple of n_states emission distributions.\ninitial_dist::Distributions.Categorical: prior over hidden states at the previous observation time.\nΔt::Real: time elapsed since the previous observation.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.AbstractNormalizingFlow","page":"API","title":"NoLimits.AbstractNormalizingFlow","text":"AbstractNormalizingFlow <: Distributions.ContinuousMultivariateDistribution\n\nAbstract supertype for all normalizing flow distributions in NoLimits.jl. Subtypes include NormalizingPlanarFlow.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.NormalizingPlanarFlow","page":"API","title":"NoLimits.NormalizingPlanarFlow","text":"NormalizingPlanarFlow{D, R} <: AbstractNormalizingFlow\n\nA normalizing planar flow distribution for flexible random effects.\n\nTransforms a base distribution (typically multivariate normal) through a series of planar layers to create a more expressive distribution. Used in @randomEffects blocks to allow random effects to have non-Gaussian distributions.\n\nFields\n\nbase::D - Transformed distribution (base distribution + flow transformations)\nrebuild::R - Optimisers.Restructure function to reconstruct the bijector from flat parameters\n\nConstructors\n\n# Direct construction with dimensions\nNormalizingPlanarFlow(n_input::Int, n_layers::Int; init=glorot_init)\n\n# Construction from parameters (used internally by model macro)\nNormalizingPlanarFlow(θ::Vector, rebuild::Restructure, q0::Distribution)\n\nArguments\n\nn_input - Dimension of random effects\nn_layers - Number of planar transformation layers\ninit - Initialization function (default: Glorot normal)\nθ - Flattened flow parameters\nrebuild - Function to reconstruct bijector from θ\nq0 - Base distribution (typically MvNormal)\n\n\n# Theory\nPlanar flows apply transformations of the form:\n\nf(z) = z + u·h(wᵀz + b)\n\nwhere `h` is a nonlinear activation (typically `tanh`), and `u`, `w`, `b` are learnable\nparameters. Multiple layers compose to create complex distributions:\n\nx = fK ∘ f{K-1} ∘ ... ∘ f_1(z₀),  z₀ ~ q₀\n\n\nThe log-density is computed via change of variables:\n\nlog p(x) = log q₀(z₀) - Σᵢ log|det(Jfᵢ)| ```\n\nAdvantages\n\nFlexibility: Can approximate complex, multimodal distributions\nExpressiveness: Captures non-Gaussian features (skewness, heavy tails, multimodality)\nDifferentiability: Fully differentiable for gradient-based optimization\nInterpretability: Parameters are estimated along with other fixed effects\n\nLimitations\n\nComputational cost: More expensive than multivariate normal\nConvergence: May require more iterations to converge\nIdentifiability: Flow parameters and base distribution parameters may trade off\n\nImplementation Details\n\nUses planar layer architecture from NormalizingFlows.jl\nParameters are flattened for optimization and reconstructed during evaluation\nBase distribution is typically MvNormal(zeros(d), I)\nDefault initialization: Glorot normal scaled by 1/√n_input\n\nSee Also\n\nNPFParameter - Parameter specification for flows (in @fixedEffects)\nPlanarLayer - Individual transformation layer (from NormalizingFlows.jl)\n\nReferences\n\nRezende, D. J., & Mohamed, S. (2015). \"Variational Inference with Normalizing Flows\" ICML 2015.\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.SoftTree","page":"API","title":"NoLimits.SoftTree","text":"SoftTree(input_dim::Int, depth::Int, n_output::Int)\n\nA differentiable soft decision tree with input_dim input features, depth levels, and n_output outputs.\n\nThe tree has 2^depth - 1 internal nodes and 2^depth leaves. Each internal node applies a soft sigmoid split to route inputs; each leaf stores a learnable output value. The forward pass returns the weighted sum of leaf values, differentiable with respect to both inputs and parameters.\n\nArguments\n\ninput_dim::Int: number of input features (must be > 0).\ndepth::Int: number of tree levels (must be > 0).\nn_output::Int: number of output values per evaluation (must be > 0).\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.SoftTreeParams","page":"API","title":"NoLimits.SoftTreeParams","text":"SoftTreeParams{WM, BV, LM}\n\nParameters for a SoftTree. Created via init_params.\n\nFields:\n\nnode_weights::WM: weight matrix of shape (n_internal, input_dim).\nnode_biases::BV: bias vector of length n_internal.\nleaf_values::LM: leaf value matrix of shape (n_output, n_leaves).\n\n\n\n\n\n","category":"type"},{"location":"api/#NoLimits.init_params","page":"API","title":"NoLimits.init_params","text":"init_params(tree::SoftTree; init_weight=0.0, init_bias=0.0, init_leaf=0.0)\n-> SoftTreeParams\n\ninit_params(tree::SoftTree, rng::AbstractRNG; init_weight_std=0.1,\n            init_bias_std=0.0, init_leaf_std=0.1) -> SoftTreeParams\n\nInitialise parameters for a SoftTree.\n\nThe no-rng overload fills all parameters with the given constant values. The rng overload draws parameters from zero-mean Normal distributions with the specified standard deviations.\n\nArguments\n\ntree::SoftTree: the soft tree architecture.\nrng::AbstractRNG: random-number generator (second overload only).\n\nKeyword Arguments (constant initialisation)\n\ninit_weight::Real = 0.0: node weight initial value.\ninit_bias::Real = 0.0: node bias initial value.\ninit_leaf::Real = 0.0: leaf value initial value.\n\nKeyword Arguments (random initialisation)\n\ninit_weight_std::Real = 0.1: standard deviation for node weights.\ninit_bias_std::Real = 0.0: standard deviation for node biases.\ninit_leaf_std::Real = 0.1: standard deviation for leaf values.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.destructure_params","page":"API","title":"NoLimits.destructure_params","text":"destructure_params(params::SoftTreeParams) -> (Vector, Restructure)\n\nFlatten a SoftTreeParams to a parameter vector and return the vector together with a reconstruction function (using Optimisers.destructure).\n\nThe reconstruction function can be called with a new flat vector to reconstruct a SoftTreeParams with the same structure.\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.bspline_basis","page":"API","title":"NoLimits.bspline_basis","text":"bspline_basis(x::Real, knots::AbstractVector{<:Real}, degree::Integer)\n-> Vector{Float64}\n\nEvaluate the B-spline basis functions of the given degree at the scalar point x using the provided knots vector.\n\nReturns a vector of length length(knots) - degree - 1 containing the values of each basis function at x. The knots must be sorted in non-decreasing order and x must lie within [knots[1], knots[end]].\n\nArguments\n\nx::Real: evaluation point.\nknots::AbstractVector{<:Real}: sorted knot sequence (may include repeated boundary knots).\ndegree::Integer: polynomial degree of the spline (e.g. 2 for quadratic, 3 for cubic).\n\n\n\n\n\n","category":"function"},{"location":"api/#NoLimits.bspline_eval","page":"API","title":"NoLimits.bspline_eval","text":"bspline_eval(x::Real, coeffs::AbstractVector{<:Real},\n             knots::AbstractVector{<:Real}, degree::Integer) -> Real\n\nbspline_eval(x::AbstractVector{<:Real}, coeffs::AbstractVector{<:Real},\n             knots::AbstractVector{<:Real}, degree::Integer) -> Real\n\nEvaluate a B-spline at x given coefficient vector coeffs, knot sequence knots, and polynomial degree.\n\nThe coefficient vector must have length length(knots) - degree - 1. When x is a length-1 vector it is treated as a scalar.\n\nArguments\n\nx::Real (or length-1 AbstractVector): evaluation point.\ncoeffs::AbstractVector{<:Real}: B-spline coefficients.\nknots::AbstractVector{<:Real}: sorted knot sequence.\ndegree::Integer: polynomial degree.\n\n\n\n\n\n","category":"function"},{"location":"model-building/model-macro/#@Model","page":"@Model","title":"@Model","text":"@Model is the top-level macro that assembles a complete NoLimits model from one or more block macros. It validates the block composition, wires cross-block dependencies, and returns a Model struct ready for data binding and estimation.","category":"section"},{"location":"model-building/model-macro/#Syntax","page":"@Model","title":"Syntax","text":"A minimal model requires only @formulas and at least one parameter (fixed or random):\n\nmodel = @Model begin\n    @fixedEffects begin\n        a = RealNumber(0.2)\n        sigma = RealNumber(0.3, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n    end\n\n    @formulas begin\n        y ~ Normal(a, sigma)\n    end\nend\n\nOnly macro blocks are allowed inside @Model – no bare Julia statements.","category":"section"},{"location":"model-building/model-macro/#Available-Blocks","page":"@Model","title":"Available Blocks","text":"Each block can appear at most once within @Model:\n\nBlock Required? Purpose\n@helpers No Reusable helper functions\n@fixedEffects No* Population-level parameters\n@covariates No Covariate declarations\n@randomEffects No* Random-effect definitions\n@preDifferentialEquation No Time-constant derived quantities\n@DifferentialEquation No ODE dynamics\n@initialDE No ODE initial conditions\n@formulas Yes Observation model\n\n*At least one of @fixedEffects or @randomEffects must be present.","category":"section"},{"location":"model-building/model-macro/#Validation-Rules","page":"@Model","title":"Validation Rules","text":"@Model enforces the following constraints at construction time:\n\n@formulas is always required.\nAt least one fixed effect or one random effect must be defined.\n@DifferentialEquation requires @initialDE, and vice versa.\nDuplicate blocks are rejected.\nUnknown blocks are rejected.\nNon-macro statements inside @Model are rejected.\n\nWhen a @DifferentialEquation block is present, additional covariate usage rules are enforced:\n\nNon-dynamic varying covariates cannot appear in DE expressions.\nDynamic covariates must be called with a time argument (e.g., w(t)), not used as bare symbols.\nConstant covariates cannot be called as functions in DEs.","category":"section"},{"location":"model-building/model-macro/#Default-Behavior-for-Omitted-Blocks","page":"@Model","title":"Default Behavior for Omitted Blocks","text":"Blocks that are omitted default to empty or inactive states:\n\n@helpers defaults to an empty NamedTuple.\n@fixedEffects, @covariates, and @randomEffects default to empty blocks.\n@preDifferentialEquation, @DifferentialEquation, and @initialDE default to nothing.","category":"section"},{"location":"model-building/model-macro/#Example:-Nonlinear-Mixed-Effects-Model-(No-ODE)","page":"@Model","title":"Example: Nonlinear Mixed-Effects Model (No ODE)","text":"using NoLimits\nusing Distributions\n\nmodel = @Model begin\n    @helpers begin\n        sat(u) = u / (1 + abs(u))\n    end\n\n    @fixedEffects begin\n        a = RealNumber(0.4)\n        sigma = RealNumber(0.3, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n        x = ConstantCovariateVector([:Age, :BMI]; constant_on=:ID)\n    end\n\n    @randomEffects begin\n        eta = RandomEffect(TDist(6.0); column=:ID)\n    end\n\n    @formulas begin\n        mu = sat(a + eta^2 + 0.01 * x.Age^2 + log1p(x.BMI^2))\n        y ~ Normal(mu, sigma)\n    end\nend","category":"section"},{"location":"model-building/model-macro/#Example:-Full-ODE-Model","page":"@Model","title":"Example: Full ODE Model","text":"This example demonstrates the full composition of blocks, including a dynamic covariate, a pre-differential-equation transformation, ODE dynamics with a derived signal, and a nonlinear observation model:\n\nusing NoLimits\nusing Distributions\n\nmodel = @Model begin\n    @fixedEffects begin\n        a = RealNumber(0.25)\n        b = RealNumber(0.1)\n        sigma = RealNumber(0.2, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n        w = DynamicCovariate()\n    end\n\n    @randomEffects begin\n        eta = RandomEffect(SkewNormal(0.0, 1.0, 0.8); column=:ID)\n    end\n\n    @preDifferentialEquation begin\n        pre = exp(a + eta^2)\n    end\n\n    @DifferentialEquation begin\n        s(t) = tanh(w(t) + pre)\n        D(x1) ~ -(b + tanh(eta)) * x1 + s(t)^2\n    end\n\n    @initialDE begin\n        x1 = pre\n    end\n\n    @formulas begin\n        y ~ Gamma(log1p(abs(x1(t)) + eta^2) + 1e-6, sigma)\n    end\nend","category":"section"},{"location":"model-building/model-macro/#Model-Summary","page":"@Model","title":"Model Summary","text":"After construction, use NoLimits.summarize(model) to inspect the declared structure:\n\nmodel_summary = NoLimits.summarize(model)\nmodel_summary","category":"section"},{"location":"model-building/model-macro/#Runtime-Evaluation-Helpers","page":"@Model","title":"Runtime Evaluation Helpers","text":"@Model wires internal evaluation functions used during fitting and simulation:\n\ncalculate_prede(...) – evaluate pre-DE expressions\ncalculate_initial_state(...) – compute ODE initial conditions\ncalculate_formulas_all(...) – evaluate all formula nodes\ncalculate_formulas_obs(...) – evaluate observation-node formulas only\n\nWhen formulas reference ODE states or signals, formula evaluation requires DE solution accessors from get_de_accessors_builder(...).","category":"section"},{"location":"estimation/vi/#VI","page":"VI","title":"VI","text":"Variational inference (VI) provides approximate Bayesian inference by optimizing a parameterized posterior family instead of drawing Markov chains. In NoLimits.jl, VI is integrated through Turing + AdvancedVI and supports both fixed-effects-only and mixed-effects models.\n\nCompared with MCMC, VI is often faster and easier to scale, but it returns an approximation whose quality depends on the selected variational family.","category":"section"},{"location":"estimation/vi/#Applicability","page":"VI","title":"Applicability","text":"The following conditions must hold to use VI:\n\nAll free fixed effects must have priors.\nAt least one parameter must be sampled.\nFixed-only models: at least one fixed effect must remain free.\nMixed-effects models: random effects can be sampled even if all fixed effects are held constant.\npenalty is not supported.\n\nVI samples on the natural (untransformed) parameter scale.","category":"section"},{"location":"estimation/vi/#Basic-Usage-(Fixed-Effects)","page":"VI","title":"Basic Usage (Fixed Effects)","text":"using NoLimits\nusing DataFrames\nusing Distributions\nusing Random\n\nmodel = @Model begin\n    @covariates begin\n        t = Covariate()\n    end\n\n    @fixedEffects begin\n        a = RealNumber(0.0, prior=Normal(0.0, 1.0))\n        b = RealNumber(0.3, prior=Normal(0.0, 1.0))\n        sigma = RealNumber(0.2, scale=:log, prior=LogNormal(-1.5, 0.3))\n    end\n\n    @formulas begin\n        y ~ Normal(a + b * t, sigma)\n    end\nend\n\ndf = DataFrame(\n    ID = [:A, :A, :B, :B, :C, :C],\n    t = [0.0, 1.0, 0.0, 1.0, 0.0, 1.0],\n    y = [0.1, 0.45, -0.05, 0.22, 0.02, 0.32],\n)\n\ndm = DataModel(model, df; primary_id=:ID, time_col=:t)\n\nres = fit_model(\n    dm,\n    NoLimits.VI(; turing_kwargs=(max_iter=300, family=:meanfield, progress=false)),\n    rng=Random.Xoshiro(1),\n)","category":"section"},{"location":"estimation/vi/#Constructor-Options","page":"VI","title":"Constructor Options","text":"using NoLimits\n\nmethod = NoLimits.VI(; turing_kwargs=NamedTuple())\n\nturing_kwargs are forwarded to Turing.vi after NoLimits consumes VI-specific control keys:\n\nmax_iter::Int (default 1000)\nfamily::Symbol (:meanfield or :fullrank, default :meanfield)\nq_init (optional custom variational initialization)\nadtype (default Turing.AutoForwardDiff())\nprogress / show_progress (default false)\nalgorithm (optional AdvancedVI algorithm)\nconvergence_window, convergence_rtol, convergence_atol (NoLimits convergence rule)","category":"section"},{"location":"estimation/vi/#Fit-Keywords","page":"VI","title":"Fit Keywords","text":"fit_model(dm, NoLimits.VI(...); ...) supports:\n\nconstants for fixed effects\nconstants_re for selected random-effect levels\node_args, ode_kwargs\nserialization\nrng\ntheta_0_untransformed (currently only relevant when custom q_init uses it)\nstore_data_model\n\nNot supported:\n\npenalty","category":"section"},{"location":"estimation/vi/#Mixed-Effects-Pattern","page":"VI","title":"Mixed-Effects Pattern","text":"using NoLimits\nusing DataFrames\nusing Distributions\nusing Random\n\nmodel_re = @Model begin\n    @covariates begin\n        t = Covariate()\n    end\n\n    @fixedEffects begin\n        a = RealNumber(0.0, prior=Normal(0.0, 1.0))\n        b = RealNumber(0.2, prior=Normal(0.0, 1.0))\n        omega = RealNumber(0.4, scale=:log, prior=LogNormal(-1.0, 0.4))\n        sigma = RealNumber(0.2, scale=:log, prior=LogNormal(-1.5, 0.3))\n    end\n\n    @randomEffects begin\n        eta = RandomEffect(Normal(0.0, omega); column=:ID)\n    end\n\n    @formulas begin\n        y ~ Normal(a + b * t + eta, sigma)\n    end\nend\n\ndf_re = DataFrame(\n    ID = [:A, :A, :B, :B, :C, :C, :D, :D],\n    t = [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0],\n    y = [0.05, 0.25, -0.1, 0.15, 0.15, 0.35, -0.05, 0.10],\n)\n\ndm_re = DataModel(model_re, df_re; primary_id=:ID, time_col=:t)\n\nres_re = fit_model(\n    dm_re,\n    NoLimits.VI(; turing_kwargs=(max_iter=400, family=:fullrank, progress=false)),\n    rng=Random.Xoshiro(2),\n)","category":"section"},{"location":"estimation/vi/#Accessing-VI-Outputs","page":"VI","title":"Accessing VI Outputs","text":"VI does not return an MCMC chain. Use VI-specific accessors instead:\n\nobjective = get_objective(res)                    # final ELBO\nconverged = get_converged(res)                    # NoLimits convergence flag\ntrace = get_vi_trace(res)                         # per-iteration trace entries\nstate = get_vi_state(res)                         # final optimizer state\nposterior = get_variational_posterior(res)        # variational posterior object\n\ndraws = sample_posterior(\n    res;\n    n_draws=200,\n    rng=Random.Xoshiro(3),\n    return_names=true,\n)\n\nYou can inspect compact summaries with:\n\nfit_summary = NoLimits.summarize(res)\nfit_summary","category":"section"},{"location":"estimation/vi/#Uncertainty-Quantification-with-VI","page":"VI","title":"Uncertainty Quantification with VI","text":"For VI fits, use compute_uq(...; method=:chain) to build intervals from posterior samples drawn from the variational posterior:\n\nuq = compute_uq(\n    res;\n    method=:chain,\n    level=0.95,\n    mcmc_draws=150,\n    rng=Random.Xoshiro(4),\n)\n\nmcmc_warmup is ignored for VI because there is no chain adaptation phase.","category":"section"},{"location":"estimation/vi/#Multistart-with-VI","page":"VI","title":"Multistart with VI","text":"VI is fully compatible with Multistart. Use it when the ELBO landscape may be multimodal or when sensitivity to initialization is a concern:\n\nusing NoLimits\nusing Random\n\nms = NoLimits.Multistart(;\n    n_draws_requested = 10,\n    n_draws_used      = 3,\n    sampling          = :lhs,\n    rng               = Random.Xoshiro(42),\n)\n\nres_ms = fit_model(\n    ms, dm,\n    NoLimits.VI(; turing_kwargs=(max_iter=300, family=:meanfield, progress=false)),\n    rng=Random.Xoshiro(1),\n)\n\nposterior = get_variational_posterior(res_ms)\nobjective = get_objective(res_ms)   # final ELBO of the best run\n\nThe screening phase evaluates the marginal log-likelihood at η = 0 for each candidate before running any VI optimization. The best run is selected by the final ELBO value (get_objective). See the Multistart page for full details on the two-phase workflow and logging.","category":"section"},{"location":"estimation/vi/#Practical-Notes","page":"VI","title":"Practical Notes","text":"Start with family=:meanfield for speed, then compare with :fullrank when posterior correlations are expected.\nCheck get_vi_trace(res) and downstream predictive diagnostics (plot_fits, residual plots, VPC) to assess approximation quality.\nFor highly multimodal or strongly non-Gaussian posteriors, MCMC remains the more faithful Bayesian baseline.","category":"section"},{"location":"model-building/#Model-Building","page":"Overview","title":"Model Building","text":"Model specification in NoLimits.jl uses a composable domain-specific language (DSL) centered on the @Model macro. The key design principle is that mechanistic structure, hierarchical variability, and learned nonlinear components can all be expressed and combined within a single, coherent model definition.\n\nAll examples in this documentation section use nonlinear models. When random effects are present, they enter the model nonlinearly – reflecting the typical use case in applied longitudinal analysis.","category":"section"},{"location":"model-building/#Overview","page":"Overview","title":"Overview","text":"This page introduces the main composition patterns available in the modeling DSL. Detailed documentation for each block macro is provided on the dedicated sub-pages listed at the bottom of this page.","category":"section"},{"location":"model-building/#What-Can-Be-Composed","page":"Overview","title":"What Can Be Composed","text":"The @Model DSL supports flexible combinations of:\n\nMultiple outcomes in a single model, including mixed outcome types (e.g., a continuous measurement and a count variable jointly).\nMultiple random-effect grouping structures – for example, subject-level and site-level variability in the same specification.\nCovariates at different temporal resolutions – time-varying, group-constant, and interpolated covariates can coexist.\nLearned function approximators such as neural networks and soft decision trees, freely combined with known mechanistic terms.\nODE and non-ODE workflows sharing the same specification language.\n\nThe following examples illustrate representative composition patterns.","category":"section"},{"location":"model-building/#Example:-Multiple-Outcomes-and-Grouping-Structures","page":"Overview","title":"Example: Multiple Outcomes and Grouping Structures","text":"This model defines two outcomes (y1, y2) with three random effects spanning two grouping levels (:ID and :SITE), together with constant covariates and nonlinear transformations:\n\nmodel = @Model begin\n    @fixedEffects begin\n        a = RealNumber(0.2)\n        b = RealNumber(0.1)\n        s1 = RealNumber(0.3, scale=:log)\n        σ2 = RealNumber(0.4, scale=:log)\n        μ_re = RealVector([0.0, 0.0])\n        Ω_re = RealPSDMatrix([0.6 0.0; 0.0 0.9], scale=:cholesky)\n    end\n\n    @covariates begin\n        t = Covariate()\n        x = ConstantCovariateVector([:Age, :BMI]; constant_on=[:ID, :SITE])\n    end\n\n    @randomEffects begin\n        η_id = RandomEffect(TDist(6.0); column=:ID)\n        η_mv = RandomEffect(MvNormal(μ_re, Ω_re); column=:ID)\n        η_site = RandomEffect(SkewNormal(0.0, 1.0, 0.8); column=:SITE)\n    end\n\n    @formulas begin\n        μ = a + b * x.Age^2 + log1p(x.BMI^2) + tanh(η_id) + η_mv[1]^2 + tanh(η_mv[2]) + η_site^2\n        y1 ~ LogNormal(μ, s1)\n        y2 ~ Gamma(abs(μ) + 1e-6, σ2)\n    end\nend","category":"section"},{"location":"model-building/#Example:-Learned-Components-in-Random-Effect-Distributions","page":"Overview","title":"Example: Learned Components in Random-Effect Distributions","text":"Neural networks and soft decision trees can parameterize random-effect distributions, enabling covariate-dependent heterogeneity structures that are difficult to express in conventional NLME software:\n\nchain = Chain(Dense(2, 4, tanh), Dense(4, 1))\n\nmodel = @Model begin\n    @fixedEffects begin\n        μ0 = RealNumber(0.2)\n        ση = RealNumber(0.7, scale=:log)\n        sy = RealNumber(0.2, scale=:log)\n        ζ = NNParameters(chain; function_name=:NN1, calculate_se=false)\n        Γ = SoftTreeParameters(2, 2; function_name=:ST1, calculate_se=false)\n    end\n\n    @covariates begin\n        x = ConstantCovariateVector([:Age, :BMI]; constant_on=:ID)\n    end\n\n    @randomEffects begin\n        η = RandomEffect(\n            LogNormal(μ0 + NN1([x.Age, x.BMI], ζ)[1] + ST1([x.Age, x.BMI], Γ)[1], ση);\n            column=:ID\n        )\n    end\n\n    @formulas begin\n        y ~ Exponential(log1p(η^2) + sy)\n    end\nend","category":"section"},{"location":"model-building/#Example:-Hidden-Markov-Observation-Model","page":"Overview","title":"Example: Hidden Markov Observation Model","text":"For data generated by a latent switching process, hidden Markov outcome models can be specified directly in @formulas:\n\nmodel = @Model begin\n    @fixedEffects begin\n        β = RealNumber(0.4)\n    end\n\n    @randomEffects begin\n        η_id = RandomEffect(Gamma(2.0, 1.0); column=:ID)\n    end\n\n    @formulas begin\n        p11 = 1 / (1 + exp(-(β + η_id^2)))\n        p22 = 1 / (1 + exp(-(β^2 + log1p(η_id^2))))\n        P = [p11 1 - p11; 1 - p22 p22]\n        y ~ DiscreteTimeDiscreteStatesHMM(\n            P,\n            (Laplace(-0.5, 0.4), LogNormal(0.1, 0.6)),\n            Categorical([0.5, 0.5]),\n        )\n    end\nend","category":"section"},{"location":"model-building/#Block-Reference","page":"Overview","title":"Block Reference","text":"The following sub-pages document each model block in detail:\n\n@Model – Top-level macro and validation rules\n@helpers – Reusable helper functions\n@fixedEffects – Population-level parameter definitions\n@covariates – Covariate types and interpolation\n@randomEffects – Random-effect distributions and grouping\n@preDifferentialEquation – Time-constant derived quantities\n@DifferentialEquation – ODE dynamics and derived signals\n@initialDE – Initial conditions for ODE states\n@formulas – Observation models and deterministic nodes\nFunction Approximators (NNs + SoftTrees) – Neural networks and soft decision trees","category":"section"},{"location":"tutorials/mixed-effects-multiple-methods/#Mixed-Effects-Tutorial-1:-Nonlinear-Random-Effects-Model-Across-Multiple-Estimation-Methods","page":"Mixed-Effects Tutorial 1: Nonlinear Random-Effects Model Across Multiple Estimation Methods","title":"Mixed-Effects Tutorial 1: Nonlinear Random-Effects Model Across Multiple Estimation Methods","text":"Nonlinear mixed-effects (NLME) models are a cornerstone of longitudinal data analysis in the biological sciences. They describe how individual trajectories vary around a shared population-level trend – capturing, for example, how different organisms grow at different rates toward different asymptotes, even when the underlying biological mechanism is the same. A natural question arises in practice: how sensitive are my conclusions to the estimation algorithm I choose? This tutorial addresses that question directly. You will fit a single nonlinear growth model to a classic biological dataset using five distinct estimation strategies, then compare the results in terms of fitted trajectories, observation-level predictions, and parameter uncertainty. By the end, you will have a practical template for multi-method comparison and a clear intuition for when each approach is most appropriate.","category":"section"},{"location":"tutorials/mixed-effects-multiple-methods/#What-You-Will-Learn","page":"Mixed-Effects Tutorial 1: Nonlinear Random-Effects Model Across Multiple Estimation Methods","title":"What You Will Learn","text":"By the end of this tutorial, you will be able to:\n\nBuild a nonlinear mixed-effects model with lognormal random effects on a growth asymptote.\nConfigure five fundamentally different estimation strategies – Laplace approximation, MCEM, SAEM, full Bayesian MCMC, and variational inference (VI) – with sensible defaults.\nCompare methods in predictive space using NoLimits' diagnostic and visualization tools, rather than relying solely on objective function values.\nInterpret where the estimators converge, where they diverge, and what each method uniquely provides.\n\nThe goal is not just to run five fits, but to build understanding of the trade-offs involved in choosing an estimation strategy for your own longitudinal analyses.","category":"section"},{"location":"tutorials/mixed-effects-multiple-methods/#Step-1:-Data-Setup","page":"Mixed-Effects Tutorial 1: Nonlinear Random-Effects Model Across Multiple Estimation Methods","title":"Step 1: Data Setup","text":"In this first step, you will load the Orange tree growth dataset, a classic longitudinal dataset originally from Draper and Smith (1981) and available in R's datasets package. The dataset records the trunk circumference of five orange trees measured at seven time points over approximately four years. Although small, it is representative of a broad class of problems in biology: repeated measurements of a continuous outcome on a set of individuals, with growth that follows a saturating nonlinear trajectory. The between-tree variation in final size makes it a natural candidate for random-effects modeling.\n\nThe code below loads the required packages and retrieves the data directly from the Rdatasets GitHub repository.\n\nusing NoLimits\nusing CSV\nusing DataFrames\nusing Distributions\nusing Downloads\nusing Random\nusing SciMLBase\nusing Turing\n\ninclude(joinpath(@__DIR__, \"_data_loaders.jl\"))\n\nRandom.seed!(42)\n\ndf = load_orange()\n\nfirst(df, 8)","category":"section"},{"location":"tutorials/mixed-effects-multiple-methods/#Step-2:-Define-the-Nonlinear-Mixed-Effects-Model","page":"Mixed-Effects Tutorial 1: Nonlinear Random-Effects Model Across Multiple Estimation Methods","title":"Step 2: Define the Nonlinear Mixed-Effects Model","text":"Next, you will specify the statistical model. The biological reasoning is straightforward: each tree's circumference follows a saturating growth curve, increasing from an initial size toward a tree-specific maximum. You will model this maximum (the asymptote) as a random effect, allowing each tree to have its own upper bound while sharing a common growth shape across the population.\n\nConcretely, the model uses a logistic-style saturating function with three population-level parameters: an initial size phi1, a log-scale population mean for the asymptote log_vmax, and a midpoint parameter phi3 that controls the timing of the growth inflection. Each tree's individual asymptote vmax_i is drawn from a lognormal distribution, which ensures positivity and places between-tree variability on a multiplicative scale – a natural choice when larger individuals tend to show proportionally larger variation. The observation model is also lognormal, so residual variability scales with the predicted circumference rather than being additive. To maintain numerical stability during optimization, the predicted mean is passed through a softplus function that enforces positivity without introducing hard discontinuities.\n\nAll fixed effects are given weakly informative priors. These priors are not strictly necessary for the optimization-based methods (Laplace, MCEM, SAEM), but they are required for MCMC and VI and serve to regularize the likelihood surface for all methods.\n\nmodel = @Model begin\n    @helpers begin\n        softplus(u) = u > 20 ? u : log1p(exp(u))\n    end\n\n    @covariates begin\n        age = Covariate()\n    end\n\n    @fixedEffects begin\n        phi1     = RealNumber(30.0,  prior=LogNormal(log(30.0), 0.30), calculate_se=true)\n        log_vmax = RealNumber(10.0,  prior=Normal(5.00, 0.35),          calculate_se=true)\n        phi3     = RealNumber(700.0, prior=LogNormal(log(700.0), 0.30), calculate_se=true)\n        omega    = RealNumber(0.3, scale=:log, prior=LogNormal(log(0.155), 0.35), calculate_se=true)\n        sigma    = RealNumber(0.3, scale=:log, prior=LogNormal(log(0.113), 0.30), calculate_se=true)\n    end\n\n    @randomEffects begin\n        vmax_i = RandomEffect(LogNormal(log_vmax, omega); column=:Tree)\n    end\n\n    @formulas begin\n        mu_raw = phi1 + (vmax_i - phi1) / (1 + exp(-(age - phi3) / 100))\n        mu = softplus(mu_raw) + 1e-6\n        circumference ~ LogNormal(log(mu), sigma)\n    end\nend","category":"section"},{"location":"tutorials/mixed-effects-multiple-methods/#Model-Summary","page":"Mixed-Effects Tutorial 1: Nonlinear Random-Effects Model Across Multiple Estimation Methods","title":"Model Summary","text":"You can inspect the model structure to verify that all blocks were parsed correctly and that the parameter dimensions, scales, and priors match what you intended.\n\nmodel_summary = NoLimits.summarize(model)\nmodel_summary","category":"section"},{"location":"tutorials/mixed-effects-multiple-methods/#Step-3:-Build-the-DataModel-and-Configure-Estimation-Methods","page":"Mixed-Effects Tutorial 1: Nonlinear Random-Effects Model Across Multiple Estimation Methods","title":"Step 3: Build the DataModel and Configure Estimation Methods","text":"In this step, you will wrap the model and data together into a DataModel – a structure that validates the data schema, groups individuals into batches for the random effects structure, and prepares internal representations for each estimation method.\n\nYou will then configure five estimation methods. Each represents a fundamentally different strategy for handling the random effects integral that appears in the marginal likelihood:\n\nLaplace approximates the integral analytically using a second-order Taylor expansion around each individual's best estimate of the random effects (the empirical Bayes estimate). It is fast and deterministic, making it a good default for moderate-sized problems. However, the approximation can lose accuracy when the true distribution of random effects is far from Gaussian.\nMCEM (Monte Carlo Expectation-Maximization) uses MCMC sampling within each iteration to approximate the expected complete-data log-likelihood, then maximizes that approximation. In plain terms, it alternates between \"filling in\" the missing random effects via sampling and updating the population parameters given those samples. It is more robust than Laplace to non-Gaussian random effects but requires more computation.\nSAEM (Stochastic Approximation EM) follows a similar alternating logic but replaces the full sampling step with a stochastic approximation that updates a running average of sufficient statistics. This means it converges with fewer samples per iteration than MCEM, making it attractive for larger problems, though its stochastic nature can make convergence harder to diagnose.\nMCMC (Markov chain Monte Carlo) samples the full joint posterior over both fixed and random effects. Rather than returning a single \"best\" parameter estimate, it produces a collection of plausible parameter sets that together characterize uncertainty. This provides the richest picture of parameter uncertainty – including asymmetric or multimodal posteriors – but is the most computationally expensive and requires careful convergence assessment.\nVI (variational inference) optimizes an approximate posterior family instead of drawing full chains. It is often much faster than MCMC and still provides posterior draws for predictive and uncertainty analyses, but it is approximate and can miss complex posterior features if the chosen variational family is too restrictive.\n\nThe configuration values below are chosen to balance runtime and stability for this tutorial; in a research setting, you would typically increase iteration counts, sample sizes, and warmup periods.\n\ndm = DataModel(model, df; primary_id=:Tree, time_col=:age)\n\nlaplace_method = NoLimits.Laplace(; multistart_n=0, multistart_k=0, optim_kwargs=(maxiters=120,))\n\nmcem_method = NoLimits.MCEM(;\n    maxiters=6,\n    sample_schedule=i -> min(40 + 20 * (i - 1), 140),\n    turing_kwargs=(n_samples=40, n_adapt=15, progress=false),\n    optim_kwargs=(maxiters=120,),\n    progress=false,\n)\n\nsaem_method = NoLimits.SAEM(;\n    maxiters=80,\n    mcmc_steps=16,\n    t0=15,\n    kappa=0.65,\n    turing_kwargs=(n_adapt=20, progress=false),\n    optim_kwargs=(maxiters=160,),\n    verbose=false,\n    progress=false,\n)\n\nmcmc_method = NoLimits.MCMC(;\n    sampler=NUTS(0.75),\n    progress=false,\n    turing_kwargs=(n_samples=1000, n_adapt=500, progress=false),\n)\n\nvi_method = NoLimits.VI(;\n    turing_kwargs=(max_iter=30000, progress=false),\n)\n\nserialization = SciMLBase.EnsembleThreads()","category":"section"},{"location":"tutorials/mixed-effects-multiple-methods/#DataModel-Summary","page":"Mixed-Effects Tutorial 1: Nonlinear Random-Effects Model Across Multiple Estimation Methods","title":"DataModel Summary","text":"Before proceeding to estimation, inspect the DataModel summary to confirm that individuals, covariates, and random effect groupings were detected correctly.\n\ndm_summary = NoLimits.summarize(dm)\ndm_summary","category":"section"},{"location":"tutorials/mixed-effects-multiple-methods/#Step-4:-Fit-All-Methods","page":"Mixed-Effects Tutorial 1: Nonlinear Random-Effects Model Across Multiple Estimation Methods","title":"Step 4: Fit All Methods","text":"With the model, data, and methods configured, you will now fit the same DataModel with all five estimators. Each call to fit_model returns a FitResult object that stores the estimated parameters, convergence diagnostics, and a reference to the DataModel for downstream analysis.\n\nEach method receives a different random seed to ensure reproducibility while allowing independent stochastic behavior across methods.\n\nres_laplace = fit_model(dm, laplace_method; serialization=serialization, rng=Random.Xoshiro(11))\nres_mcem = fit_model(dm, mcem_method; serialization=serialization, rng=Random.Xoshiro(12))\nres_saem = fit_model(dm, saem_method; serialization=serialization, rng=Random.Xoshiro(13))\nres_mcmc = fit_model(dm, mcmc_method; serialization=serialization, rng=Random.Xoshiro(14))\nres_vi = fit_model(dm, vi_method; serialization=serialization, rng=Random.Xoshiro(12))\n","category":"section"},{"location":"tutorials/mixed-effects-multiple-methods/#FitResult-Summaries","page":"Mixed-Effects Tutorial 1: Nonlinear Random-Effects Model Across Multiple Estimation Methods","title":"FitResult Summaries","text":"Each fit result can be summarized to display estimated parameter values, convergence status, and method-specific diagnostics. Reviewing these summaries side by side is a quick first check for whether the methods have arrived at broadly similar parameter estimates.\n\nfit_summary_laplace = NoLimits.summarize(res_laplace)\nfit_summary_mcem = NoLimits.summarize(res_mcem)\nfit_summary_saem = NoLimits.summarize(res_saem)\nfit_summary_mcmc = NoLimits.summarize(res_mcmc)\nfit_summary_vi = NoLimits.summarize(res_vi)\n\nfit_summary_laplace","category":"section"},{"location":"tutorials/mixed-effects-multiple-methods/#Step-5:-Compare-Objective-Values-(Laplace,-MCEM,-SAEM,-VI)","page":"Mixed-Effects Tutorial 1: Nonlinear Random-Effects Model Across Multiple Estimation Methods","title":"Step 5: Compare Objective Values (Laplace, MCEM, SAEM, VI)","text":"It is tempting to compare objective function values across methods, but this requires care: each method optimizes a different quantity, so raw values are not directly comparable.\n\nobjectives = (\n    laplace=NoLimits.get_objective(res_laplace),\n    mcem=NoLimits.get_objective(res_mcem),\n    saem=NoLimits.get_objective(res_saem),\n    vi=NoLimits.get_objective(res_vi),\n)\n\nobjectives\n\nThe signs and magnitudes differ because each method defines its objective differently:\n\nLaplace reports the minimized value of the Laplace-approximated marginal likelihood (a loss function, so lower is better).\nMCEM and SAEM report the EM auxiliary quantity Q at the final iterate, which is a lower bound on the log-likelihood (higher is better).\nVI reports the final evidence lower bound (ELBO), which is optimized over the variational family (higher is better within the same VI configuration).\n\nBecause these quantities differ by construction, raw values are not directly comparable across methods. Within a single method family, however, objective values can be compared meaningfully – for example, when evaluating different starting points or model specifications with the same estimator.","category":"section"},{"location":"tutorials/mixed-effects-multiple-methods/#Step-6:-Fitted-Trajectories-for-the-First-Two-Individuals","page":"Mixed-Effects Tutorial 1: Nonlinear Random-Effects Model Across Multiple Estimation Methods","title":"Step 6: Fitted Trajectories for the First Two Individuals","text":"The most informative way to compare methods is in predictive space: do the fitted trajectories agree when overlaid on the observed data? In this step, you will generate fit plots for the first two trees under each method. For MCMC and VI, you will additionally overlay posterior predictive quantile bands (5th and 95th percentiles), which provide a visual summary of prediction uncertainty that accounts for both parameter and random-effect uncertainty.\n\ninds = collect(1:min(2, length(dm.individuals)))\n\np_fit_laplace = plot_fits(\n    res_laplace;\n    observable=:circumference,\n    individuals_idx=inds,\n    ncols=2,\n    shared_x_axis=true,\n    shared_y_axis=true,\n)\n\np_fit_mcem = plot_fits(\n    res_mcem;\n    observable=:circumference,\n    individuals_idx=inds,\n    ncols=2,\n    shared_x_axis=true,\n    shared_y_axis=true,\n)\n\np_fit_saem = plot_fits(\n    res_saem;\n    observable=:circumference,\n    individuals_idx=inds,\n    ncols=2,\n    shared_x_axis=true,\n    shared_y_axis=true,\n)\n\np_fit_mcmc = plot_fits(\n    res_mcmc;\n    observable=:circumference,\n    individuals_idx=inds,\n    ncols=2,\n    shared_x_axis=true,\n    shared_y_axis=true,\n    plot_mcmc_quantiles=true,\n    mcmc_quantiles=[5, 95],\n    mcmc_warmup=500,\n    mcmc_draws=300,\n)\n\np_fit_vi = plot_fits(\n    res_vi;\n    observable=:circumference,\n    #individuals_idx=inds,\n    ncols=2,\n    shared_x_axis=true,\n    shared_y_axis=true,\n    plot_mcmc_quantiles=true,\n    mcmc_quantiles=[5, 95],\n    mcmc_draws=300,\n)\n\n\nWhen all five methods are well-calibrated, you should see broadly similar trajectories. Differences, when they appear, tend to be most visible in the tails of the growth curve where data are sparse.\n\nLaplace fit plot:\n\np_fit_laplace\n\nMCEM fit plot:\n\np_fit_mcem\n\nSAEM fit plot:\n\np_fit_saem\n\nMCMC fit plot (with posterior predictive bands):\n\np_fit_mcmc\n\nVI fit plot (with posterior predictive bands):\n\np_fit_vi","category":"section"},{"location":"tutorials/mixed-effects-multiple-methods/#Step-7:-Observation-Distribution-Diagnostics-(First-Individual)","page":"Mixed-Effects Tutorial 1: Nonlinear Random-Effects Model Across Multiple Estimation Methods","title":"Step 7: Observation Distribution Diagnostics (First Individual)","text":"Beyond trajectory-level agreement, you can examine how well each method captures the observation-level distribution. The plots below compare the observed circumference value for the first observation of the first tree against the model-implied observation distribution at that data point. If the model is well-specified, the observed value should fall in a region of reasonable density under the predicted distribution.\n\nThis diagnostic is particularly useful for detecting model misspecification. If the observed value consistently falls in the tail of the predicted distribution across individuals and methods, the observation model (here, lognormal) may need revision.\n\np_obs_laplace = plot_observation_distributions(\n    res_laplace;\n    observables=:circumference,\n    individuals_idx=1,\n    obs_rows=1,\n)\n\np_obs_mcem = plot_observation_distributions(\n    res_mcem;\n    observables=:circumference,\n    individuals_idx=1,\n    obs_rows=1,\n)\n\np_obs_saem = plot_observation_distributions(\n    res_saem;\n    observables=:circumference,\n    individuals_idx=1,\n    obs_rows=1,\n)\n\np_obs_mcmc = plot_observation_distributions(\n    res_mcmc;\n    observables=:circumference,\n    individuals_idx=1,\n    obs_rows=1,\n    mcmc_warmup=500,\n    mcmc_draws=300,\n)\n\np_obs_vi = plot_observation_distributions(\n    res_vi;\n    observables=:circumference,\n    individuals_idx=1,\n    obs_rows=1,\n    mcmc_draws=300,\n)\n\n\nLaplace observation distribution:\n\np_obs_laplace\n\nMCEM observation distribution:\n\np_obs_mcem\n\nSAEM observation distribution:\n\np_obs_saem\n\nMCMC observation distribution:\n\np_obs_mcmc\n\nVI observation distribution:\n\np_obs_vi","category":"section"},{"location":"tutorials/mixed-effects-multiple-methods/#Step-8:-Uncertainty-Quantification-Across-Methods","page":"Mixed-Effects Tutorial 1: Nonlinear Random-Effects Model Across Multiple Estimation Methods","title":"Step 8: Uncertainty Quantification Across Methods","text":"A key reason to compare methods is to understand how they characterize parameter uncertainty. The optimization-based methods (Laplace, MCEM, SAEM) return point estimates; you can obtain approximate uncertainty via Wald-type confidence intervals, which are derived from the curvature of the objective function at the optimum. Intuitively, a sharply peaked objective implies tight uncertainty, while a flat objective implies wide intervals. MCMC and VI, by contrast, produce posterior draws (exact-chain and variational-approximation respectively), enabling distribution-based uncertainty summaries.\n\nBelow, you will compute uncertainty quantification (UQ) summaries for all five methods and generate density plots of the resulting parameter distributions on the natural (untransformed) scale.\n\nuq_laplace = compute_uq(\n    res_laplace;\n    method=:wald,\n    vcov=:hessian,\n    pseudo_inverse=true,\n    serialization=serialization,\n    n_draws=400,\n    rng=Random.Xoshiro(101),\n)\nuq_mcem = compute_uq(\n    res_mcem;\n    method=:wald,\n    vcov=:hessian,\n    re_approx=:laplace,\n    pseudo_inverse=true,\n    serialization=serialization,\n    n_draws=400,\n    rng=Random.Xoshiro(102),\n)\nuq_saem = compute_uq(\n    res_saem;\n    method=:wald,\n    vcov=:hessian,\n    re_approx=:laplace,\n    pseudo_inverse=true,\n    serialization=serialization,\n    n_draws=400,\n    rng=Random.Xoshiro(103),\n)\nuq_mcmc = compute_uq(\n    res_mcmc;\n    method=:chain,\n    serialization=serialization,\n    mcmc_warmup=500,\n    mcmc_draws=300,\n    rng=Random.Xoshiro(104),\n)\nuq_vi = compute_uq(\n    res_vi;\n    method=:chain,\n    serialization=serialization,\n    mcmc_draws=300,\n    rng=Random.Xoshiro(105),\n)\n\np_uq_laplace = plot_uq_distributions(uq_laplace; scale=:natural, plot_type=:density, show_legend=false)\np_uq_mcem = plot_uq_distributions(uq_mcem; scale=:natural, plot_type=:density, show_legend=false)\np_uq_saem = plot_uq_distributions(uq_saem; scale=:natural, plot_type=:density, show_legend=false)\np_uq_mcmc = plot_uq_distributions(uq_mcmc; scale=:natural, plot_type=:density, show_legend=false)\np_uq_vi = plot_uq_distributions(uq_vi; scale=:natural, plot_type=:density, show_legend=false)\n","category":"section"},{"location":"tutorials/mixed-effects-multiple-methods/#UQ-Summaries","page":"Mixed-Effects Tutorial 1: Nonlinear Random-Effects Model Across Multiple Estimation Methods","title":"UQ Summaries","text":"The summaries below report point estimates alongside uncertainty intervals for each parameter. Where the methods agree, you can be confident that inference is robust. Where they diverge, the discrepancy may signal sensitivity to the estimation strategy, an under-identified parameter, or simply a need for more data.\n\nuq_summary_laplace = NoLimits.summarize(uq_laplace)\nuq_summary_mcem = NoLimits.summarize(uq_mcem)\nuq_summary_saem = NoLimits.summarize(uq_saem)\nuq_summary_mcmc = NoLimits.summarize(uq_mcmc)\nuq_summary_vi = NoLimits.summarize(uq_vi)\n\nfit_uq_summary_laplace = NoLimits.summarize(res_laplace, uq_laplace)\nfit_uq_summary_mcem = NoLimits.summarize(res_mcem, uq_mcem)\nfit_uq_summary_saem = NoLimits.summarize(res_saem, uq_saem)\nfit_uq_summary_mcmc = NoLimits.summarize(res_mcmc, uq_mcmc)\nfit_uq_summary_vi = NoLimits.summarize(res_vi, uq_vi)\n\nfit_uq_summary_laplace\n\nLaplace UQ distribution:\n\np_uq_laplace\n\nMCEM UQ distribution:\n\np_uq_mcem\n\nSAEM UQ distribution:\n\np_uq_saem\n\nMCMC UQ distribution:\n\np_uq_mcmc\n\nVI UQ distribution:\n\np_uq_vi","category":"section"},{"location":"tutorials/mixed-effects-multiple-methods/#Interpretation-and-Practical-Guidance","page":"Mixed-Effects Tutorial 1: Nonlinear Random-Effects Model Across Multiple Estimation Methods","title":"Interpretation and Practical Guidance","text":"Several principles emerge from this multi-method comparison:\n\nEvaluate agreement in predictive space, not objective space. Because each method optimizes a different quantity, comparing raw objective values across methods is misleading. Instead, compare fitted trajectories, observation-level distributions, and uncertainty intervals. When methods agree in predictive space, you can be more confident that your conclusions are robust to the choice of estimator.\n\nMethod agreement on central structure is the norm for well-specified models. For a dataset like Orange, where the model is a reasonable description of the data-generating process, Laplace, MCEM, and SAEM will typically recover similar point estimates and trajectory shapes. Disagreement is a useful diagnostic signal – it may indicate model misspecification, insufficient iterations, or a challenging likelihood surface.\n\nMCMC provides the richest uncertainty characterization, while VI provides a faster approximate posterior alternative. Posterior predictive bands and marginal posterior distributions from MCMC capture asymmetry, multimodality, and correlation structure most faithfully. VI can be substantially faster and often gives useful posterior summaries, but remains an approximation whose quality should be checked against predictive diagnostics and, when feasible, MCMC.\n\nChoose your method based on your inferential goal. If you need fast point estimates with approximate standard errors for model selection or screening, Laplace is often sufficient. If you need robust estimates under flexible random-effect distributions, SAEM or MCEM may be preferable. If full posterior inference is the goal – for example, for decision-making under uncertainty or for propagating parameter uncertainty into downstream predictions – MCMC is the strongest choice. If you need Bayesian posterior-style outputs with lower runtime, VI is a practical middle ground.","category":"section"},{"location":"#NoLimits.jl","page":"Home","title":"NoLimits.jl","text":"NoLimits.jl is a Julia package for nonlinear mixed-effects (NLME) modeling of longitudinal data. It provides a unified framework for specifying, estimating, and diagnosing hierarchical models that arise across the life sciences, including ecology, neuroscience, epidemiology, pharmacology, and beyond.","category":"section"},{"location":"#Why-NoLimits.jl?","page":"Home","title":"Why NoLimits.jl?","text":"Longitudinal studies – where repeated measurements are collected from multiple individuals over time – are ubiquitous in biomedical and natural sciences research. Analyzing such data requires models that capture both the underlying process dynamics and the variability across individuals. Nonlinear mixed-effects models provide a principled statistical framework for this, but existing software often forces users to choose between model expressiveness, estimation flexibility, and modern machine-learning integration.\n\nNoLimits.jl removes these trade-offs. It supports:\n\nDiverse structural models. Classical nonlinear functions, mechanistic ODE systems, and hidden Markov outcome models can be combined within a single specification.\nFlexible estimation. The same model can be fitted using frequentist maximum-likelihood methods (Laplace approximation, MCEM, SAEM), full Bayesian MCMC sampling, or variational inference (VI), enabling comparison across inferential paradigms.\nMachine-learning integration. Neural-network components – including neural-ODE constructions – and soft decision trees can be embedded alongside known mechanistic terms. This allows models to retain established scientific structure while learning unknown nonlinear behavior from data.\nRich hierarchical variability. Random-effect distributions are not restricted to Gaussian forms; heavy-tailed, skewed, and normalizing-flow-based distributions are supported. These distributions can themselves be parameterized by covariates and learned functions.\nComposability. Multiple outcomes, multiple grouping structures (e.g., subject-level and site-level), covariates at different temporal resolutions, and learned components can all coexist in one coherent model definition.\n\nFixed-effects-only workflows are also supported for problems where random effects are not required.","category":"section"},{"location":"#Getting-Started","page":"Home","title":"Getting Started","text":"New users should begin with the Installation page, then work through the Tutorials for hands-on examples covering fixed-effects models, mixed-effects estimation with multiple methods, ODE-based models, and machine-learning-augmented dynamics.\n\nFor a concise overview of what the package can do, see Capabilities. For the mathematical foundations, see NLME Methodology.","category":"section"},{"location":"#How-to-Cite","page":"Home","title":"How to Cite","text":"A manuscript describing NoLimits.jl is in preparation. In the meantime, please cite this repository:\n\n@software{NoLimits_jl_2026,\n  title  = {{NoLimits.jl}},\n  author = {Huth, Manuel and Arruda, Jonas and Peiter, Clemens and Schmid, Nina and Hasenauer, Jan},\n  year   = {2026},\n  url    = {https://github.com/manuhuth/NoLimits.jl}\n}","category":"section"},{"location":"uncertainty-quantification/#Uncertainty-Quantification","page":"Overview","title":"Uncertainty Quantification","text":"Point estimates alone are insufficient for reliable scientific conclusions. Uncertainty quantification (UQ) provides confidence intervals and variance-covariance information that characterize the precision of estimated parameters, enabling principled model comparison and decision-making. Reporting parameter uncertainty is standard practice in the biological sciences and is typically required for publication.\n\nNoLimits.jl offers a unified UQ interface through compute_uq, which accepts a fitted model result (FitResult) and returns a UQResult. The same interface supports multiple uncertainty backends – Wald-based approximations, profile-likelihood intervals, posterior-chain intervals, and MCMC refit workflows – so that the choice of UQ method can be varied independently of the estimation method.","category":"section"},{"location":"uncertainty-quantification/#Quick-Start","page":"Overview","title":"Quick Start","text":"using NoLimits\n\nuq = compute_uq(res)  # method=:auto\n\nWhen method=:auto is used, the backend is selected automatically based on the source fit:\n\nIf res comes from MCMC or VI, the backend is :chain.\nIf interval=:profile is explicitly requested, the backend is :profile.\nOtherwise, the backend defaults to :wald.","category":"section"},{"location":"uncertainty-quantification/#Backends","page":"Overview","title":"Backends","text":"The table below summarizes the available UQ backends, their associated estimation methods, and the type of output each produces.\n\nBackend method Typical source fit Output style\nWald :wald MLE, MAP, Laplace, LaplaceMAP, MCEM, SAEM covariance + Gaussian-draw intervals\nChain :chain MCMC, VI posterior-draw intervals\nProfile likelihood :profile MLE, MAP, Laplace, LaplaceMAP profile intervals\nMCMC refit :mcmc_refit non-MCMC fits posterior-draw intervals from refit","category":"section"},{"location":"uncertainty-quantification/#Wald-(method:wald)","page":"Overview","title":"Wald (method=:wald)","text":"The Wald backend computes an approximate variance-covariance matrix for selected fixed-effect coordinates, then draws from a Gaussian approximation to construct confidence intervals. This is the most computationally efficient approach and works well for well-identified models whose log-likelihood surface is approximately quadratic near the optimum.\n\nuq_wald = compute_uq(\n    res;\n    method=:wald,\n    vcov=:hessian,\n    level=0.95,\n    n_draws=2000,\n)\n\nKey options:\n\nvcov: :hessian or :sandwich\npseudo_inverse: use pseudo-inverse if direct inversion is unstable\nhessian_backend: :auto, :forwarddiff, or :fd_gradient\nfinite-difference controls: fd_abs_step, fd_rel_step, fd_max_tries\n\nFor MCEM and SAEM source fits, the Wald backend also accepts random-effects approximation controls via re_approx and re_approx_method.","category":"section"},{"location":"uncertainty-quantification/#Chain-(method:chain)","page":"Overview","title":"Chain (method=:chain)","text":"The chain backend extracts posterior samples from an existing MCMC fit or draws from an existing VI variational posterior and computes equal-tail credible intervals directly from those samples.\n\nif @isdefined(res_mcmc) && res_mcmc !== nothing\n    uq_chain = compute_uq(\n        res_mcmc;\n        method=:chain,\n        level=0.95,\n        mcmc_warmup=200,\n        mcmc_draws=1000,\n    )\nend\n\nKey options:\n\nmcmc_warmup: number of warmup/adaptation iterations to discard\nmcmc_draws: number of retained draws used for interval construction","category":"section"},{"location":"uncertainty-quantification/#Profile-(method:profile)","page":"Overview","title":"Profile (method=:profile)","text":"The profile-likelihood backend computes likelihood-based confidence intervals by profiling one parameter at a time around the fitted optimum. Unlike the Wald approach, profile intervals do not assume a Gaussian posterior shape and can capture asymmetric uncertainty – a common situation for variance parameters or parameters near constraints.\n\nuq_profile = compute_uq(\n    res;\n    method=:profile,\n    level=0.95,\n    profile_method=:LIN_EXTRAPOL,\n    profile_scan_width=3.0,\n    profile_scan_tol=1e-3,\n    profile_loss_tol=1e-3,\n)\n\nKey options:\n\nprofile_method\nprofile_scan_width, profile_scan_tol\nprofile_loss_tol\nprofile_local_alg, profile_max_iter, profile_ftol_abs\nprofile_kwargs","category":"section"},{"location":"uncertainty-quantification/#MCMC-Refit-(method:mcmc_refit)","page":"Overview","title":"MCMC Refit (method=:mcmc_refit)","text":"The MCMC refit backend is designed for cases where the original fit was obtained by an optimization-based method (such as MLE or Laplace), but fully Bayesian uncertainty quantification is desired. It launches a new MCMC sampling run initialized from the fitted parameter values and reports chain-based intervals from the resulting posterior.\n\nuq_refit = compute_uq(\n    res;\n    method=:mcmc_refit,\n    level=0.95,\n    mcmc_draws=1000,\n)\n\nImportant behavior:\n\nmethod=:mcmc_refit is intended for non-MCMC source fits.\nAll sampled fixed effects must have priors specified.\nconstants and constants_re can be used to hold selected parameters fixed during the refit.","category":"section"},{"location":"uncertainty-quantification/#Returned-Object-and-Accessors","page":"Overview","title":"Returned Object and Accessors","text":"compute_uq returns a UQResult object. The following accessor functions provide a consistent interface across all backends.\n\nbackend = get_uq_backend(uq)\nsource_method = get_uq_source_method(uq)\nparam_names = get_uq_parameter_names(uq)\n\nest_nat = get_uq_estimates(uq; scale=:natural)\nest_tr = get_uq_estimates(uq; scale=:transformed)\n\nints_nat = get_uq_intervals(uq; scale=:natural)\nints_tr = get_uq_intervals(uq; scale=:transformed)\n\nV_nat = get_uq_vcov(uq; scale=:natural)\nV_tr = get_uq_vcov(uq; scale=:transformed)\n\ndraws_nat = get_uq_draws(uq; scale=:natural)\ndraws_tr = get_uq_draws(uq; scale=:transformed)\n\ndiag = get_uq_diagnostics(uq)\n\nNote that not all quantities are available from every backend:\n\n:wald and :chain provide covariance matrices and draws.\n:profile provides intervals but does not return covariance or draw matrices.\n:mcmc_refit returns chain-based quantities from the refit.","category":"section"},{"location":"uncertainty-quantification/#UQ-Summaries","page":"Overview","title":"UQ Summaries","text":"For convenient inspection, NoLimits.summarize produces formatted summaries. A standalone UQ summary is obtained as follows.\n\nuq_summary = NoLimits.summarize(uq)\nuq_summary\n\nTo combine fit results and uncertainty information into a single summary, pass both objects.\n\nfit_uq_summary = NoLimits.summarize(res, uq)\nfit_uq_summary","category":"section"},{"location":"uncertainty-quantification/#Parameter-Inclusion-Rules","page":"Overview","title":"Parameter Inclusion Rules","text":"UQ is computed on the subset of free fixed-effect coordinates that are eligible for uncertainty calculation. Eligibility is controlled at parameter definition time through the calculate_se argument in @fixedEffects constructors (e.g., RealNumber, RealVector, RealPSDMatrix, NNParameters, SoftTreeParameters).\n\nBy default, calculate_se is false for NNParameters, SoftTreeParameters, and SplineParameters, because standard errors for high-dimensional parameter vectors are rarely scientifically informative. Classical fixed-effect types such as RealNumber and RealVector are included by default unless explicitly excluded with calculate_se=false.\n\nmodel = @Model begin\n    @fixedEffects begin\n        ka = RealNumber(1.0, scale=:log, calculate_se=true)   # included in UQ\n        ke = RealNumber(0.5, scale=:log, calculate_se=true)   # included in UQ\n        sigma = RealNumber(0.2, scale=:log, calculate_se=false) # excluded from UQ\n    end\n\n    @formulas begin\n        y ~ Distributions.Normal(ka + ke, sigma)\n    end\nend\n\nA coordinate is excluded from UQ if:\n\nits parent fixed effect is held constant via the constants argument, or\nits parameter block has calculate_se=false.\n\nFor mixed-effects fits, constants_re can be passed to hold selected random-effect levels fixed during UQ computations that involve random-effect approximations.","category":"section"},{"location":"uncertainty-quantification/#Practical-Notes","page":"Overview","title":"Practical Notes","text":"compute_uq requires access to the original DataModel. This is available when the fit was run with store_data_model=true (the default).\nThe level argument must satisfy 0 < level < 1.\nFor methods that rely on random draws (:wald, :chain, :mcmc_refit), pass an rng argument to ensure reproducibility.","category":"section"},{"location":"model-building/helpers/#@helpers","page":"@helpers","title":"@helpers","text":"The @helpers block defines reusable helper functions within the @Model DSL. These are useful when the same nonlinear transformation appears in multiple places – for example, a saturation function used in both the structural model and the observation model.\n\nHelper functions are parsed at macro-expansion time and returned as a NamedTuple of callables.","category":"section"},{"location":"model-building/helpers/#Syntax","page":"@helpers","title":"Syntax","text":"Both short-form and long-form function definitions are supported:\n\nhelpers = @helpers begin\n    sat(u) = u / (1 + abs(u))\n\n    function softplus(u::Float64)\n        return log1p(exp(u))\n    end\nend","category":"section"},{"location":"model-building/helpers/#Validation-Rules","page":"@helpers","title":"Validation Rules","text":"Only function definitions are allowed inside @helpers.\nHelper arguments must be simple symbols, optionally typed (e.g., x or x::Float64).\nDuplicate helper names within the same block are rejected.\nAn empty block is valid and yields NamedTuple().\nMutating helper patterns (e.g., in-place array operations) trigger a warning, since reverse-mode automatic differentiation backends such as Zygote require non-mutating code.","category":"section"},{"location":"model-building/helpers/#Example:-Standalone-Helper-Block","page":"@helpers","title":"Example: Standalone Helper Block","text":"using NoLimits\nusing LinearAlgebra\n\nhelpers = @helpers begin\n    clamp01(u) = max(0.0, min(1.0, u))\n    softplus(u) = log1p(exp(u))\n    dotp(a, b) = dot(a, b)\nend\n\nhelpers.clamp01(-1.0)   # 0.0\nhelpers.softplus(0.0)   # log(2)\nhelpers.dotp([1.0, 2.0], [3.0, 4.0])  # 11.0","category":"section"},{"location":"model-building/helpers/#Example:-Helper-in-Formulas","page":"@helpers","title":"Example: Helper in Formulas","text":"using NoLimits\nusing Distributions\n\nmodel = @Model begin\n    @helpers begin\n        sat(u) = u / (1 + abs(u))\n    end\n\n    @fixedEffects begin\n        a = RealNumber(1.0)\n        σ = RealNumber(0.5, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n        x = ConstantCovariateVector([:Age])\n    end\n\n    @randomEffects begin\n        η = RandomEffect(TDist(6.0); column=:id)\n    end\n\n    @formulas begin\n        μ = sat(a + η^2 + x.Age)\n        y ~ Laplace(μ, σ)\n    end\nend","category":"section"},{"location":"model-building/helpers/#Example:-Helper-in-Random-Effect-Distribution","page":"@helpers","title":"Example: Helper in Random-Effect Distribution","text":"Helpers can also appear inside random-effect distribution expressions, enabling nonlinear transformations of fixed effects before they enter the distribution:\n\nusing NoLimits\nusing Distributions\n\nmodel = @Model begin\n    @helpers begin\n        softplus(u) = log1p(exp(u))\n    end\n\n    @fixedEffects begin\n        β = RealNumber(0.2)\n        ση = RealNumber(0.7, scale=:log)\n        σy = RealNumber(0.5, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n    end\n\n    @randomEffects begin\n        η = RandomEffect(LogNormal(softplus(β), ση); column=:ID)\n    end\n\n    @formulas begin\n        y ~ Gamma(log1p(η^2) + 1e-6, σy)\n    end\nend","category":"section"},{"location":"model-building/helpers/#Note-on-Automatic-Differentiation","page":"@helpers","title":"Note on Automatic Differentiation","text":"Helpers that mutate arrays in place are detected and produce a warning. When reverse-mode AD compatibility is needed (e.g., with Zygote), prefer non-mutating implementations.","category":"section"},{"location":"estimation/multistart/#Multistart","page":"Multistart","title":"Multistart","text":"Nonlinear models frequently have multiple local optima, making the estimated parameters sensitive to initialization. The Multistart wrapper addresses this by running multiple fits from different initial parameter values and selecting the best result. This strategy is especially important for complex models where a single optimization run provides limited confidence that the global optimum has been found.\n\nMultistart operates as a method-agnostic wrapper around fit_model: it samples many candidate starting points, screens them with a cheap log-likelihood evaluation to identify the most promising ones, then fully optimizes only the top candidates. Because each optimization is independent, the runs can be executed in parallel.\n\nThe call pattern is:\n\nres_ms = fit_model(ms, dm, method; kwargs...)\n\nwhere ms is a NoLimits.Multistart(...) object and method is any fitting method.","category":"section"},{"location":"estimation/multistart/#Supported-Methods","page":"Multistart","title":"Supported Methods","text":"Multistart wraps any FittingMethod and has been tested with:\n\nMLE\nMAP\nLaplace\nLaplaceMAP\nFOCEI\nFOCEIMAP\nMCEM\nSAEM\nVI (see note below)\nMCMC (supported, but usually not recommended as a primary restart strategy)","category":"section"},{"location":"estimation/multistart/#Recommendation","page":"Multistart","title":"Recommendation","text":"Multistart is most beneficial for optimization- and EM-based methods (MLE, MAP, Laplace, LaplaceMAP, FOCEI, FOCEIMAP, MCEM, SAEM, VI), where the choice of starting values strongly influences which local optimum is found.\n\nFor MCMC, multistart is technically supported but generally not recommended as the primary strategy. In most Bayesian workflows, tuning sampler settings and chain diagnostics is more effective than varying initial values across restarts.","category":"section"},{"location":"estimation/multistart/#Constructor","page":"Multistart","title":"Constructor","text":"using NoLimits\nusing Random\nusing SciMLBase\n\nms = NoLimits.Multistart(;\n    dists=NamedTuple(),\n    n_draws_requested=100,\n    n_draws_used=50,\n    sampling=:random,               # :random or :lhs\n    serialization=EnsembleSerial(), # controls parallelism across starts\n    rng=Random.default_rng(),\n)","category":"section"},{"location":"estimation/multistart/#Two-Phase-Workflow","page":"Multistart","title":"Two-Phase Workflow","text":"Multistart uses a two-phase approach to avoid running full optimizations from all sampled candidates:\n\nPhase 1 — Screening. All n_draws_requested candidates are evaluated cheaply by computing the marginal log-likelihood at η = 0 (no random-effects perturbation) using a pre-compiled ODE and covariate cache. Candidates are ranked by this screening log-likelihood and the top n_draws_used are selected via a partial sort — no full optimization is performed during this phase.\n\nPhase 2 — Optimization. The selected n_draws_used candidates are passed to the wrapped fitting method as independent starting points. Each run produces a full FitResult. The run with the best final objective is returned as the best result.\n\nIf n_draws_requested == n_draws_used, Phase 1 is skipped entirely — no screening cache is built and all candidates proceed directly to optimization.","category":"section"},{"location":"estimation/multistart/#Progress-Logging","page":"Multistart","title":"Progress Logging","text":"At the start of each fit_model call, a summary is logged:\n\n┌ Info: Multistart\n│   candidates = 20\n│   selected = 5\n│   varying = \"a, σ\"\n│   best_screening_ll = -12.4\n└   worst_screening_ll = -18.7\n\ncandidates — total starting points generated (= n_draws_requested, after any automatic adjustment).\nselected — candidates forwarded to full optimization (= n_draws_used).\nvarying — names of parameters whose values differ across starts (those with a prior or an entry in dists).\nbest_screening_ll / worst_screening_ll — log-likelihood range of the selected candidates at η = 0. Omitted when screening is skipped (candidates == selected) or when all candidates produce non-finite likelihoods.","category":"section"},{"location":"estimation/multistart/#Screening-and-Method-Direction","page":"Multistart","title":"Screening and Method Direction","text":"Screening always ranks candidates by the marginal log-likelihood (higher is better). This is the correct direction for every supported method:\n\nMLE / MAP / Laplace / FOCEI / MCEM / SAEM / VI — all internally minimize the negative log-likelihood (or a penalized variant). Selecting candidates with the highest screening LL puts the optimizer in the most promising region.\nMCMC — though MCMC does not optimize, starting from a high-likelihood region improves early mixing and reduces warm-up cost.\n\nNo sign adjustment is made per-method; the screening criterion is uniform.","category":"section"},{"location":"estimation/multistart/#How-Starts-Are-Built","page":"Multistart","title":"How Starts Are Built","text":"Starting points are constructed as follows:\n\nStart 1 is always the model's default initial fixed-effect values (obtained via get_θ0_untransformed).\nSubsequent starts are sampled independently for each parameter, drawing from:\nthe distribution specified in ms.dists for that parameter name, if provided;\nthe fixed-effect prior, if one is defined;\notherwise, the parameter retains its default value across all starts.\n\nAll sampled values are validated against the natural-scale bounds of each parameter. If any sampled value violates its bounds, an error is raised before fitting begins.","category":"section"},{"location":"estimation/multistart/#Distribution-Inputs","page":"Multistart","title":"Distribution Inputs","text":"The dists argument is a NamedTuple keyed by fixed-effect names. The expected distribution type depends on the parameter structure:\n\nScalar parameters: a univariate distribution (e.g., Normal(...)).\nVector or matrix parameters: either a single multivariate/matrix-variate distribution, or an array of element-wise univariate distributions.\n\nFor square-matrix parameters, Multistart symmetrizes the sampled matrix and applies a small diagonal perturbation if needed to ensure numerical stability.","category":"section"},{"location":"estimation/multistart/#Sampling-Modes","page":"Multistart","title":"Sampling Modes","text":"Two sampling strategies are available:\n\nsampling=:random – Draws are taken directly from the specified distributions.\nsampling=:lhs – Latin Hypercube Sampling is used when the distribution supports quantile-based inversion, producing more uniform coverage of the parameter space. For distributions where a direct LHS quantile path is unavailable, the method falls back to random draws.","category":"section"},{"location":"estimation/multistart/#Requested-vs.-Used-Draws","page":"Multistart","title":"Requested vs. Used Draws","text":"The n_draws_requested and n_draws_used parameters control the two phases:\n\nParameter Phase Effect\nn_draws_requested Screening Total candidates sampled. More candidates give better coverage but increase screening cost.\nn_draws_used Optimization Candidates forwarded to full optimization. Larger values improve coverage but increase runtime proportionally.\n\nIf n_draws_used exceeds n_draws_requested, the number of requested draws is automatically increased to match and a warning is emitted.\n\nSetting n_draws_requested == n_draws_used disables screening: all candidates proceed directly to optimization without any pre-evaluation.","category":"section"},{"location":"estimation/multistart/#Scoring-and-Best-Run-Selection","page":"Multistart","title":"Scoring and Best-Run Selection","text":"After all fits complete, successful runs are ranked by the following scoring rule:\n\nThe objective value from get_objective, if finite.\nOtherwise, the negative log-likelihood from -get_loglikelihood, if finite.\nOtherwise, Inf (effectively deprioritizing the run).\n\nThe run with the lowest score is selected as the best result:\n\nbest = get_multistart_best(res_ms)\nbest_idx = get_multistart_best_index(res_ms)\n\nIf all runs fail, Multistart raises an error reporting the first recorded failure.","category":"section"},{"location":"estimation/multistart/#Parallelism-and-RNG-Behavior","page":"Multistart","title":"Parallelism and RNG Behavior","text":"The ms.serialization field controls execution of the individual fits:\n\nEnsembleSerial(): Fits run sequentially in a single thread.\nEnsembleThreads(): Fits run in parallel across available threads.\n\nNote that screening (Phase 1) always runs serially regardless of serialization; only Phase 2 (the full optimizations) is parallelized.\n\nRandom number generator behavior depends on how rng is supplied:\n\nIf rng is not passed to fit_model(ms, ...), each start receives an internally spawned child RNG to ensure independence.\nIf rng is explicitly provided in the fit keywords, that generator is forwarded to every underlying fit call.","category":"section"},{"location":"estimation/multistart/#Fit-Keyword-Forwarding","page":"Multistart","title":"Fit Keyword Forwarding","text":"All fit keywords are forwarded to the wrapped method, with one exception:\n\ntheta_0_untransformed is ignored (with a warning), because Multistart manages starting points internally.\n\nAll other keywords – such as constants, constants_re, ode_args, ode_kwargs, serialization, and store_data_model – are passed through unchanged.","category":"section"},{"location":"estimation/multistart/#Multistart-Result-Accessors","page":"Multistart","title":"Multistart Result Accessors","text":"The MultistartFitResult provides detailed access to both successful and failed runs:\n\nok_runs    = get_multistart_results(res_ms)\nok_starts  = get_multistart_starts(res_ms)\n\nfailed_runs   = get_multistart_failed_results(res_ms)\nfailed_starts = get_multistart_failed_starts(res_ms)\nfailed_errors = get_multistart_errors(res_ms)\n\nbest_run = get_multistart_best(res_ms)\nbest_idx = get_multistart_best_index(res_ms)\n\nStandard fit accessors also work directly on a MultistartFitResult, dispatching to the best run:\n\ntheta_best = get_params(res_ms; scale=:untransformed)\nobj_best   = get_objective(res_ms)","category":"section"},{"location":"estimation/multistart/#Example:-Fixed-Effects-MLE-with-Screening","page":"Multistart","title":"Example: Fixed-Effects MLE with Screening","text":"The following example generates 20 candidates via LHS, screens them to the top 5 by log-likelihood, and runs 5 full optimizations:\n\nusing NoLimits\nusing DataFrames\nusing Distributions\n\nmodel = @Model begin\n    @covariates begin\n        t = Covariate()\n    end\n\n    @fixedEffects begin\n        a = RealNumber(0.2)\n        sigma = RealNumber(0.5, scale=:log)\n    end\n\n    @formulas begin\n        y ~ Laplace(a, sigma)\n    end\nend\n\ndf = DataFrame(\n    ID    = [:A, :A, :B, :B],\n    t     = [0.0, 1.0, 0.0, 1.0],\n    y     = [0.1, 0.2, 0.0, -0.1],\n)\n\ndm = DataModel(model, df; primary_id=:ID, time_col=:t)\n\nms = NoLimits.Multistart(;\n    dists             = (; a=Normal(0.0, 1.0)),\n    n_draws_requested = 20,\n    n_draws_used      = 5,\n    sampling          = :lhs,\n)\n\nres_ms = fit_model(ms, dm, NoLimits.MLE(; optim_kwargs=(maxiters=80,)))\n\nbest      = get_multistart_best(res_ms)\ntheta_best = get_params(res_ms; scale=:untransformed)\n\nThe logged output will look similar to:\n\n┌ Info: Multistart\n│   candidates = 20\n│   selected = 5\n│   varying = \"a\"\n│   best_screening_ll = -2.1\n└   worst_screening_ll = -8.4","category":"section"},{"location":"estimation/multistart/#Example:-Variational-Inference-with-Multistart","page":"Multistart","title":"Example: Variational Inference with Multistart","text":"VI benefits from multistart when the ELBO landscape is multimodal. The usage is identical to any other method:\n\nusing NoLimits\nusing DataFrames\nusing Distributions\nusing Random\n\nmodel = @Model begin\n    @covariates begin\n        t = Covariate()\n    end\n\n    @fixedEffects begin\n        a     = RealNumber(0.0, prior=Normal(0.0, 1.0))\n        sigma = RealNumber(0.5, scale=:log, prior=LogNormal(0.0, 0.5))\n    end\n\n    @formulas begin\n        y ~ Normal(a, sigma)\n    end\nend\n\ndf = DataFrame(\n    ID = [:A, :A, :B, :B],\n    t  = [0.0, 1.0, 0.0, 1.0],\n    y  = [0.1, 0.2, 0.0, -0.1],\n)\n\ndm = DataModel(model, df; primary_id=:ID, time_col=:t)\n\nms = NoLimits.Multistart(;\n    n_draws_requested = 10,\n    n_draws_used      = 3,\n    sampling          = :lhs,\n    rng               = Random.Xoshiro(42),\n)\n\nres_ms = fit_model(\n    ms, dm,\n    NoLimits.VI(; turing_kwargs=(max_iter=300, family=:meanfield, progress=false)),\n    rng=Random.Xoshiro(1),\n)\n\nposterior = get_variational_posterior(res_ms)\nobjective = get_objective(res_ms)   # final ELBO of the best run","category":"section"},{"location":"estimation/multistart/#Optional:-MCMC-with-Multistart-(Supported,-Usually-Not-Recommended)","page":"Multistart","title":"Optional: MCMC with Multistart (Supported, Usually Not Recommended)","text":"While multistart is primarily designed for optimization-based methods, it can be used with MCMC when a restart-style sampling workflow is explicitly desired:\n\nusing NoLimits\nusing DataFrames\nusing Distributions\nusing Turing\n\nmodel = @Model begin\n    @covariates begin\n        t = Covariate()\n    end\n\n    @fixedEffects begin\n        a     = RealNumber(0.2, prior=Normal(0.0, 1.0))\n        sigma = RealNumber(0.5, scale=:log, prior=LogNormal(0.0, 0.2))\n    end\n\n    @formulas begin\n        y ~ LogNormal(a, sigma)\n    end\nend\n\ndf = DataFrame(\n    ID = [:A, :A, :B, :B],\n    t  = [0.0, 1.0, 0.0, 1.0],\n    y  = [1.0, 1.1, 0.9, 1.0],\n)\n\ndm = DataModel(model, df; primary_id=:ID, time_col=:t)\n\nms = NoLimits.Multistart(; n_draws_requested=6, n_draws_used=3)\n\nres_ms = fit_model(\n    ms, dm,\n    NoLimits.MCMC(; sampler=MH(), turing_kwargs=(n_samples=200, n_adapt=0, progress=false)),\n)\n\nchain_best = get_chain(res_ms)\n\nUse this pattern only when a restart-style MCMC workflow is explicitly needed.","category":"section"},{"location":"model-building/pre-differential-equation/#@preDifferentialEquation","page":"@preDifferentialEquation","title":"@preDifferentialEquation","text":"In many longitudinal models, certain quantities depend on fixed effects, random effects, and covariates but do not vary with time. Computing these derived quantities once before ODE integration – rather than redundantly at each solver step – improves both clarity and performance.\n\nThe @preDifferentialEquation block defines such time-constant derived quantities. Values declared here are available in @DifferentialEquation, @initialDE, and @formulas.","category":"section"},{"location":"model-building/pre-differential-equation/#Core-Syntax","page":"@preDifferentialEquation","title":"Core Syntax","text":"The block accepts only assignment statements. Each left-hand side must be a plain symbol, and expressions may reference any in-scope model quantities except time (t and \\xi are forbidden).\n\nprede = @preDifferentialEquation begin\n    ka = exp(tka + eta[1])\n    cl = exp(tcl + eta[2])\n    v = exp(tv + eta[3])\nend","category":"section"},{"location":"model-building/pre-differential-equation/#Parsing-and-Validation-Rules","page":"@preDifferentialEquation","title":"Parsing and Validation Rules","text":"The following constraints are enforced at parse time:\n\nThe block must be wrapped in begin ... end.\nOnly assignments are allowed; other statement forms are rejected.\nLeft-hand side must be a symbol.\nThe time symbols t and \\xi are forbidden in expressions, since preDE values must be time-invariant.\nMutating patterns trigger a warning, as they may break automatic differentiation.","category":"section"},{"location":"model-building/pre-differential-equation/#Symbol-Resolution","page":"@preDifferentialEquation","title":"Symbol Resolution","text":"@preDifferentialEquation expressions can reference the following model components:\n\nFixed effects\nRandom effects\nConstant covariates (constant_features_i)\nHelper functions declared in @helpers\nModel functions produced by learned parameter blocks (NNParameters, SoftTreeParameters, SplineParameters, NPFParameter)","category":"section"},{"location":"model-building/pre-differential-equation/#Example:-Nonlinear-Transform-of-Multivariate-Random-Effects","page":"@preDifferentialEquation","title":"Example: Nonlinear Transform of Multivariate Random Effects","text":"A common pattern is to define individual-level parameters as log-linear functions of population means and random effects. Here, three rate and volume parameters are derived from fixed effects and a multivariate random effect vector before being passed into the ODE system.\n\nusing NoLimits\nusing Distributions\nusing LinearAlgebra\n\nmodel = @Model begin\n    @fixedEffects begin\n        tka = RealNumber(0.45)\n        tcl = RealNumber(1.0)\n        tv = RealNumber(3.45)\n        omega1 = RealNumber(1.0, scale=:log)\n        omega2 = RealNumber(1.0, scale=:log)\n        omega3 = RealNumber(1.0, scale=:log)\n        sigma = RealNumber(0.3, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n    end\n\n    @randomEffects begin\n        eta = RandomEffect(MvNormal([0.0, 0.0, 0.0], Diagonal([omega1, omega2, omega3])); column=:ID)\n    end\n\n    @preDifferentialEquation begin\n        ka = exp(tka + eta[1])\n        cl = exp(tcl + eta[2])\n        v = exp(tv + eta[3])\n    end\n\n    @DifferentialEquation begin\n        D(depot) ~ -ka * depot\n        D(center) ~ ka * depot - cl / v * center\n    end\n\n    @initialDE begin\n        depot = 0.0\n        center = 0.0\n    end\n\n    @formulas begin\n        y ~ Erlang(3, log1p((center(t) / v)^2) + sigma)\n    end\nend","category":"section"},{"location":"model-building/pre-differential-equation/#Example:-Using-Helpers-and-Learned-Model-Functions","page":"@preDifferentialEquation","title":"Example: Using Helpers and Learned Model Functions","text":"PreDE expressions can also incorporate neural networks, soft decision trees, and spline functions. This is useful when individual-level baseline quantities are modeled as flexible, learned functions of covariates.\n\nusing NoLimits\nusing Lux\nusing Distributions\n\nchain = Chain(Dense(2, 3, tanh), Dense(3, 1))\nknots = collect(range(0.0, 1.0; length=6))\n\nmodel = @Model begin\n    @helpers begin\n        sat(u) = u / (1 + abs(u))\n    end\n\n    @fixedEffects begin\n        ζ = NNParameters(chain; function_name=:NNB, calculate_se=false)\n        Γ = SoftTreeParameters(2, 2; function_name=:STB, calculate_se=false)\n        sp = SplineParameters(knots; function_name=:SPB, calculate_se=false)\n        sy = RealNumber(0.4, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n        x = ConstantCovariateVector([:Age, :BMI]; constant_on=:ID)\n    end\n\n    @preDifferentialEquation begin\n        nn = NNB([x.Age, x.BMI], ζ)[1]\n        st = STB([x.Age, x.BMI], Γ)[1]\n        spv = SPB(0.4, sp)\n        drive = sat(nn + st + spv)\n    end\n\n    @formulas begin\n        y ~ Gamma(log1p(drive^2) + 1e-6, sy)\n    end\nend","category":"section"},{"location":"model-building/pre-differential-equation/#Accessors","page":"@preDifferentialEquation","title":"Accessors","text":"The following functions provide programmatic access to preDE internals:\n\nget_prede_names(prede) – returns the declared preDE variable names.\nget_prede_syms(prede) – returns the parsed symbol dependencies for each variable.\nget_prede_builder(prede) – returns the evaluation function that computes preDE values from model inputs.","category":"section"},{"location":"model-building/pre-differential-equation/#Data-Model-Constraint-for-Random-Effects-in-preDE","page":"@preDifferentialEquation","title":"Data-Model Constraint for Random Effects in preDE","text":"Because preDE values are computed once per individual per model evaluation, any random effects referenced in this block must be grouped by primary_id in the DataModel. Random effects grouped by a different column (e.g., a site-level grouping) are not permitted in preDE expressions, since their values would not be uniquely determined at the individual level.","category":"section"},{"location":"model-building/random-effects/#@randomEffects","page":"@randomEffects","title":"@randomEffects","text":"Random effects capture between-group variability in nonlinear mixed-effects models, allowing individual-level (or site-level, cluster-level, etc.) deviations from population parameters. The @randomEffects block declares these latent variables, each linked to a grouping column that defines the hierarchical structure.","category":"section"},{"location":"model-building/random-effects/#Core-Syntax","page":"@randomEffects","title":"Core Syntax","text":"Each random effect is declared as an assignment. The right-hand side must be a RandomEffect(distribution; column=:GROUP_COL) call, where distribution is any univariate or multivariate distribution and column specifies the grouping variable in the data.\n\nname = RandomEffect(Normal(0.0, 1.0); column=:GROUP_COL)\n\nMultiple random effects with different distributions and grouping structures can be declared in a single block:\n\nre = @randomEffects begin\n    η_id = RandomEffect(TDist(6.0); column=:ID)\n    η_mv = RandomEffect(MvNormal([0.0, 0.0], [0.5 0.0; 0.0 0.8]); column=:ID)\n    η_site = RandomEffect(SkewNormal(0.0, 1.0, 0.8); column=:SITE)\nend","category":"section"},{"location":"model-building/random-effects/#Parsing-Rules","page":"@randomEffects","title":"Parsing Rules","text":"The following constraints are enforced at macro expansion time:\n\nThe block must be begin ... end.\nOnly assignments are allowed.\nThe left-hand side must be a symbol.\nThe right-hand side must be a RandomEffect(...) call.\nExactly one positional argument is required: the distribution expression.\nThe column keyword is required and must be a symbol.\nUnsupported keywords are rejected.\nThe symbols t and ξ (reserved for time) are forbidden inside distribution expressions.","category":"section"},{"location":"model-building/random-effects/#Available-Bindings-in-Distribution-Expressions","page":"@randomEffects","title":"Available Bindings in Distribution Expressions","text":"Distribution expressions are not limited to literal values. At construction time, NoLimits resolves the following symbols within each distribution expression:\n\nFixed effects declared in @fixedEffects\nConstant covariates declared in @covariates\nModel functions generated by parameter blocks (e.g., neural networks, soft trees, splines, normalizing flows)\nHelper functions declared in @helpers\n\nThis enables flexible, data-driven random-effect distributions whose parameters depend on covariates, learned representations, or user-defined transformations.","category":"section"},{"location":"model-building/random-effects/#Example:-Multiple-Grouping-Structures","page":"@randomEffects","title":"Example: Multiple Grouping Structures","text":"Models may include random effects at several hierarchical levels simultaneously. In the following example, subject-level effects (η_id, η_mv) are grouped by :ID, while a site-level effect (η_site) is grouped by :SITE.\n\nusing NoLimits\nusing Distributions\n\nmodel = @Model begin\n    @fixedEffects begin\n        s = RealNumber(0.3, scale=:log)\n        μ_re = RealVector([0.0, 0.0])\n        Ω_re = RealPSDMatrix([0.4 0.0; 0.0 0.7], scale=:cholesky)\n    end\n\n    @covariates begin\n        t = Covariate()\n    end\n\n    @randomEffects begin\n        η_id = RandomEffect(TDist(6.0); column=:ID)\n        η_mv = RandomEffect(MvNormal(μ_re, Ω_re); column=:ID)\n        η_site = RandomEffect(SkewNormal(0.0, 1.0, 0.7); column=:SITE)\n    end\n\n    @formulas begin\n        μ = tanh(η_id) + η_mv[1]^2 + tanh(η_mv[2]) + η_site^2\n        y ~ Laplace(μ, s)\n    end\nend","category":"section"},{"location":"model-building/random-effects/#Example:-Covariate-and-Function-Parameterized-Distributions","page":"@randomEffects","title":"Example: Covariate- and Function-Parameterized Distributions","text":"Random-effect distributions can be parameterized by neural networks, soft trees, or other learned functions of covariates. This is useful when the distribution of individual-level parameters is expected to vary systematically with observed characteristics.\n\nusing NoLimits\nusing Distributions\nusing Lux\n\nchain = Chain(Dense(2, 4, tanh), Dense(4, 1))\n\nmodel = @Model begin\n    @helpers begin\n        softplus(u) = log1p(exp(u))\n    end\n\n    @fixedEffects begin\n        ζη = NNParameters(chain; function_name=:NN1, calculate_se=false)\n        Γη = SoftTreeParameters(2, 2; function_name=:ST1, calculate_se=false)\n        sη = RealNumber(0.6, scale=:log)\n        sy = RealNumber(0.4, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n        x = ConstantCovariateVector([:Age, :BMI]; constant_on=:ID)\n    end\n\n    @randomEffects begin\n        η = RandomEffect(\n            LogNormal(\n                NN1([x.Age, x.BMI], ζη)[1] + ST1([x.Age, x.BMI], Γη)[1],\n                sη\n            );\n            column=:ID\n        )\n    end\n\n    @formulas begin\n        y ~ Gamma(softplus(η) + 1e-6, sy)\n    end\nend","category":"section"},{"location":"model-building/random-effects/#Example:-Normalizing-Flow-Random-Effects","page":"@randomEffects","title":"Example: Normalizing Flow Random Effects","text":"For applications where standard parametric distributions are insufficiently flexible, NoLimits supports normalizing planar flows as random-effect distributions. A NormalizingPlanarFlow(ψ) transforms a base distribution through a sequence of invertible mappings, with parameters ψ registered as fixed effects via NPFParameter.\n\nusing NoLimits\nusing Distributions\n\nmodel = @Model begin\n    @fixedEffects begin\n        ψ = NPFParameter(1, 3, seed=1, calculate_se=false)\n        sy = RealNumber(0.3, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n    end\n\n    @randomEffects begin\n        η_flow = RandomEffect(NormalizingPlanarFlow(ψ); column=:ID)\n    end\n\n    @formulas begin\n        y ~ Gamma(log1p(abs(η_flow)^2) + 1e-6, sy)\n    end\nend","category":"section"},{"location":"model-building/random-effects/#Metadata-and-Builder-Accessors","page":"@randomEffects","title":"Metadata and Builder Accessors","text":"The parsed random-effects object provides programmatic access to its components through the following accessor functions:\n\nAccessor Returns\nget_re_names Vector of random effect names\nget_re_groups NamedTuple mapping each effect to its grouping column\nget_re_types NamedTuple mapping each effect to its distribution type symbol\nget_re_syms NamedTuple mapping each effect to the symbols used in its distribution\nget_re_dist_exprs Distribution expressions (as parsed)\nget_create_random_effect_distribution Function that constructs distributions at runtime given fixed effects, covariates, model functions, and helpers\nget_re_logpdf Function computing the joint log-density of all random effects","category":"section"},{"location":"model-building/random-effects/#Grouping-and-Data-Requirements","page":"@randomEffects","title":"Grouping and Data Requirements","text":"The column keyword defines the hierarchical level at which each random effect varies. This grouping structure is used during data-model construction, batching, and estimation.\nWhen a model includes multiple grouping columns, any constant covariate used within a random-effect distribution must declare constant_on for the corresponding grouping column to ensure consistency.\nGrouping columns must be present in the data and may not have unique values per observation (which would render the random effects unidentifiable).","category":"section"},{"location":"tutorials/mixed-effects-nn-saem/#Mixed-Effects-Tutorial-3:-Neural-Differential-Equation-Components-(SAEM)","page":"Mixed-Effects Tutorial 3: Neural Differential-Equation Components (SAEM)","title":"Mixed-Effects Tutorial 3: Neural Differential-Equation Components (SAEM)","text":"In many scientific domains – from systems biology to chemical engineering to ecology – we understand the broad structure of a dynamical system (e.g., compartments connected by transfer rates) but lack precise knowledge of every rate law or interaction term. Neural ordinary differential equations (Neural ODEs) offer a principled way to address this gap: they embed small neural networks directly inside an ODE system, allowing the data to shape the functional forms that mechanistic reasoning alone cannot specify. Crucially, this approach preserves the interpretable compartmental structure while letting learned components capture the unknown nonlinearities.\n\nIn this tutorial, you will build a mixed-effects ODE model in which multiple neural-network components parameterize the ODE right-hand side, and fit it with the Stochastic Approximation Expectation-Maximization (SAEM) algorithm. By the end, you will have a working example of a hybrid mechanistic-neural model that accounts for between-subject variability through random effects on the network weights themselves.","category":"section"},{"location":"tutorials/mixed-effects-nn-saem/#Learning-Goals","page":"Mixed-Effects Tutorial 3: Neural Differential-Equation Components (SAEM)","title":"Learning Goals","text":"By the end of this tutorial, you will be able to:\n\nDeclare neural-network parameter blocks (NNParameters) and wire them into an ODE system using the @DifferentialEquation macro.\nCouple network weights to subject-level random effects via multivariate normal distributions, so that every individual in the dataset receives a personalized version of the dynamics.\nFit the model using SAEM with Gaussian-block closed-form updates, a strategy that improves stability when random-effect dimensions are high.\nVisualize and diagnose the fitted trajectories and observation distributions.","category":"section"},{"location":"tutorials/mixed-effects-nn-saem/#Step-1:-Data-Setup","page":"Mixed-Effects Tutorial 3: Neural Differential-Equation Components (SAEM)","title":"Step 1: Data Setup","text":"In this step, you will load and prepare the data. We use the classic Theophylline dataset, which records concentration-time profiles for 12 subjects after oral administration. Although this dataset originates from pharmacology, the underlying dynamics – a substance entering a depot compartment and transferring to a central compartment where it is observed and cleared – are a standard example of a two-compartment transfer system that arises across many fields, from tracer kinetics to nutrient cycling. You will reshape the data into a flat format where the initial amount d enters as a constant covariate.\n\nusing NoLimits\nusing CSV\nusing DataFrames\nusing Distributions\nusing Downloads\nusing Random\nusing LinearAlgebra\nusing OrdinaryDiffEq\nusing SciMLBase\nusing Lux\nusing Turing\n\ninclude(joinpath(@__DIR__, \"_data_loaders.jl\"))\n\nRandom.seed!(321)\n\ntheoph_df = load_theoph()\n\nfunction build_theoph_non_event_df(tbl::DataFrame)\n    df = DataFrame(\n        ID=Int.(tbl.Subject),\n        t=Float64.(tbl.Time),\n        y=Float64.(tbl.conc),\n        d=Float64.(tbl.Wt .* tbl.Dose),\n    )\n    sort!(df, [:ID, :t])\n    return df\nend\n\ndf = build_theoph_non_event_df(theoph_df)\nfirst(df, 10)","category":"section"},{"location":"tutorials/mixed-effects-nn-saem/#Step-2:-Define-the-Neural-ODE-Mixed-Effects-Model","page":"Mixed-Effects Tutorial 3: Neural Differential-Equation Components (SAEM)","title":"Step 2: Define the Neural ODE Mixed-Effects Model","text":"In this step, you will define the core model. The key idea is that instead of specifying closed-form rate functions (such as first-order kinetics), you let neural networks learn these functions directly from data. Each NNParameters block declares a small feedforward network whose flattened weights become part of the fixed-effects parameter vector. At evaluation time, a callable function (e.g., NNA1) reconstructs the network from its weight vector and evaluates it – so you can use it inside @DifferentialEquation just like any other function.\n\nThe ODE right-hand side uses four neural components arranged in a two-compartment transfer system:\n\nfA1(t) and fA2(t) govern the dynamics of the depot (input) compartment.\nfC1(t) and fC2(t) govern the dynamics of the central (observed) compartment.\n\nTo capture between-subject variability, each network's weight vector is paired with a subject-level random-effect vector (etaA1, etaA2, etaC1, etaC2) drawn from a MvNormal distribution centered on the population weights. This means every individual effectively receives their own personalized network: the population learns shared structure, while the random effects allow individual departures.\n\nusing NoLimits\nusing Distributions\nusing LinearAlgebra\nusing OrdinaryDiffEq\nusing Lux\n\nwidth_nn = 2\nchain_A1 = Lux.Chain(Lux.Dense(1, width_nn, tanh), Lux.Dense(width_nn, 1))\nchain_A2 = Lux.Chain(Lux.Dense(1, width_nn, tanh), Lux.Dense(width_nn, 1))\nchain_C1 = Lux.Chain(Lux.Dense(1, width_nn, tanh), Lux.Dense(width_nn, 1))\nchain_C2 = Lux.Chain(Lux.Dense(1, width_nn, tanh), Lux.Dense(width_nn, 1))\n\nmodel_raw = @Model begin\n    @helpers begin\n        softplus(u) = u > 20 ? u : log1p(exp(u))\n    end\n\n    @covariates begin\n        t = Covariate()\n        d = ConstantCovariate(constant_on=:ID)\n    end\n\n    @fixedEffects begin\n        sigma = RealNumber(1.0, scale=:log, prior=LogNormal(log(1.0), 0.5), calculate_se=true)\n\n        zA1 = NNParameters(chain_A1; function_name=:NNA1, calculate_se=false)\n        zA2 = NNParameters(chain_A2; function_name=:NNA2, calculate_se=false)\n        zC1 = NNParameters(chain_C1; function_name=:NNC1, calculate_se=false)\n        zC2 = NNParameters(chain_C2; function_name=:NNC2, calculate_se=false)\n    end\n\n    @randomEffects begin\n        etaA1 = RandomEffect(MvNormal(zA1, Diagonal(ones(length(zA1)))); column=:ID)\n        etaA2 = RandomEffect(MvNormal(zA2, Diagonal(ones(length(zA2)))); column=:ID)\n        etaC1 = RandomEffect(MvNormal(zC1, Diagonal(ones(length(zC1)))); column=:ID)\n        etaC2 = RandomEffect(MvNormal(zC2, Diagonal(ones(length(zC2)))); column=:ID)\n    end\n\n    @DifferentialEquation begin\n        a_A(t) = softplus(depot)\n        x_C(t) = softplus(center)\n\n        fA1(t) = softplus(NNA1([t / 24], etaA1)[1])\n        fA2(t) = softplus(NNA2([a_A(t)], etaA2)[1])\n        fC1(t) = -softplus(NNC1([x_C(t)], etaC1)[1])\n        fC2(t) = softplus(NNC2([t / 24], etaC2)[1])\n\n        D(depot) ~ -d * fA1(t) - fA2(t)\n        D(center) ~ d * fA1(t) + fA2(t) + fC1(t) + d * fC2(t)\n    end\n\n    @initialDE begin\n        depot = d\n        center = 0.0\n    end\n\n    @formulas begin\n        y ~ Normal(center(t), sigma)\n    end\nend\n\nmodel = set_solver_config(\n    model_raw;\n    saveat_mode=:saveat,\n    alg=AutoTsit5(Rosenbrock23()),\n    kwargs=(abstol=1e-2, reltol=1e-2),\n)\n\nBefore moving on, inspect the model structure to verify that all blocks were assembled correctly.","category":"section"},{"location":"tutorials/mixed-effects-nn-saem/#Model-Summary","page":"Mixed-Effects Tutorial 3: Neural Differential-Equation Components (SAEM)","title":"Model Summary","text":"model_summary = NoLimits.summarize(model)\nmodel_summary","category":"section"},{"location":"tutorials/mixed-effects-nn-saem/#Step-3:-Build-the-DataModel-and-Configure-SAEM","page":"Mixed-Effects Tutorial 3: Neural Differential-Equation Components (SAEM)","title":"Step 3: Build the DataModel and Configure SAEM","text":"In this step, you will pair the model with the data by constructing a DataModel, then configure the SAEM algorithm. SAEM alternates between sampling random effects conditional on the current parameter estimates (the E-step) and updating population parameters (the M-step). A key practical detail is the builtin_stats=:closed_form option, which enables built-in closed-form block updates rather than gradient-based optimization for those mapped parameters. This can substantially improve stability when the random-effect dimension is high, as it is here with four separate network weight vectors per subject.\n\nTwo configuration choices deserve attention. First, the re_mean_params mapping tells SAEM which fixed-effect parameter serves as the mean of each random-effect distribution, enabling the closed-form update. Second, the ebe_multistart_n and ebe_multistart_k settings control multistart initialization of the empirical Bayes estimates, helping avoid poor local optima in the early iterations.\n\ndm = DataModel(model, df; primary_id=:ID, time_col=:t)\n\nsaem_method = NoLimits.SAEM(;\n    sampler=MH(),\n    builtin_stats=:closed_form,\n    re_mean_params=(; etaA1=:zA1, etaA2=:zA2, etaC1=:zC1, etaC2=:zC2),\n    re_cov_params=NamedTuple(),\n    resid_var_param=:sigma,\n    maxiters=1000,\n    mcmc_steps=80,\n    t0=30,\n    turing_kwargs=(n_samples=80, n_adapt=0, progress=false),\n    optim_kwargs=(maxiters=300,),\n    progress=true,\n    ebe_multistart_n=300,\n    ebe_multistart_k=3,\n    ebe_rescue_on_high_grad=false,\n    verbose=false,\n)\n\nserialization = SciMLBase.EnsembleThreads()\n\nBefore fitting, review the data model summary to confirm that individuals, covariates, and grouping structures were parsed as expected.","category":"section"},{"location":"tutorials/mixed-effects-nn-saem/#DataModel-Summary","page":"Mixed-Effects Tutorial 3: Neural Differential-Equation Components (SAEM)","title":"DataModel Summary","text":"dm_summary = NoLimits.summarize(dm)\ndm_summary","category":"section"},{"location":"tutorials/mixed-effects-nn-saem/#Step-4:-Fit-the-Model-and-Inspect-Results","page":"Mixed-Effects Tutorial 3: Neural Differential-Equation Components (SAEM)","title":"Step 4: Fit the Model and Inspect Results","text":"You are now ready to run SAEM. The algorithm will iterate up to 1000 times, using Metropolis-Hastings sampling for the random effects within each E-step. After fitting, you will extract the final objective value and the number of estimated parameters as an initial sanity check.\n\nres_saem = fit_model(\n    dm,\n    saem_method;\n    serialization=serialization,\n    rng=Random.Xoshiro(21),\n)\n\n(\n    objective=NoLimits.get_objective(res_saem),\n    n_params=length(NoLimits.get_params(res_saem; scale=:untransformed)),\n)\n\nFor a more detailed summary of the fit – including parameter estimates and convergence diagnostics – call the summarize function.\n\nfit_summary_saem = NoLimits.summarize(res_saem)\nfit_summary_saem","category":"section"},{"location":"tutorials/mixed-effects-nn-saem/#Step-5:-Visualize-Fitted-Trajectories","page":"Mixed-Effects Tutorial 3: Neural Differential-Equation Components (SAEM)","title":"Step 5: Visualize Fitted Trajectories","text":"In this step, you will overlay the model predictions on the raw observations for the first two subjects. Plotting fitted trajectories against observed data provides an immediate visual assessment of model adequacy – you should see the neural ODE tracking the characteristic rise-and-decay pattern of the two-compartment transfer system, with subject-specific variation captured by the random effects.\n\np_fit_saem = plot_fits(\n    res_saem;\n    observable=:y,\n    individuals_idx=[1, 2],\n    ncols=2,\n    shared_x_axis=true,\n    shared_y_axis=true,\n)\n\np_fit_saem","category":"section"},{"location":"tutorials/mixed-effects-nn-saem/#Step-6:-Inspect-the-Observation-Distribution","page":"Mixed-Effects Tutorial 3: Neural Differential-Equation Components (SAEM)","title":"Step 6: Inspect the Observation Distribution","text":"As a final diagnostic, you will examine the implied observation distribution at a single data point for the first individual. This plot shows the full predictive distribution (not just the point estimate), which helps you assess whether the residual variance is well-calibrated and whether the model's uncertainty is reasonable.\n\np_obs_saem = plot_observation_distributions(\n    res_saem;\n    observables=:y,\n    individuals_idx=1,\n    obs_rows=1,\n)\n\np_obs_saem","category":"section"},{"location":"tutorials/mixed-effects-nn-saem/#Interpretation-Notes","page":"Mixed-Effects Tutorial 3: Neural Differential-Equation Components (SAEM)","title":"Interpretation Notes","text":"This modeling pattern combines mechanistic compartmental states with learned nonlinear rate functions inside a single mixed-effects ODE. The compartmental structure encodes known domain knowledge (e.g., mass conservation, transfer between compartments), while the neural networks fill in the unknown functional forms. This hybrid strategy is broadly applicable to any system where the topology is known but the governing laws are not fully specified.\nThe built-in closed-form updates (builtin_stats=:closed_form) can materially improve SAEM stability when the random-effect dimension is large, as is typical with neural-network weight vectors.\nThe hyperparameter settings used here (network width, iteration counts, tolerance levels) are intentionally modest to keep the tutorial fast. For production analyses, consider increasing maxiters, mcmc_steps, and the number of MCMC samples to ensure thorough convergence, and tightening the ODE solver tolerances.","category":"section"},{"location":"developers-guide/#Developers-Guide","page":"Developers Guide","title":"Developers Guide","text":"This page is a placeholder and will be expanded with maintainer and contributor developer workflows.","category":"section"},{"location":"developers-guide/#Scope","page":"Developers Guide","title":"Scope","text":"TODO","category":"section"},{"location":"developers-guide/#Codebase-Layout","page":"Developers Guide","title":"Codebase Layout","text":"TODO","category":"section"},{"location":"developers-guide/#Local-Development-Setup","page":"Developers Guide","title":"Local Development Setup","text":"TODO","category":"section"},{"location":"developers-guide/#Testing-Strategy","page":"Developers Guide","title":"Testing Strategy","text":"TODO","category":"section"},{"location":"developers-guide/#Documentation-Workflow","page":"Developers Guide","title":"Documentation Workflow","text":"TODO","category":"section"},{"location":"developers-guide/#Release-Process","page":"Developers Guide","title":"Release Process","text":"TODO","category":"section"},{"location":"model-building/differential-equation/#@DifferentialEquation","page":"@DifferentialEquation","title":"@DifferentialEquation","text":"Many longitudinal processes are naturally described by systems of ordinary differential equations (ODEs). The @DifferentialEquation block specifies the dynamics of such systems, defining how model states evolve over time and optionally declaring derived signals that depend on time and the current state.\n\nWithin @Model, this block is optional. When present, the companion @initialDE block is required to supply initial conditions.","category":"section"},{"location":"model-building/differential-equation/#Core-Syntax","page":"@DifferentialEquation","title":"Core Syntax","text":"Statements inside @DifferentialEquation take one of two forms:\n\nState dynamics: D(state) ~ rhs_expression – defines the time derivative of a state variable.\nDerived signals: signal(t) = expression (or signal(\\xi) = expression) – defines an auxiliary quantity computed within the ODE, useful for intermediate calculations that appear in multiple state equations.\n\nde = @DifferentialEquation begin\n    s(t) = sin(t)\n    D(x1) ~ -a * x1 + s(t)\n    D(x2) ~ a * x1 - b * x2\nend\n\nThe ordering of D(state) declarations determines the state indexing in the solver's u and du vectors.","category":"section"},{"location":"model-building/differential-equation/#Parsing-and-Validation-Rules","page":"@DifferentialEquation","title":"Parsing and Validation Rules","text":"The following constraints are enforced at parse time:\n\nThe block must be wrapped in begin ... end.\nOnly D(state) ~ expr and signal(t) = expr / signal(\\xi) = expr forms are permitted.\nState names and signal names must be unique across the block.\nD(...) must contain exactly one symbolic state name.\nDerived signals must always be referenced as function calls (s(t) or s(\\xi)), never as bare symbols (s).\nUnresolved symbols or functions in DE expressions raise an error during compilation.","category":"section"},{"location":"model-building/differential-equation/#Symbol-and-Function-Resolution","page":"@DifferentialEquation","title":"Symbol and Function Resolution","text":"NoLimits resolves symbols appearing in DE expressions from the following sources:\n\nVariable-like symbols are drawn from:\n\nFixed effects\nRandom effects\nConstant covariates\nPreDE outputs (@preDifferentialEquation)\n\nFunction-like calls are drawn from:\n\nDynamic covariates (e.g., w(t))\nHelper functions (@helpers)\nModel functions from learned parameter blocks (e.g., neural network, soft tree, or spline functions)\n\nNote that time-varying (non-dynamic) covariates are not available inside the ODE, since their values are defined only at observation times. Use dynamic covariates with an appropriate interpolation method for continuous-time access.","category":"section"},{"location":"model-building/differential-equation/#Example:-Standalone-DE-Compilation-and-Evaluation","page":"@DifferentialEquation","title":"Example: Standalone DE Compilation and Evaluation","text":"The @DifferentialEquation macro can be used outside of @Model for testing or prototyping. The following demonstrates direct compilation and evaluation of a single-state ODE.\n\nusing NoLimits\nusing ComponentArrays\n\nde = @DifferentialEquation begin\n    s(t) = sin(t)\n    D(x1) ~ -a * x1^2 + s(t) + w(t) + pre\nend\n\nctx = (\n    fixed_effects = ComponentArray(a = 0.3),\n    random_effects = ComponentArray(),\n    constant_covariates = NamedTuple(),\n    varying_covariates = (w = t -> 0.2 * t,),\n    helpers = NamedTuple(),\n    model_funs = NamedTuple(),\n    preDE = (pre = 0.5,),\n)\n\npc = get_de_compiler(de)(ctx)\nu = [1.0]\ndu = similar(u)\nget_de_f!(de)(du, u, pc, 0.4)","category":"section"},{"location":"model-building/differential-equation/#Example:-Nonlinear-Mixed-Effects-ODE-Model","page":"@DifferentialEquation","title":"Example: Nonlinear Mixed-Effects ODE Model","text":"This example illustrates a complete model in which random effects enter both the ODE dynamics and the observation distribution. The derived signal s(t) encapsulates a nonlinear transformation of a dynamic covariate and a preDE quantity, keeping the state equation concise.\n\nusing NoLimits\nusing Distributions\n\nmodel = @Model begin\n    @helpers begin\n        sat(u) = u / (1 + abs(u))\n    end\n\n    @fixedEffects begin\n        a = RealNumber(0.4)\n        b = RealNumber(0.2)\n        sigma = RealNumber(0.3, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n        w = DynamicCovariate()\n    end\n\n    @randomEffects begin\n        eta = RandomEffect(TDist(6.0); column=:ID)\n    end\n\n    @preDifferentialEquation begin\n        pre = exp(a + eta^2)\n    end\n\n    @DifferentialEquation begin\n        s(t) = sat(w(t) + pre)\n        D(x1) ~ -(b + tanh(eta)) * x1 + s(t)^2\n    end\n\n    @initialDE begin\n        x1 = pre\n    end\n\n    @formulas begin\n        y ~ Normal(log1p(abs(x1(t)) + eta^2), sigma)\n    end\nend","category":"section"},{"location":"model-building/differential-equation/#Example:-Learned-Right-Hand-Side-with-Neural-Networks-and-Soft-Trees","page":"@DifferentialEquation","title":"Example: Learned Right-Hand Side with Neural Networks and Soft Trees","text":"When the functional form of the ODE right-hand side is unknown or difficult to specify mechanistically, learned components such as neural networks and soft decision trees can be embedded directly into the dynamics. In this example, the transfer and elimination rates of a two-state system are parameterized by neural networks and soft trees, with individual-level random effects on the learned parameters.\n\nusing NoLimits\nusing Distributions\nusing Lux\nusing LinearAlgebra\n\nchain_A1 = Chain(Dense(1, 4, tanh), Dense(4, 1))\nchain_A2 = Chain(Dense(1, 4, tanh), Dense(4, 1))\ndepth_st = 2\n\nmodel = @Model begin\n    @helpers begin\n        softplus(u) = u > 20 ? u : log1p(exp(u))\n    end\n\n    @covariates begin\n        t = Covariate()\n        d = ConstantCovariate(; constant_on=:ID)\n    end\n\n    @fixedEffects begin\n        sigma = RealNumber(0.3, scale=:log)\n\n        zA1 = NNParameters(chain_A1; function_name=:NNA1, calculate_se=false)\n        zA2 = NNParameters(chain_A2; function_name=:NNA2, calculate_se=false)\n        gC1 = SoftTreeParameters(1, depth_st; function_name=:STC1, calculate_se=false)\n        gC2 = SoftTreeParameters(1, depth_st; function_name=:STC2, calculate_se=false)\n    end\n\n    @randomEffects begin\n        etaA1 = RandomEffect(MvNormal(zA1, Diagonal(ones(length(zA1)))); column=:ID)\n        etaA2 = RandomEffect(MvNormal(zA2, Diagonal(ones(length(zA2)))); column=:ID)\n        etaC1 = RandomEffect(MvNormal(gC1, Diagonal(ones(length(gC1)))); column=:ID)\n        etaC2 = RandomEffect(MvNormal(gC2, Diagonal(ones(length(gC2)))); column=:ID)\n    end\n\n    @DifferentialEquation begin\n        a_A(t) = softplus(depot)\n        x_C(t) = softplus(center)\n\n        fA1(t) = softplus(NNA1([t / 24], etaA1)[1])\n        fA2(t) = softplus(NNA2([a_A(t)], etaA2)[1])\n        fC1(t) = -softplus(STC1([x_C(t)], etaC1)[1])\n        fC2(t) = softplus(STC2([t / 24], etaC2)[1])\n\n        D(depot) ~ -d * fA1(t) - fA2(t)\n        D(center) ~ d * fA1(t) + fA2(t) + fC1(t) + d * fC2(t)\n    end\n\n    @initialDE begin\n        depot = d\n        center = 0.0\n    end\n\n    @formulas begin\n        y ~ LogNormal(center(t), sigma)\n    end\nend","category":"section"},{"location":"model-building/differential-equation/#DE-Accessors-for-States-and-Signals","page":"@DifferentialEquation","title":"DE Accessors for States and Signals","text":"After ODE integration, accessor functions provide callable trajectories for each state and derived signal. These are the same accessors that formula evaluation uses internally when outcomes reference x1(t) or s(t).\n\nif @isdefined(sol) && @isdefined(compiled_de_context)\n    accessors = get_de_accessors_builder(model.de.de)(sol, compiled_de_context)\n\n    x1_at_12 = accessors.x1(1.2)\n    s_at_12 = accessors.s(1.2)\nend","category":"section"},{"location":"model-building/differential-equation/#Related-APIs","page":"@DifferentialEquation","title":"Related APIs","text":"The following functions provide lower-level access to the DE subsystem:\n\nget_de_states(de) and get_de_signals(de) – enumerate declared states and signals.\nget_de_compiler(de), get_de_f!(de), get_de_f(de) – access the compiled ODE function and its context builder.\nget_de_accessors_builder(de) – returns a function that constructs solution accessors from an ODE solution.\nbuild_de_params(de, \\theta; ...) – assembles ODE parameters with tunable modes (:theta, :eta, :both).","category":"section"},{"location":"data-model-construction/#Data-Model-Construction","page":"Data Model Construction","title":"Data Model Construction","text":"The DataModel constructor binds a @Model specification to a tabular dataset, producing the per-individual data structures required by estimation, simulation, and plotting workflows.\n\nAt construction time, DataModel validates that the dataset schema is consistent with the model definition, splits rows by individual identifier, builds observation and covariate series, prepares random-effect group mappings, and – for ODE models with event columns – assembles event callback metadata.","category":"section"},{"location":"data-model-construction/#Constructor","page":"Data Model Construction","title":"Constructor","text":"using NoLimits\n\ndm = DataModel(\n    model,\n    df;\n    primary_id=:ID,\n    time_col=:t,\n)\n\nThe key constructor arguments are:\n\nprimary_id: the column that identifies individual trajectories (e.g., subject ID).\ntime_col: the column used for longitudinal time indexing.\nevid_col: enables event-table parsing for ODE models (see below).\namt_col, rate_col, cmt_col: event-related columns, used when evid_col is set.\nserialization: controls parallel execution mode (default EnsembleSerial(); use EnsembleThreads() for multi-threaded evaluation).\n\nIf primary_id is omitted, it is inferred automatically when the model uses exactly one random-effect grouping column. When multiple grouping columns are present, primary_id must be specified explicitly.","category":"section"},{"location":"data-model-construction/#Example:-Non-ODE-DataModel","page":"Data Model Construction","title":"Example: Non-ODE DataModel","text":"This example builds a DataModel for a model without differential equations, featuring two random-effect grouping levels and a non-Gaussian observation distribution:\n\nusing NoLimits\nusing DataFrames\nusing Distributions\n\nmodel = @Model begin\n    @fixedEffects begin\n        a = RealNumber(0.2)\n        sigma = RealNumber(0.4, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n        z = Covariate()\n    end\n\n    @randomEffects begin\n        eta_id = RandomEffect(TDist(6.0); column=:ID)\n        eta_site = RandomEffect(Gamma(2.0, 1.0); column=:SITE)\n    end\n\n    @formulas begin\n        mu = a + z + tanh(eta_id) + eta_site^2\n        y ~ Laplace(mu, sigma)\n    end\nend\n\ndf = DataFrame(\n    ID   = [1, 1, 2, 2],\n    SITE = [:A, :A, :B, :B],\n    t    = [0.0, 1.0, 0.0, 1.0],\n    z    = [0.2, 0.3, -0.1, 0.0],\n    y    = [0.5, 0.6, 0.1, 0.2],\n)\n\ndm = DataModel(model, df; primary_id=:ID, time_col=:t)","category":"section"},{"location":"data-model-construction/#Example:-ODE-DataModel-(No-Events)","page":"Data Model Construction","title":"Example: ODE DataModel (No Events)","text":"For models with differential equations but no dosing or reset events, the constructor requires only the standard arguments:\n\nusing NoLimits\nusing DataFrames\nusing Distributions\n\nmodel = @Model begin\n    @fixedEffects begin\n        k = RealNumber(0.3)\n        sigma = RealNumber(0.2, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n    end\n\n    @randomEffects begin\n        eta = RandomEffect(TDist(5.0); column=:ID)\n    end\n\n    @DifferentialEquation begin\n        D(x1) ~ -k * x1 + eta^2\n    end\n\n    @initialDE begin\n        x1 = 1.0\n    end\n\n    @formulas begin\n        y ~ Normal(x1(t), sigma)\n    end\nend\n\ndf = DataFrame(\n    ID = [1, 1, 1],\n    t  = [0.0, 0.5, 1.0],\n    y  = [1.0, 0.9, 0.8],\n)\n\ndm = DataModel(model, df; primary_id=:ID, time_col=:t)","category":"section"},{"location":"data-model-construction/#Example:-ODE-DataModel-with-Event-Columns","page":"Data Model Construction","title":"Example: ODE DataModel with Event Columns","text":"For models where external inputs (e.g., doses, stimuli, or resets) modify the ODE state during integration, use the evid_col argument to activate event parsing. The supported event types are:\n\nEVID = 1: input event (instantaneous bolus if RATE = 0, or constant-rate infusion if RATE > 0).\nEVID = 2: reset event (sets the specified compartment to the AMT value).\n\nusing NoLimits\nusing DataFrames\nusing Distributions\n\nmodel = @Model begin\n    @fixedEffects begin\n        ka = RealNumber(0.5)\n        kel = RealNumber(0.2)\n        sigma = RealNumber(0.3, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n    end\n\n    @randomEffects begin\n        eta = RandomEffect(TDist(6.0); column=:ID)\n    end\n\n    @DifferentialEquation begin\n        D(depot) ~ -ka * depot\n        D(center) ~ ka * depot - kel * center + eta^2\n    end\n\n    @initialDE begin\n        depot = 0.0\n        center = 0.0\n    end\n\n    @formulas begin\n        y ~ LogNormal(center(t), sigma)\n    end\nend\n\ndf = DataFrame(\n    ID   = [1, 1, 1, 1],\n    t    = [0.0, 0.0, 1.0, 2.0],\n    EVID = [1, 0, 0, 0],\n    AMT  = [10.0, 0.0, 0.0, 0.0],\n    RATE = [0.0, 0.0, 0.0, 0.0],\n    CMT  = [\"depot\", \"depot\", \"center\", \"center\"],\n    y    = [missing, 0.0, 2.1, 1.8],\n)\n\ndm = DataModel(\n    model,\n    df;\n    primary_id=:ID,\n    time_col=:t,\n    evid_col=:EVID,\n    amt_col=:AMT,\n    rate_col=:RATE,\n    cmt_col=:CMT,\n)\n\nThe CMT column can contain either integer indices or state names (String/Symbol), but the format must be consistent within the DataFrame.","category":"section"},{"location":"data-model-construction/#Dynamic-Covariates-and-Interpolation","page":"Data Model Construction","title":"Dynamic Covariates and Interpolation","text":"When using DynamicCovariate or DynamicCovariateVector, the DataInterpolations package must be loaded in the user environment:\n\nusing DataInterpolations\n\nDataModel requires that time values are sorted within each individual and enforces minimum observation counts per interpolation type:\n\nInterpolation Minimum observations per individual\nConstantInterpolation 1\nSmoothedConstantInterpolation 2\nLinearInterpolation 2\nLagrangeInterpolation 2\nAkimaInterpolation 2\nQuadraticInterpolation 3\nQuadraticSpline 3\nCubicSpline 3","category":"section"},{"location":"data-model-construction/#Validation-Rules","page":"Data Model Construction","title":"Validation Rules","text":"DataModel performs comprehensive validation at construction time to catch specification and data inconsistencies early. The checks include:\n\nSchema validation: primary_id, time_col, and outcome columns referenced in @formulas must exist in the DataFrame. Neither primary_id nor time_col may contain missing values.\nTime column declaration: time_col must be declared in @covariates as Covariate() or DynamicCovariate().\nEvent columns: when evid_col is set, event columns (AMT, RATE, CMT) are required and validated for completeness on event rows.\nMissing values: observation rows (EVID == 0) cannot have missing values in outcome or covariate columns used by @formulas.\nGrouping consistency: random-effect grouping columns cannot contain missing values. Grouping columns other than primary_id must be constant within each primary_id.\nNumeric random-effect grouping levels: if any random-effect grouping column uses numeric levels, DataModel emits: DataModel detected numeric random-effect grouping levels in column(s) $(cols_str). You wwill not be able to use constant random-effects. If you want to use constantr andom effects, consider ralabeling your random efefcts to strings or symbols. In this case, constant random-effects are unavailable unless grouping levels are relabeled to strings or symbols.\nConstant covariate consistency: constant covariates must be constant within primary_id and within all declared constant_on groups.\nRandom-effect covariate dependencies: random-effect distributions that use covariates require those covariates to be ConstantCovariate or ConstantCovariateVector.\nFormula time offsets: constant time offsets in formulas (e.g., x1(t - 0.5)) extend the integration window automatically. Offsets that push evaluation before t = 0 are rejected. Non-constant offsets require saveat_mode = :dense.\nPreDE random-effect constraint: random effects used in @preDifferentialEquation must be grouped by primary_id.","category":"section"},{"location":"data-model-construction/#DataModel-Summary","page":"Data Model Construction","title":"DataModel Summary","text":"Use NoLimits.summarize(dm) for a compact structural overview:\n\ndm_summary = NoLimits.summarize(dm)\ndm_summary","category":"section"},{"location":"data-model-construction/#Accessors","page":"Data Model Construction","title":"Accessors","text":"After construction, several accessor functions are available for diagnostics, custom tooling, and low-level inspection:\n\nindividuals = get_individuals(dm)\nind1 = get_individual(dm, 1)\nbatches = get_batches(dm)\nbatch_ids = get_batch_ids(dm)\nrow_groups = get_row_groups(dm)\nre_info = get_re_group_info(dm)\nre_idx_obs = get_re_indices(dm, 1)               # observation rows\nre_idx_all = get_re_indices(dm, 1; obs_only=false)","category":"section"},{"location":"plotting/#Plotting","page":"Plotting","title":"Plotting","text":"Effective model assessment depends on graphical diagnostics that expose patterns invisible in summary statistics alone. NoLimits provides a unified plotting interface for data inspection, fitted-model evaluation, residual analysis, random-effects diagnostics, and uncertainty visualization. Each function targets a specific aspect of model adequacy – from individual-level trajectory fits to distributional calibration of predictions – and all follow a consistent API.\n\nThis page contains executable examples that render directly during the documentation build.\n\nAll plotting functions accept a file-path keyword for saving output directly:\n\nsave_path=\"path/to/plot.png\" (existing)\nplot_path=\"path/to/plot.png\" (alias)","category":"section"},{"location":"plotting/#Plotting-APIs","page":"Plotting","title":"Plotting APIs","text":"The following functions are organized by the diagnostic question they address.\n\nCore trajectory and data plotting:\n\nbuild_plot_cache: Precompute reusable plot inputs (parameters, random effects, ODE solves, optional observation distributions) from a FitResult or DataModel. Avoids redundant computation when generating multiple plots from the same fit.\nplot_data: Plot observed trajectories by individual.\nplot_fits: Plot model-implied trajectories, with optional predictive-density overlays.\nplot_fits_comparison: Overlay fitted trajectories from multiple fit results on the same individual panels, enabling direct visual comparison of competing models or estimation methods.\nplot_multistart_waterfall: Plot successful multistart runs as objective-valued dots, sorted from best to worst by the package multistart ranking criterion.\nplot_multistart_fixed_effect_variability: Plot fixed-effect variability across top multistart runs as z-scores (single panel).\n\nObservation and predictive diagnostics:\n\nplot_observation_distributions: Plot predictive outcome distributions at selected observations, with observed values overlaid. Reveals local miscalibration that aggregate summaries may obscure.\nplot_vpc: Visual predictive check comparing simulated observations with observed data. Provides a calibration assessment in the observation space.\n\nResidual diagnostics:\n\nget_residuals: Return residual metrics in tabular form (:quantile, :pit, :raw, :pearson, :logscore).\nplot_residuals: Residuals versus a chosen x-axis variable (time or any varying covariate).\nplot_residual_distribution: Residual distribution histograms by observable.\nplot_residual_qq: QQ diagnostics for selected residual metrics.\nplot_residual_pit: Probability integral transform (PIT) diagnostics via histogram, KDE, or QQ view.\nplot_residual_acf: Residual autocorrelation by lag, useful for detecting unmodeled temporal structure.\n\nRandom-effects diagnostics:\n\nplot_random_effects_pdf: Marginal random-effect density, including posterior summaries for MCMC fits.\nplot_random_effect_distributions: Distribution-level diagnostics with empirical Bayes estimate (EBE) or posterior-mean overlays by level.\nplot_random_effect_pit: PIT diagnostics for random effects.\nplot_random_effect_standardized: Distribution of standardized EBEs, assessing whether the assumed random-effect variance is appropriate.\nplot_random_effect_standardized_scatter: Standardized EBEs versus level index or a constant covariate.\nplot_random_effect_pairplot: Pairwise EBE dependency visualization for detecting unexplained correlations.\nplot_random_effects_scatter: EBE or posterior-mean scatter versus level index or a constant covariate.\n\nUncertainty quantification diagnostics:\n\nplot_uq_distributions: Parameter uncertainty visualization from a UQResult (density or histogram).","category":"section"},{"location":"plotting/#Executable-setup","page":"Plotting","title":"Executable setup","text":"The following model and data are used throughout this page. The model specifies a linear trend with normally distributed random intercepts, fitted via the Laplace approximation. The data comprise four individuals, each observed at three time points.\n\nusing NoLimits\nusing DataFrames\nusing Distributions\nusing Random\nusing Turing\n\nRandom.seed!(12)\n\nmodel = @Model begin\n    @fixedEffects begin\n        a = RealNumber(0.3, calculate_se=true)\n        b = RealNumber(0.1, calculate_se=true)\n        ω = RealNumber(0.4, scale=:log, calculate_se=true)\n        σ = RealNumber(0.2, scale=:log, calculate_se=false)\n    end\n\n    @covariates begin\n        t = Covariate()\n    end\n\n    @randomEffects begin\n        η = RandomEffect(Normal(0.0, ω); column=:ID)\n    end\n\n    @formulas begin\n        μ = a + b * t + exp(η)\n        y ~ Normal(μ, σ)\n    end\nend\n\ndf = DataFrame(\n    ID=repeat([:A, :B, :C, :D], inner=3),\n    t=repeat([0.0, 1.0, 2.0], 4),\n    y=[1.2, 1.6, 2.1, 1.1, 1.4, 1.8, 0.9, 1.1, 1.4, 1.3, 1.8, 2.2],\n)\n\ndm = DataModel(model, df; primary_id=:ID, time_col=:t)\nres = fit_model(dm, NoLimits.Laplace(; optim_kwargs=(maxiters=8,)))\ncache = build_plot_cache(res; cache_obs_dists=true)\nnothing","category":"section"},{"location":"plotting/#Reusing-computations-with-build_plot_cache","page":"Plotting","title":"Reusing computations with build_plot_cache","text":"Many diagnostic plots share the same underlying computations: parameter extraction, random-effect estimation, and (for ODE models) numerical integration. Calling build_plot_cache once stores these intermediate results so that subsequent plotting calls avoid redundant work.\n\nThe cache contains:\n\nFixed effects used for plotting (including params overrides, when provided)\nRandom-effect values for each individual (respecting constants_re, when provided)\nODE solutions for each individual when the model includes a @DifferentialEquation block\nOptional per-observation outcome distributions when cache_obs_dists=true\n\nFor MCMC fits, cache construction also accepts mcmc_draws and mcmc_warmup to control how posterior draws are summarized before plotting.\n\ncache_fast = build_plot_cache(res; cache_obs_dists=false)\ncache_full = build_plot_cache(\n    res;\n    cache_obs_dists=true,\n    params=(a=0.35,),\n    constants_re=(η=Dict(:A => 0.0),),\n)\nnothing","category":"section"},{"location":"plotting/#Data-plot","page":"Plotting","title":"Data plot","text":"Before evaluating any fitted model, it is good practice to examine the raw observations. plot_data displays observed trajectories for each individual, providing an immediate sense of data coverage over time, individual-level variability, and potential outliers.\n\np_data = plot_data(res)\np_data","category":"section"},{"location":"plotting/#Fitted-trajectories","page":"Plotting","title":"Fitted trajectories","text":"plot_fits overlays observed data with model-implied trajectories for each individual. This is the primary tool for assessing whether the structural model captures the dominant trends in the data and whether individual-level fits are adequate. Discrepancies visible here may indicate model misspecification or insufficient flexibility in the random-effects structure.\n\np_fits = plot_fits(res; cache=cache)\np_fits","category":"section"},{"location":"plotting/#Multistart-objective-value-plot","page":"Plotting","title":"Multistart objective-value plot","text":"When the optimization landscape is multimodal, a single fit may converge to a local rather than global optimum. plot_multistart_waterfall visualizes the objective values from all successful multistart runs, sorted by rank. A flat plateau among the top runs suggests convergence to a consistent solution, while large gaps or a smooth decline may signal identifiability issues or the presence of multiple basins of attraction.\n\nms = NoLimits.Multistart(;\n    dists=(; a=Normal(0.0, 0.5), b=Normal(0.0, 0.5)),\n    n_draws_requested=3,\n    n_draws_used=2,\n    sampling=:lhs,\n)\n\nlaplace_quick = NoLimits.Laplace(;\n    optim_kwargs=(maxiters=4,),\n    inner_kwargs=(maxiters=20,),\n    multistart_n=0,\n    multistart_k=0,\n)\n\nres_ms = fit_model(ms, dm, laplace_quick)\np_ms = plot_multistart_waterfall(res_ms)\np_ms","category":"section"},{"location":"plotting/#Fitted-trajectory-comparison-across-models","page":"Plotting","title":"Fitted trajectory comparison across models","text":"Comparing fitted trajectories from different estimation methods or model specifications is a natural step in model selection. plot_fits_comparison overlays trajectories from multiple fit results on the same individual panels, making differences in structural predictions immediately visible.\n\nFor vectors, legend labels are assigned as Model 1, Model 2, and so on in input order. For NamedTuple and Dict inputs, the provided keys serve as legend labels. Per-model line styles can be customized through PlotStyle(comparison_line_styles=Dict(...)).\n\nsaem_quick = NoLimits.SAEM(;\n    sampler=MH(),\n    maxiters=20,\n    mcmc_steps=8,\n    t0=8,\n    kappa=0.7,\n    turing_kwargs=(n_samples=20, n_adapt=0, progress=false),\n    optim_kwargs=(maxiters=80,),\n    progress=false,\n    verbose=false,\n)\nres_saem_quick = fit_model(dm, saem_quick; rng=Random.Xoshiro(24))\ncomparison_style = PlotStyle(comparison_line_styles=Dict(\"SAEM\" => :dash))\np_compare = plot_fits_comparison(\n    (Laplace=res, SAEM=res_saem_quick);\n    individuals_idx=1:2,\n    style=comparison_style,\n)\np_compare","category":"section"},{"location":"plotting/#Multistart-fixed-effect-variability","page":"Plotting","title":"Multistart fixed-effect variability","text":"Beyond comparing objective values, it is informative to examine how fixed-effect estimates vary across top-ranked multistart runs. plot_multistart_fixed_effect_variability displays this variation as z-scores in a single panel, providing a concise summary of parameter stability across optimization restarts.\n\nConfiguration options:\n\nscale=:untransformed (default): display parameters on the natural scale\nOnly top-level fixed-effect blocks with calculate_se=true are included by default\nmode=:points shows all values from the top-k_best runs; mode=:quantiles shows quantile summaries instead\nUse include_parameters or exclude_parameters to select specific parameter blocks by name\n\np_ms_var_points = plot_multistart_fixed_effect_variability(\n    res_ms;\n    k_best=3,\n    mode=:points,\n)\np_ms_var_points\n\np_ms_var_quant = plot_multistart_fixed_effect_variability(\n    res_ms;\n    k_best=3,\n    mode=:quantiles,\n    quantiles=[0.1, 0.5, 0.9],\n    include_parameters=[:σ],\n)\np_ms_var_quant","category":"section"},{"location":"plotting/#Observation-distribution-diagnostic","page":"Plotting","title":"Observation distribution diagnostic","text":"Mean-level trajectory plots can mask important miscalibration in the predictive distribution. plot_observation_distributions addresses this by displaying the full predicted outcome distribution at selected observations alongside the observed value. When the observed value falls consistently in the tails of the predicted distribution, this signals that the assumed error model or its parameterization may need revision.\n\np_obs_dist = plot_observation_distributions(\n    res;\n    cache=cache,\n    individuals_idx=1,\n    obs_rows=2,\n    observables=:y,\n)\np_obs_dist","category":"section"},{"location":"plotting/#Residual-QQ-diagnostic","page":"Plotting","title":"Residual QQ diagnostic","text":"Quantile-quantile plots provide a sensitive assessment of whether residuals conform to their expected reference distribution. plot_residual_qq compares observed residual quantiles against theoretical quantiles; systematic departures from the diagonal indicate structural misspecification, heavy tails, or incorrect assumptions about the observation noise.\n\np_qq = plot_residual_qq(res; cache=cache, residual=:quantile)\np_qq","category":"section"},{"location":"plotting/#Visual-predictive-check","page":"Plotting","title":"Visual predictive check","text":"The visual predictive check (VPC) is a widely used diagnostic in longitudinal modelling. It evaluates a model's ability to reproduce the distribution of observed data through simulation. plot_vpc generates predictive envelopes from repeated simulations under the fitted model and overlays them on the observed data summaries. Agreement between the simulated envelopes and observed trends indicates that the model captures both the central tendency and the variability structure of the data.\n\np_vpc = plot_vpc(res; n_simulations=20, percentiles=[5, 50, 95])\np_vpc","category":"section"},{"location":"plotting/#Random-effects-distribution-diagnostic","page":"Plotting","title":"Random-effects distribution diagnostic","text":"A well-specified random-effects model should produce empirical Bayes estimates consistent with the assumed distributional form. plot_random_effects_pdf overlays estimated random-effect values on the model-implied density, providing a direct visual check for distributional adequacy. Departures such as multimodality, skewness, or outlying values may indicate that a more flexible random-effects distribution is needed.\n\np_re_pdf = plot_random_effects_pdf(res)\np_re_pdf","category":"section"},{"location":"plotting/#Uncertainty-quantification-plot","page":"Plotting","title":"Uncertainty quantification plot","text":"Reliable inference requires understanding the precision of parameter estimates. plot_uq_distributions visualizes parameter-level uncertainty from a computed UQResult object, revealing asymmetry, spread, and potential boundary effects that point summaries alone cannot convey. This is particularly informative for parameters estimated on transformed scales, where uncertainty may be highly asymmetric on the natural scale.\n\nuq = compute_uq(res; method=:wald, n_draws=80, rng=Random.Xoshiro(7))\np_uq = plot_uq_distributions(uq; scale=:natural, plot_type=:density)\np_uq","category":"section"},{"location":"estimation/mle-map/#MAP","page":"MAP","title":"MAP","text":"Maximum a posteriori (MAP) estimation extends MLE by incorporating prior information about the fixed effects into the objective function. Where MLE finds the parameters that maximize the data likelihood alone, MAP finds the parameters that maximize the posterior – the product of the likelihood and the prior. This makes MAP the natural choice when substantive domain knowledge is available and the model contains only fixed effects.","category":"section"},{"location":"estimation/mle-map/#Relationship-to-MLE","page":"MAP","title":"Relationship to MLE","text":"Both MLE and MAP optimize a scalar objective over the fixed-effect parameter space. The distinction lies in the objective itself:\n\nMLE minimizes the negative log-likelihood.\nMAP minimizes the negative log-likelihood plus the negative log-prior over fixed effects defined in @fixedEffects.\n\nThe model structure, optimizer interface, and result accessors are otherwise identical.","category":"section"},{"location":"estimation/mle-map/#Prior-Requirement","page":"MAP","title":"Prior Requirement","text":"MAP requires at least one fixed effect to carry a prior distribution (i.e., not Priorless()). If no priors are specified in the @fixedEffects block, fit_model will raise an error. Priors are assigned per parameter using the prior keyword; see the @fixedEffects documentation for supported distribution types.","category":"section"},{"location":"estimation/mle-map/#Minimal-Usage","page":"MAP","title":"Minimal Usage","text":"The following example assigns a Normal prior to the intercept and a LogNormal prior to the scale parameter, then fits the model via MAP.\n\nusing NoLimits\nusing DataFrames\nusing Distributions\n\nmodel = @Model begin\n    @fixedEffects begin\n        a = RealNumber(0.2, prior=Normal(0.0, 1.0))\n        sigma = RealNumber(0.5, scale=:log, prior=LogNormal(0.0, 0.5))\n    end\n\n    @covariates begin\n        t = Covariate()\n    end\n\n    @formulas begin\n        y ~ Pareto(exp(a), sigma)\n    end\nend\n\ndf = DataFrame(\n    ID = [1, 1, 2, 2],\n    t = [0.0, 1.0, 0.0, 1.0],\n    y = [1.0, 1.1, 0.9, 1.0],\n)\n\ndm = DataModel(model, df; primary_id=:ID, time_col=:t)\n\nres = fit_model(dm, NoLimits.MAP(; optim_kwargs=(maxiters=80,)))","category":"section"},{"location":"estimation/mle-map/#Comparing-MLE-and-MAP","page":"MAP","title":"Comparing MLE and MAP","text":"Because MAP shares the same constructor options and fit interface as MLE, switching between the two requires only changing the method argument. This makes it easy to assess how the prior influences the estimated parameters.\n\nres_mle = fit_model(dm, NoLimits.MLE())\nres_map = fit_model(dm, NoLimits.MAP())\n\nAll constructor options documented on the MLE page – including optimizer, optim_kwargs, adtype, lb, ub, and the fit_model keywords (constants, penalty, etc.) – apply to MAP as well.","category":"section"},{"location":"how-to-contribute/#How-to-Contribute","page":"How to Contribute","title":"How to Contribute","text":"Contributions and feedback are welcome. The easiest ways to get involved are:\n\nOpen an issue on GitHub to report a bug, request a feature, or ask a question.\nGet in touch by emailing the lead developer and maintainer Manuel Huth at manuel.huth@uni-bonn.de.","category":"section"},{"location":"estimation/mcmc/#MCMC","page":"MCMC","title":"MCMC","text":"Markov chain Monte Carlo (MCMC) methods provide a principled approach to Bayesian inference by drawing samples from the posterior distribution of model parameters. Rather than returning a single point estimate, the MCMC method in NoLimits.jl produces a full set of posterior samples, enabling rich uncertainty characterization. This is particularly valuable when the posterior landscape is complex, multimodal, or when downstream analyses require propagating parameter uncertainty into predictions. NoLimits interfaces with Turing.jl for all MCMC sampling.\n\nMCMC supports both:\n\nMixed-effects models with random effects jointly sampled alongside fixed effects\nFixed-effects-only models where only population-level parameters are inferred","category":"section"},{"location":"estimation/mcmc/#Applicability","page":"MCMC","title":"Applicability","text":"The following conditions must hold to use MCMC:\n\nAll free fixed effects must have prior distributions assigned.\nAt least one parameter must be sampled: either a free fixed effect, or random effects in a mixed-effects model (even if all fixed effects are held constant via constants).\nPenalty terms are not supported; use MAP for penalized estimation instead.\n\nNote that MCMC samples on the natural (untransformed) parameter scale. The parameter transforms used by optimization-based methods (such as MLE or Laplace) do not apply during sampling.","category":"section"},{"location":"estimation/mcmc/#Basic-Usage-(Mixed-Effects,-Nonlinear-in-Random-Effects)","page":"MCMC","title":"Basic Usage (Mixed Effects, Nonlinear in Random Effects)","text":"using NoLimits\nusing DataFrames\nusing Distributions\nusing Turing\n\nmodel = @Model begin\n    @fixedEffects begin\n        a = RealNumber(0.2, prior=Normal(0.0, 1.0))\n        b = RealNumber(0.1, prior=Normal(0.0, 1.0))\n        tau = RealNumber(0.4, scale=:log, prior=LogNormal(log(0.4), 0.5))\n        sigma = RealNumber(0.3, scale=:log, prior=LogNormal(log(0.3), 0.5))\n    end\n\n    @covariates begin\n        t = Covariate()\n    end\n\n    @randomEffects begin\n        eta = RandomEffect(Normal(0.0, tau); column=:ID)\n    end\n\n    @formulas begin\n        mu = exp(a + b * t + eta)   # nonlinear in random effects\n        y ~ LogNormal(log(mu), sigma)\n    end\nend\n\ndf = DataFrame(\n    ID = [:A, :A, :B, :B, :C, :C],\n    t = [0.0, 1.0, 0.0, 1.0, 0.0, 1.0],\n    y = [1.00, 1.26, 0.93, 1.12, 1.08, 1.37],\n)\n\ndm = DataModel(model, df; primary_id=:ID, time_col=:t)\n\nmethod = NoLimits.MCMC(;\n    sampler=NUTS(0.75),\n    turing_kwargs=(n_samples=500, n_adapt=250, progress=false),\n)\n\nres = fit_model(dm, method)","category":"section"},{"location":"estimation/mcmc/#Constructor-Options","page":"MCMC","title":"Constructor Options","text":"The MCMC constructor accepts the following keyword arguments, which control the sampler, the number of draws, and the automatic differentiation backend:\n\nusing NoLimits\nusing Turing\n\nmethod = NoLimits.MCMC(;\n    sampler=Turing.NUTS(0.75),\n    turing_kwargs=NamedTuple(),\n    adtype=Turing.AutoForwardDiff(),\n    progress=false,\n)","category":"section"},{"location":"estimation/mcmc/#Sampler-Dependent-Defaults","page":"MCMC","title":"Sampler-Dependent Defaults","text":"When n_samples and n_adapt are not explicitly provided in turing_kwargs, NoLimits applies sampler-specific defaults:\n\nSampler kind Default n_samples Default n_adapt\nNUTS 1000 500\nHMC 1500 750\nMH 2500 0\nOther 1000 500","category":"section"},{"location":"estimation/mcmc/#Turing.jl-Interface","page":"MCMC","title":"Turing.jl Interface","text":"MCMC interfaces directly with Turing.jl. The interaction between NoLimits and Turing is as follows:\n\nsampler is passed directly to Turing.sample.\nturing_kwargs are forwarded to Turing.sample after removing n_samples and n_adapt, which NoLimits handles explicitly.\nIf progress is not set in turing_kwargs, NoLimits uses the value from MCMC(progress=...).\nIf verbose is not set in turing_kwargs, the default is false.\nadtype configures Turing's automatic differentiation backend before sampling begins.","category":"section"},{"location":"estimation/mcmc/#Fit-Keywords","page":"MCMC","title":"Fit Keywords","text":"The call fit_model(dm, NoLimits.MCMC(...); ...) accepts the following keyword arguments:\n\nconstants – Fixes selected fixed effects to known values on the natural scale, removing them from the sampled parameter set.\nconstants_re – For mixed-effects models, fixes specific random-effect levels to known values while the remaining levels continue to be sampled. Level existence and dimensionality are validated.\node_args, ode_kwargs – Forwarded to ODE solvers used during likelihood evaluation.\nserialization – Controls whether likelihood evaluation proceeds via EnsembleSerial() or EnsembleThreads().\nrng – Random number generator used for sampling.\ntheta_0_untransformed – When provided (and turing_kwargs does not already include initial_params), this value initializes the free fixed effects.\nstore_data_model – If true, stores the DataModel inside the FitResult for use by downstream accessor functions.\n\nNot supported:\n\npenalty – Raises an error. Use MAP for penalized inference.","category":"section"},{"location":"estimation/mcmc/#Pattern:-Fix-Known-Random-Effect-Levels-(constants_re)","page":"MCMC","title":"Pattern: Fix Known Random-Effect Levels (constants_re)","text":"In some workflows, certain random-effect levels are known a priori – for example, from a previous analysis or by experimental design. These can be held fixed while the remaining levels are sampled:\n\nconstants_re = (; eta=(; A=0.0))\n\nres_fixed_levels = fit_model(\n    dm,\n    NoLimits.MCMC(; sampler=NUTS(0.75), turing_kwargs=(n_samples=300, n_adapt=150, progress=false));\n    constants_re=constants_re,\n)","category":"section"},{"location":"estimation/mcmc/#Pattern:-Sample-Random-Effects-with-All-Fixed-Effects-Held-Constant","page":"MCMC","title":"Pattern: Sample Random Effects with All Fixed Effects Held Constant","text":"For mixed-effects models, it can be useful to fix all population-level parameters and sample only the individual-level random effects. This is valid because random effects are treated as sampled parameters in the Bayesian framework:\n\nres_re_only = fit_model(\n    dm,\n    NoLimits.MCMC(; sampler=MH(), turing_kwargs=(n_samples=400, n_adapt=0, progress=false));\n    constants=(a=0.2, b=0.1, tau=0.4, sigma=0.3),\n)","category":"section"},{"location":"estimation/mcmc/#Fixed-Effects-Only-Example","page":"MCMC","title":"Fixed-Effects-Only Example","text":"When the model contains no random effects, MCMC samples only the fixed effects. The following example illustrates Bayesian inference for a Poisson regression model with count outcomes:\n\nusing NoLimits\nusing DataFrames\nusing Distributions\nusing Turing\n\nmodel_fixed = @Model begin\n    @covariates begin\n        t = Covariate()\n        z = Covariate()\n    end\n\n    @fixedEffects begin\n        a = RealNumber(0.0, prior=Normal(0.0, 1.0))\n        b = RealNumber(0.2, prior=Normal(0.0, 1.0))\n    end\n\n    @formulas begin\n        lambda = exp(a + b * z)\n        y ~ Poisson(lambda)\n    end\nend\n\ndf_fixed = DataFrame(\n    ID = [1, 1, 2, 2, 3, 3],\n    t = [0.0, 1.0, 0.0, 1.0, 0.0, 1.0],\n    z = [0.0, 0.2, 0.5, 0.7, 1.0, 1.2],\n    y = [1, 1, 2, 2, 2, 3],\n)\n\ndm_fixed = DataModel(model_fixed, df_fixed; primary_id=:ID, time_col=:t)\n\nres_fixed = fit_model(\n    dm_fixed,\n    NoLimits.MCMC(; sampler=MH(), turing_kwargs=(n_samples=300, n_adapt=0, progress=false)),\n)","category":"section"},{"location":"estimation/mcmc/#HMM-Example-(Mixed-Effects)","page":"MCMC","title":"HMM Example (Mixed Effects)","text":"Hidden Markov models (HMMs) with subject-level random effects can also be estimated via MCMC. In the following example, random effects enter the emission probabilities nonlinearly:\n\nusing NoLimits\nusing DataFrames\nusing Distributions\nusing Turing\n\nmodel_hmm = @Model begin\n    @covariates begin\n        t = Covariate()\n    end\n\n    @fixedEffects begin\n        p1_r = RealNumber(0.0, prior=Normal(0.0, 1.0))\n        p2_r = RealNumber(0.0, prior=Normal(0.0, 1.0))\n    end\n\n    @randomEffects begin\n        eta = RandomEffect(Normal(0.0, 1.0); column=:ID)\n    end\n\n    @formulas begin\n        p1 = 0.8 / (1 + exp(-(p1_r + eta))) + 0.1  # nonlinear in random effects\n        p2 = 0.8 / (1 + exp(-p2_r)) + 0.1\n        P = [0.9 0.1; 0.2 0.8]\n        y ~ DiscreteTimeDiscreteStatesHMM(\n            P,\n            (Bernoulli(p1), Bernoulli(p2)),\n            Categorical([0.6, 0.4]),\n        )\n    end\nend\n\ndf_hmm = DataFrame(\n    ID = [:A, :A, :A, :B, :B, :B],\n    t = [0.0, 1.0, 2.0, 0.0, 1.0, 2.0],\n    y = [0, 1, 1, 1, 0, 1],\n)\n\ndm_hmm = DataModel(model_hmm, df_hmm; primary_id=:ID, time_col=:t)\n\nres_hmm = fit_model(\n    dm_hmm,\n    NoLimits.MCMC(; sampler=MH(), turing_kwargs=(n_samples=200, n_adapt=0, progress=false)),\n)","category":"section"},{"location":"estimation/mcmc/#Accessing-Results","page":"MCMC","title":"Accessing Results","text":"Because MCMC produces posterior samples rather than a single point estimate, some result accessors differ from those of optimization-based methods:\n\nchain = get_chain(res)\nsampler_used = get_sampler(res)\nn_samples = get_n_samples(res)\nobserved = get_observed(res)\ndiagnostics = get_diagnostics(res)\n\nSince there is no single optimized objective value, get_objective(res) returns NaN and get_converged(res) returns missing.","category":"section"},{"location":"model-building/initial-de/#@initialDE","page":"@initialDE","title":"@initialDE","text":"Every ODE system requires a complete specification of initial conditions. The @initialDE block defines the initial value of each state declared in @DifferentialEquation, and it is mandatory whenever that block is present.\n\nInitial values may be constants, expressions involving fixed effects and random effects, or outputs of learned model functions – enabling individual-specific starting conditions in mixed-effects models.","category":"section"},{"location":"model-building/initial-de/#Core-Syntax","page":"@initialDE","title":"Core Syntax","text":"The block accepts only assignment statements. Each left-hand side must match a state name from @DifferentialEquation.\n\ninit = @initialDE begin\n    x1 = 1.0\n    x2 = 0.0\nend","category":"section"},{"location":"model-building/initial-de/#Parsing-and-Validation-Rules","page":"@initialDE","title":"Parsing and Validation Rules","text":"The following constraints are enforced at parse time:\n\nThe block must be wrapped in begin ... end.\nOnly assignments are allowed; other statement forms are rejected.\nLeft-hand side must be a symbol.\nDuplicate initial-state names are rejected.\nThe time symbols t and \\xi are forbidden, since initial conditions are evaluated before integration begins.\n\nAt build time, additional checks ensure consistency with @DifferentialEquation:\n\nEvery DE state must have a corresponding initial value.\nNo extra names beyond those declared as DE states are permitted.","category":"section"},{"location":"model-building/initial-de/#Symbol-and-Function-Resolution","page":"@initialDE","title":"Symbol and Function Resolution","text":"Initial-value expressions can reference the following model components:\n\nFixed effects\nRandom effects\nConstant covariates\nPreDE outputs (@preDifferentialEquation)\nHelper functions (@helpers)\nModel functions from learned parameter blocks (e.g., neural network, soft tree, or spline functions)\n\nThis resolution order mirrors that of @preDifferentialEquation, ensuring a consistent namespace across the pre-integration model components.","category":"section"},{"location":"model-building/initial-de/#Example:-Basic-Initial-State-Builder","page":"@initialDE","title":"Example: Basic Initial State Builder","text":"The @initialDE macro can be used independently for testing. Here, the builder function is constructed and called directly with explicit model inputs.\n\nusing NoLimits\nusing ComponentArrays\n\ninit = @initialDE begin\n    x1 = 1.0\n    x2 = a + b\nend\n\nbuilder = get_initialde_builder(init, [:x1, :x2])\n\ntheta = ComponentArray(a = 2.0, b = 3.0)\neta = ComponentArray()\nconst_covariates = NamedTuple()\nmodel_funs = NamedTuple()\nhelpers = NamedTuple()\nprede = NamedTuple()\n\nu0 = builder(theta, eta, const_covariates, model_funs, helpers, prede)","category":"section"},{"location":"model-building/initial-de/#Example:-Using-Helpers,-preDE,-and-Model-Functions","page":"@initialDE","title":"Example: Using Helpers, preDE, and Model Functions","text":"Initial conditions can incorporate helper functions, preDE outputs, and learned model functions. This is useful when the starting state depends on a nonlinear transformation of covariates or parameters.\n\nusing NoLimits\nusing ComponentArrays\n\ninit = @initialDE begin\n    x1 = helper(a) + preA\n    x2 = NN1([c1], z)[1]\nend\n\nhelpers = @helpers begin\n    helper(u) = u + 1.0\nend\n\nbuilder = get_initialde_builder(init, [:x1, :x2])\n\ntheta = ComponentArray(a = 2.0, z = 3.0)\neta = ComponentArray()\nconst_covariates = (c1 = 4.0,)\nmodel_funs = (NN1 = (x, z) -> [x[1] + z],)\nprede = (preA = 5.0,)\n\nu0 = builder(theta, eta, const_covariates, model_funs, helpers, prede)","category":"section"},{"location":"model-building/initial-de/#Example:-Nonlinear-Mixed-Effects-ODE-Initialization","page":"@initialDE","title":"Example: Nonlinear Mixed-Effects ODE Initialization","text":"In a complete model, initial conditions often depend on individual-level derived quantities. Here, the initial state is set to a preDE value that combines a fixed effect, a covariate, and a random effect – so each individual begins integration from a distinct starting point.\n\nusing NoLimits\nusing Distributions\n\nmodel = @Model begin\n    @fixedEffects begin\n        a = RealNumber(0.3)\n        b = RealNumber(0.1)\n        sigma = RealNumber(0.2, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n        age = ConstantCovariate(; constant_on=:ID)\n    end\n\n    @randomEffects begin\n        eta = RandomEffect(Gamma(2.0, 1.0); column=:ID)\n    end\n\n    @preDifferentialEquation begin\n        pre = exp(a + 0.01 * age + eta^2)\n    end\n\n    @DifferentialEquation begin\n        D(x1) ~ -b * x1 + pre\n    end\n\n    @initialDE begin\n        x1 = pre\n    end\n\n    @formulas begin\n        y ~ Gamma(log1p(x1(t)^2 + eta^2) + 1e-6, sigma)\n    end\nend","category":"section"},{"location":"model-building/initial-de/#Static-Initial-State-Variant","page":"@initialDE","title":"Static Initial-State Variant","text":"For performance-sensitive workflows using static arrays, the builder can return an SVector instead of a standard mutable vector. This is enabled by passing static=true.\n\nbuilder_static = get_initialde_builder(init, [:x1, :x2]; static=true)","category":"section"},{"location":"uncertainty-quantification/mcmc-based-uncertainty/#MCMC-Based-Uncertainty","page":"MCMC-based uncertainty","title":"MCMC-Based Uncertainty","text":"Posterior-draw uncertainty quantification in NoLimits.jl supports both exact posterior chains (MCMC) and approximate posterior draws (VI). This family of methods is useful when Gaussian approximations (Wald-style) are too restrictive, for example in correlated, skewed, or otherwise non-quadratic posterior geometries.\n\nNoLimits.jl provides two posterior-draw UQ workflows:\n\nChain UQ (method=:chain): extracts uncertainty directly from an existing MCMC or VI fit.\nMCMC refit UQ (method=:mcmc_refit): launches a new MCMC sampling run from a non-MCMC source fit.\n\nBoth workflows build on the package's MCMC interface, which integrates with Turing.jl.","category":"section"},{"location":"uncertainty-quantification/mcmc-based-uncertainty/#1)-Chain-UQ-(method:chain)","page":"MCMC-based uncertainty","title":"1) Chain UQ (method=:chain)","text":"Chain UQ operates on posterior draws already available from a fitted MCMC or VI result:\n\nFor MCMC, it uses retained post-warmup chain samples.\nFor VI, it samples draws from the fitted variational posterior.\n\nusing NoLimits\nusing Random\n\nif @isdefined(res_mcmc) && res_mcmc !== nothing\n    uq_chain = compute_uq(\n        res_mcmc;\n        method=:chain,\n        level=0.95,\n        mcmc_warmup=200,\n        mcmc_draws=1000,\n        rng=Random.Xoshiro(1),\n    )\nend\n\nif @isdefined(res_vi) && res_vi !== nothing\n    uq_chain_vi = compute_uq(\n        res_vi;\n        method=:chain,\n        level=0.95,\n        mcmc_draws=1000,\n        rng=Random.Xoshiro(2),\n    )\nend","category":"section"},{"location":"uncertainty-quantification/mcmc-based-uncertainty/#Requirements","page":"MCMC-based uncertainty","title":"Requirements","text":"The source fit must originate from NoLimits.MCMC or NoLimits.VI.\nThe fit must include a stored DataModel (store_data_model=true).","category":"section"},{"location":"uncertainty-quantification/mcmc-based-uncertainty/#Key-Controls","page":"MCMC-based uncertainty","title":"Key Controls","text":"mcmc_warmup: number of initial iterations discarded per chain as burn-in for MCMC. If omitted, NoLimits uses n_adapt from the fit diagnostics when available. Ignored for VI.\nmcmc_draws: number of posterior draws used for interval construction (subsampled from MCMC chains, or sampled from the VI posterior).\nconstants: optional fixed-effect constants that exclude specified coordinates from UQ.\nrng: controls random subsampling when mcmc_draws is smaller than the total number of available draws.","category":"section"},{"location":"uncertainty-quantification/mcmc-based-uncertainty/#Interval-Mode","page":"MCMC-based uncertainty","title":"Interval Mode","text":"For the chain backend, the interval argument accepts :auto, :equaltail, or :chain. Equal-tail intervals report symmetric quantiles of the posterior distribution at the requested confidence level.","category":"section"},{"location":"uncertainty-quantification/mcmc-based-uncertainty/#Inclusion-Rules","page":"MCMC-based uncertainty","title":"Inclusion Rules","text":"Only free fixed-effect coordinates with calculate_se=true are included in the UQ output.\n\nA coordinate is excluded when:\n\nit is fixed by constants, or\nits parent parameter block has calculate_se=false.","category":"section"},{"location":"uncertainty-quantification/mcmc-based-uncertainty/#2)-MCMC-Refit-UQ-(method:mcmc_refit)","page":"MCMC-based uncertainty","title":"2) MCMC Refit UQ (method=:mcmc_refit)","text":"MCMC refit UQ bridges optimization-based and Bayesian workflows. It is designed for situations where the original fit was obtained by an optimization-based method (e.g., MLE, Laplace), but fully Bayesian uncertainty estimates are desired. It launches a new MCMC sampling run initialized at the fitted parameter values and returns chain-based uncertainty from the resulting posterior.\n\nusing NoLimits\nusing Random\n\nuq_refit = compute_uq(\n    res_non_mcmc;\n    method=:mcmc_refit,\n    level=0.95,\n    mcmc_turing_kwargs=(n_samples=400, n_adapt=100, progress=false),\n    mcmc_draws=300,\n    rng=Random.Xoshiro(2),\n)","category":"section"},{"location":"uncertainty-quantification/mcmc-based-uncertainty/#Requirements-2","page":"MCMC-based uncertainty","title":"Requirements","text":"The source fit must not be from MCMC (use method=:chain for existing MCMC results).\nAll sampled fixed effects must have priors specified in the model definition.\nAt least one parameter must remain free (i.e., not held constant) in the refit.\n\nIf no sampled fixed effects remain and no random effects are present, the refit will raise an error.","category":"section"},{"location":"uncertainty-quantification/mcmc-based-uncertainty/#How-Parameters-Are-Selected-for-Refit","page":"MCMC-based uncertainty","title":"How Parameters Are Selected for Refit","text":"The refit determines which parameters to sample as follows:\n\nUser-specified constants are respected and held fixed.\nFixed effects with calculate_se=false are automatically held constant at their fitted values.\nAll remaining free fixed effects become sampled parameters in the MCMC run.\nconstants_re can be passed to hold specific random-effect levels fixed during the refit.","category":"section"},{"location":"uncertainty-quantification/mcmc-based-uncertainty/#Refit-Method-Configuration","page":"MCMC-based uncertainty","title":"Refit Method Configuration","text":"The MCMC refit can be configured in two ways. Either pass a fully configured method object:\n\nmcmc_method::NoLimits.MCMC – an explicit MCMC method instance.\n\nOr configure the sampler through individual keyword arguments:\n\nmcmc_sampler – the sampling algorithm (defaults to Turing's NUTS sampler).\nmcmc_turing_kwargs – keyword arguments forwarded to Turing's sampling call.\nmcmc_adtype – automatic differentiation backend for the sampler.\nmcmc_fit_kwargs – additional keyword arguments for the underlying fit_model call.\n\nWhen no explicit configuration is provided, defaults from NoLimits.MCMC are used with a NUTS sampler.","category":"section"},{"location":"uncertainty-quantification/mcmc-based-uncertainty/#Returned-Quantities","page":"MCMC-based uncertainty","title":"Returned Quantities","text":"Both backends return interval estimates, covariance estimates computed from retained draws, and the draws themselves.\n\nif @isdefined(uq_chain)\n    backend = get_uq_backend(uq_chain)     # :chain or :mcmc_refit\n    names = get_uq_parameter_names(uq_chain)\n\n    est_nat = get_uq_estimates(uq_chain; scale=:natural)\n    ints_nat = get_uq_intervals(uq_chain; scale=:natural)\n    V_nat = get_uq_vcov(uq_chain; scale=:natural)\n    draws_nat = get_uq_draws(uq_chain; scale=:natural)\n\n    diag = get_uq_diagnostics(uq_chain)\nend","category":"section"},{"location":"uncertainty-quantification/mcmc-based-uncertainty/#Diagnostics","page":"MCMC-based uncertainty","title":"Diagnostics","text":"The diagnostics returned by get_uq_diagnostics provide information about the draw source and the draws used for UQ.\n\nCommon fields include:\n\nwarmup: number of discarded warmup iterations (0 for VI chain UQ).\nrequested_draws, available_draws, used_draws: draw accounting information.\nn_iter, n_chains, n_samples: draw-source metadata.\nsource: draw source tag (for example :mcmc_chain or :vi_posterior).\nn_active_parameters: number of active fixed-effect coordinates used in UQ.\n\nFor method=:mcmc_refit, the diagnostics additionally include:\n\nrefit_source_method: the estimation method of the original fit.\nrefit_sampler: the sampler used in the refit.\nrefit_turing_kwargs: the Turing keyword arguments used.\nsampled_fixed_names: names of fixed effects that were sampled.\nconstants_used: the constants applied during the refit.","category":"section"},{"location":"estimation/#Estimation","page":"Overview","title":"Estimation","text":"Parameter estimation is the process of inferring model parameters from observed data. NoLimits.jl provides a unified, method-driven interface: you pass a DataModel and a method object to fit_model, which returns a structured FitResult. Because the same interface is used across all estimation methods, switching between approaches – or comparing several on the same model and data – requires changing only the method argument.","category":"section"},{"location":"estimation/#Unified-Entry-Point","page":"Overview","title":"Unified Entry Point","text":"using NoLimits\n\nres = fit_model(dm, NoLimits.Laplace())\n\nEvery FitResult provides a common set of accessors:\n\ntheta_u = get_params(res; scale=:untransformed)\nobjective = get_objective(res)\nconverged = get_converged(res)\nsummary = get_summary(res)\ndiagnostics = get_diagnostics(res)\n\nUse NoLimits.summarize(res) for a compact summary table that includes the objective value, parameter estimates, and – for mixed-effects fits – random-effects summaries:\n\nfit_summary = NoLimits.summarize(res)\nfit_summary","category":"section"},{"location":"estimation/#Available-Methods","page":"Overview","title":"Available Methods","text":"The choice of method depends on whether the model includes random effects and on the inferential framework you require:\n\nModel type Methods Notes\nMixed-effects Laplace, LaplaceMAP, MCEM, SAEM, MCMC, VI Require random effects in the model\nFixed-effects only MLE, MAP, MCMC, VI MLE is likelihood-only; MAP adds priors; MCMC/VI are Bayesian\nCross-method Multistart Wrapper that runs repeated fits from different starting values","category":"section"},{"location":"estimation/#Common-Fit-Keywords","page":"Overview","title":"Common Fit Keywords","text":"Several keyword arguments are shared across methods (though not all apply to every method):\n\nconstants: fix selected fixed effects to known values (specified on the natural, untransformed scale).\nconstants_re: fix selected random-effect levels to known values (available for mixed-effects methods only).\npenalty: L2-style parameter penalties (not supported by MCMC or VI; use MAP for penalized estimation instead).\node_args, ode_kwargs: forwarded to the ODE solver during likelihood evaluation.\nserialization: EnsembleSerial() or EnsembleThreads() for parallel evaluation across individuals.\nrng: random-number generator for reproducibility.\ntheta_0_untransformed: custom starting values on the natural (untransformed) scale.\nstore_data_model: stores the DataModel inside the FitResult for use by downstream accessors (default true).\nstore_eb_modes: for MCEM/SAEM, controls whether empirical Bayes modes are stored during fitting.\n\nPrior requirements by method:\n\nMCMC requires priors on all free fixed effects.\nVI requires priors on all free fixed effects.\nLaplaceMAP requires priors on all fixed effects.\nMAP requires at least one fixed-effect prior.\nMCEM and SAEM do not incorporate fixed-effect priors in their objective.","category":"section"},{"location":"estimation/#Multistart-Wrapper","page":"Overview","title":"Multistart Wrapper","text":"For optimization-based methods, the solution found can depend on the initial parameter values. Multistart addresses this sensitivity by running multiple fits from different starting points and selecting the best result:\n\nusing NoLimits\nusing Distributions\n\nms = NoLimits.Multistart(;\n    dists=(; a=Normal(0.0, 1.0)),\n    n_draws_requested=12,\n    n_draws_used=6,\n    sampling=:lhs,\n)\n\nres_ms = fit_model(ms, dm, NoLimits.Laplace(; optim_kwargs=(maxiters=80,)))\nres_best = get_multistart_best(res_ms)\n\nRun-level inspection is available via get_multistart_results, get_multistart_errors, get_multistart_starts, and get_multistart_best_index.","category":"section"},{"location":"estimation/#Example:-Mixed-Effects-Model-Fitted-with-Multiple-Methods","page":"Overview","title":"Example: Mixed-Effects Model Fitted with Multiple Methods","text":"The following example defines a nonlinear mixed-effects model and fits it with Laplace, MCEM, SAEM, and MCMC, illustrating how the same DataModel can be passed to all estimators without modification:\n\nusing NoLimits\nusing DataFrames\nusing Distributions\n\nmodel = @Model begin\n    @fixedEffects begin\n        a = RealNumber(0.2, prior=Normal(0.0, 1.0))\n        b = RealNumber(0.1, prior=Normal(0.0, 1.0))\n        sigma = RealNumber(0.3, scale=:log, prior=LogNormal(0.0, 0.5))\n    end\n\n    @covariates begin\n        t = Covariate()\n    end\n\n    @randomEffects begin\n        eta = RandomEffect(TDist(6.0); column=:ID)\n    end\n\n    @formulas begin\n        mu = a + b * t + exp(eta)   # nonlinear in the random effect\n        y ~ Laplace(mu, sigma)\n    end\nend\n\ndf = DataFrame(\n    ID = [:A, :A, :B, :B, :C, :C],\n    t = [0.0, 1.0, 0.0, 1.0, 0.0, 1.0],\n    y = [1.0, 1.4, 0.8, 1.1, 1.3, 1.7],\n)\n\ndm = DataModel(model, df; primary_id=:ID, time_col=:t)\n\nlaplace_method = NoLimits.Laplace(; optim_kwargs=(maxiters=80,))\nmcem_method = NoLimits.MCEM(;\n    maxiters=8,\n    sample_schedule=20,\n    turing_kwargs=(n_samples=20, n_adapt=5, progress=false),\n)\nsaem_method = NoLimits.SAEM(;\n    maxiters=20,\n    mcmc_steps=8,\n    turing_kwargs=(n_adapt=8, progress=false),\n)\nmcmc_method = NoLimits.MCMC(; turing_kwargs=(n_samples=300, n_adapt=100, progress=false))\n\nres_laplace = fit_model(dm, laplace_method)\nres_mcem = fit_model(dm, mcem_method)\nres_saem = fit_model(dm, saem_method)\nres_mcmc = fit_model(dm, mcmc_method)\n\ntheta_laplace = get_params(res_laplace; scale=:untransformed)\nre_laplace = get_random_effects(res_laplace)\nll_laplace = get_loglikelihood(res_laplace)\n\nchain_mcmc = get_chain(res_mcmc)","category":"section"},{"location":"estimation/#Fixing-Known-Random-Effect-Levels","page":"Overview","title":"Fixing Known Random-Effect Levels","text":"In some workflows, random-effect values are known for a subset of individuals – for instance, from a previous analysis or an external constraint. The constants_re keyword fixes these levels at their known values while allowing all remaining levels to be estimated normally:\n\nconstants_re = (; eta=(; A=0.0))\n\nres_laplace_fixed = fit_model(\n    dm,\n    NoLimits.Laplace(; optim_kwargs=(maxiters=80,));\n    constants_re=constants_re,\n)","category":"section"},{"location":"estimation/#Fixed-Effects-Only-Path","page":"Overview","title":"Fixed-Effects-Only Path","text":"For models without random effects, use MLE, MAP, MCMC, or VI:\n\nmodel_fixed = @Model begin\n    @fixedEffects begin\n        a = RealNumber(0.2, prior=Normal(0.0, 1.0))\n        b = RealNumber(0.1, prior=Normal(0.0, 1.0))\n        sigma = RealNumber(0.3, scale=:log, prior=LogNormal(0.0, 0.5))\n    end\n\n    @covariates begin\n        t = Covariate()\n    end\n\n    @formulas begin\n        y ~ LogNormal(a + b * t, sigma)\n    end\nend\n\ndm_fixed = DataModel(model_fixed, df; primary_id=:ID, time_col=:t)\n\nres_mle = fit_model(dm_fixed, NoLimits.MLE(; optim_kwargs=(maxiters=120,)))\nres_mcmc_fixed = fit_model(\n    dm_fixed,\n    NoLimits.MCMC(; turing_kwargs=(n_samples=300, n_adapt=100, progress=false)),\n)\n\nres_vi_fixed = fit_model(\n    dm_fixed,\n    NoLimits.VI(; turing_kwargs=(max_iter=300, progress=false)),\n)","category":"section"},{"location":"tutorials/fixed-effects-nonlinear-mle-map/#Fixed-Effects-Tutorial-1:-Nonlinear-Longitudinal-Model-(MLE-MAP)","page":"Fixed-Effects Tutorial 1: Nonlinear Longitudinal Model (MLE + MAP)","title":"Fixed-Effects Tutorial 1: Nonlinear Longitudinal Model (MLE + MAP)","text":"Many biological processes – organ growth, tumor progression, enzyme saturation – follow sigmoidal trajectories that cannot be captured by linear regression. Fitting such curves to repeated measurements collected over time is a core task in longitudinal data analysis. In this tutorial, you will build a logistic growth model for tree-trunk circumference data, estimate its parameters with two complementary methods (maximum likelihood and maximum a posteriori), and examine how prior information shapes the result. No random effects are introduced here; a later tutorial extends the same model to account for inter-subject variability.","category":"section"},{"location":"tutorials/fixed-effects-nonlinear-mle-map/#What-You-Will-Learn","page":"Fixed-Effects Tutorial 1: Nonlinear Longitudinal Model (MLE + MAP)","title":"What You Will Learn","text":"This tutorial walks through the complete modelling cycle for a fixed-effects nonlinear model in NoLimits.jl. Along the way you will learn how to:\n\nTranslate a scientific growth equation into a NoLimits model specification.\nConnect that specification to observed data by constructing a DataModel.\nEstimate parameters under two philosophies. Maximum likelihood estimation (MLE) finds the parameter values that make the observed data most probable, treating the likelihood as the sole objective. Maximum a posteriori estimation (MAP) augments the likelihood with prior distributions on the parameters, which can improve numerical stability when some parameters are weakly identified – a common situation with sigmoidal models whose asymptote or inflection point lies outside the observed time window.\nCompare and diagnose the resulting estimates by visualising fitted trajectories and inspecting observation-level distributions.\n\nBy the end, you will have a working template that you can adapt to your own nonlinear longitudinal datasets.","category":"section"},{"location":"tutorials/fixed-effects-nonlinear-mle-map/#Step-1:-Load-the-Data","page":"Fixed-Effects Tutorial 1: Nonlinear Longitudinal Model (MLE + MAP)","title":"Step 1: Load the Data","text":"In this step, you will load the dataset used throughout the tutorial: the classic Orange tree growth study, originally published by Draper and Smith and available in R as datasets::Orange. It records the trunk circumference of five orange trees measured at seven time points each, making it a compact but representative example of nonlinear longitudinal data.\n\nusing NoLimits\nusing CSV\nusing DataFrames\nusing Distributions\nusing Downloads\nusing Random\nusing SciMLBase\n\ninclude(joinpath(@__DIR__, \"_data_loaders.jl\"))\n\nRandom.seed!(202)\n\ndf = load_orange()\n\nfirst(df, 8)\n\nEach row records one measurement occasion. The Tree column identifies the individual, age gives the tree's age in days since planting, and circumference is the trunk circumference in millimetres. Because every tree is measured at the same set of ages, this is a balanced design – convenient for illustration, though NoLimits.jl handles unbalanced data equally well.","category":"section"},{"location":"tutorials/fixed-effects-nonlinear-mle-map/#Step-2:-Define-a-Nonlinear-Fixed-Effects-Model","page":"Fixed-Effects Tutorial 1: Nonlinear Longitudinal Model (MLE + MAP)","title":"Step 2: Define a Nonlinear Fixed-Effects Model","text":"In this step, you will express the scientific model as a NoLimits model specification. The structural model is a three-parameter logistic growth curve, one of the simplest sigmoidal functions used in biology:\n\nmu(t) = fraca1 + expbigl(-k(t - t_0)bigr)\n\nHere, a sets the upper asymptote (the maximum circumference the model can predict), k controls the steepness of the growth phase, and t0 locates the inflection point – the age at which growth is fastest. Observations are assumed normally distributed around this deterministic trajectory with standard deviation sigma.\n\nNotice that we also attach prior distributions to each parameter. MLE will ignore these priors entirely, but they become essential for MAP estimation in Step 4. The priors encode only weak domain knowledge: circumferences are positive and on the order of tens to hundreds of millimetres, growth rates are small and positive, and the inflection point falls somewhere within the range of observed ages.\n\nusing NoLimits\nusing Distributions\n\nmodel = @Model begin\n    @covariates begin\n        age = Covariate()\n    end\n\n    @fixedEffects begin\n        a = RealNumber(100.0, prior=Normal(100.0, 30.0), calculate_se=true)\n        k = RealNumber(0.01, prior=Normal(0.01, 0.01), calculate_se=true)\n        t0 = RealNumber(500.0, prior=Normal(500.0, 200.0), calculate_se=true)\n        σ = RealNumber(5.0, scale=:log, prior=LogNormal(1.0, 0.5), calculate_se=true)\n    end\n\n    @formulas begin\n        μ = a / (1 + exp(-k * (age - t0)))\n        circumference ~ Normal(μ, σ)\n    end\nend\n\nA few points worth noting about this specification. The @covariates block declares age as a time-varying covariate, which tells NoLimits to look up a column called age in the data frame. The residual standard deviation sigma is parameterised on the log scale (scale=:log), which constrains it to be strictly positive during optimisation without requiring explicit bounds. Finally, the @formulas block first computes the deterministic prediction mu and then declares that the observed column circumference follows a normal distribution centred on that prediction.","category":"section"},{"location":"tutorials/fixed-effects-nonlinear-mle-map/#Model-Summary","page":"Fixed-Effects Tutorial 1: Nonlinear Longitudinal Model (MLE + MAP)","title":"Model Summary","text":"Before fitting, it is good practice to inspect the model structure programmatically. The summary confirms which parameters are declared as fixed effects, which covariates are expected, and what form the observation model takes.\n\nmodel_summary = NoLimits.summarize(model)\nmodel_summary","category":"section"},{"location":"tutorials/fixed-effects-nonlinear-mle-map/#Step-3:-Build-the-DataModel-and-Configure-Estimation","page":"Fixed-Effects Tutorial 1: Nonlinear Longitudinal Model (MLE + MAP)","title":"Step 3: Build the DataModel and Configure Estimation","text":"With the model and data in hand, you will now pair them into a DataModel – the central object that validates the data against the model specification and prepares the internal structures needed for estimation. In the same step, you will configure the two estimation methods (MLE and MAP), specifying a maximum of 150 optimiser iterations for each.\n\ndm = DataModel(model, df; primary_id=:Tree, time_col=:age)\n\nmle_method = NoLimits.MLE(; optim_kwargs=(maxiters=150,))\nmap_method = NoLimits.MAP(; optim_kwargs=(maxiters=150,))\n\nserialization = SciMLBase.EnsembleThreads()\n\nThe primary_id=:Tree argument tells NoLimits which column identifies individuals, and time_col=:age specifies the time axis. Setting serialization=EnsembleThreads() enables multi-threaded likelihood evaluation across individuals – a minor convenience here with only five trees, but essential for larger datasets where per-individual computations dominate runtime.","category":"section"},{"location":"tutorials/fixed-effects-nonlinear-mle-map/#DataModel-Summary","page":"Fixed-Effects Tutorial 1: Nonlinear Longitudinal Model (MLE + MAP)","title":"DataModel Summary","text":"It is worth verifying that the data were parsed correctly. The summary should confirm five individuals, seven observations each, no missing values, and no events.\n\ndm_summary = NoLimits.summarize(dm)\ndm_summary","category":"section"},{"location":"tutorials/fixed-effects-nonlinear-mle-map/#Step-4:-Fit-with-MLE-and-MAP","page":"Fixed-Effects Tutorial 1: Nonlinear Longitudinal Model (MLE + MAP)","title":"Step 4: Fit with MLE and MAP","text":"You are now ready to estimate the model parameters under both methods. Each call to fit_model runs a gradient-based optimiser (L-BFGS by default), starting from the initial values declared in the @fixedEffects block. The key difference lies in the objective function: MLE maximises the log-likelihood alone, while MAP maximises the log-likelihood plus the log-prior.\n\nres_mle = fit_model(dm, mle_method; serialization=serialization, rng=Random.Xoshiro(41))\nres_map = fit_model(dm, map_method; serialization=serialization, rng=Random.Xoshiro(42))\n\n(\n    objective_mle=NoLimits.get_objective(res_mle),\n    objective_map=NoLimits.get_objective(res_map),\n)\n\nThe returned objective values are negative log-likelihoods (for MLE) or negative log-posteriors (for MAP), so lower values indicate a better fit. Because the MAP objective includes additional prior penalty terms, its numerical value is not directly comparable to the MLE objective; what matters at this stage is that each method converged successfully.\n\nTo verify convergence and examine standard errors, you can request a detailed fit summary. This reports convergence status, the final objective value, and standard errors for all parameters whose calculate_se flag was set to true.\n\nfit_summary_mle = NoLimits.summarize(res_mle)\nfit_summary_map = NoLimits.summarize(res_map)\n\nfit_summary_mle","category":"section"},{"location":"tutorials/fixed-effects-nonlinear-mle-map/#Step-5:-Compare-Parameter-Estimates","page":"Fixed-Effects Tutorial 1: Nonlinear Longitudinal Model (MLE + MAP)","title":"Step 5: Compare Parameter Estimates","text":"With both fits complete, you can now extract the estimated parameters on the original (untransformed) scale and compare them side by side.\n\nθ_mle = NoLimits.get_params(res_mle; scale=:untransformed)\nθ_map = NoLimits.get_params(res_map; scale=:untransformed)\n\n(\n    mle=θ_mle,\n    map=θ_map,\n)\n\nBecause this dataset is relatively small (35 observations total), the priors exert a noticeable pull on the MAP estimates. In particular, parameters whose likelihood surface is flat – such as the asymptote a, which is only weakly constrained when few trees have approached their maximum size – will be drawn toward the prior mean under MAP but left at the pure likelihood optimum under MLE. As sample size grows, the likelihood dominates and the two methods converge to the same values. This behaviour serves as a useful diagnostic: large MLE–MAP discrepancies flag parameters that the data alone cannot pin down precisely.","category":"section"},{"location":"tutorials/fixed-effects-nonlinear-mle-map/#Step-6:-Fitted-Trajectories","page":"Fixed-Effects Tutorial 1: Nonlinear Longitudinal Model (MLE + MAP)","title":"Step 6: Fitted Trajectories","text":"Plotting model predictions against raw observations is the most direct way to assess goodness of fit. In this step, you will overlay the estimated logistic curves on the data for the first two trees under each estimation method.\n\ninds = [1, 2]\n\np_fit_mle = plot_fits(\n    res_mle;\n    observable=:circumference,\n    individuals_idx=inds,\n    ncols=2,\n    shared_x_axis=true,\n    shared_y_axis=true,\n)\n\np_fit_map = plot_fits(\n    res_map;\n    observable=:circumference,\n    individuals_idx=inds,\n    ncols=2,\n    shared_x_axis=true,\n    shared_y_axis=true,\n)\n\nMLE fit plot:\n\np_fit_mle\n\nBecause there are no random effects in this model, every tree is predicted by the same population-level curve. Deviations between the curve and individual data points therefore reflect both measurement noise and genuine between-tree variability that the current model does not capture. If these residual patterns appear systematic – for example, if one tree consistently lies above the curve while another lies below – that is a strong signal that random effects should be introduced (see the mixed-effects tutorials).\n\nMAP fit plot:\n\np_fit_map\n\nThe MAP trajectories may appear nearly identical to the MLE trajectories or show subtle shifts, depending on how informative the priors are relative to the data. Comparing the two plots is a quick visual assessment of prior influence.","category":"section"},{"location":"tutorials/fixed-effects-nonlinear-mle-map/#Step-7:-Observation-Distribution-Diagnostics","page":"Fixed-Effects Tutorial 1: Nonlinear Longitudinal Model (MLE + MAP)","title":"Step 7: Observation Distribution Diagnostics","text":"Beyond trajectory plots, it is informative to inspect the implied observation distribution at individual time points. In this step, you will plot the normal distribution predicted by the model at the first observation of the first tree, with the actual observed value marked. A well-calibrated model places observed values near the centre of the predicted distribution rather than in the tails.\n\np_obs_mle = plot_observation_distributions(\n    res_mle;\n    observables=:circumference,\n    individuals_idx=1,\n    obs_rows=1,\n)\n\np_obs_map = plot_observation_distributions(\n    res_map;\n    observables=:circumference,\n    individuals_idx=1,\n    obs_rows=1,\n)\n\nMLE observation-distribution plot:\n\np_obs_mle\n\nMAP observation-distribution plot:\n\np_obs_map\n\nIf an observed value sits far in the tail of the predicted distribution, this may indicate model misspecification – for instance, a residual error distribution that is too narrow, or a structural model that systematically over- or under-predicts at certain ages.","category":"section"},{"location":"tutorials/fixed-effects-nonlinear-mle-map/#Summary-and-Next-Steps","page":"Fixed-Effects Tutorial 1: Nonlinear Longitudinal Model (MLE + MAP)","title":"Summary and Next Steps","text":"This tutorial demonstrated the end-to-end workflow for fixed-effects nonlinear modelling in NoLimits.jl: specifying a scientifically motivated structural model, pairing it with data, estimating parameters via MLE and MAP, and diagnosing the results. Several practical points are worth keeping in mind:\n\nMLE is purely data-driven. It finds the parameter values that maximise the likelihood of the observed data. Declared priors have no effect on the MLE objective.\nMAP incorporates prior knowledge. By adding log-prior terms to the objective, MAP can stabilise estimation when the data provide limited information about certain parameters – a common situation in nonlinear models with saturation or asymptotic behaviour.\nFixed-effects models assume no between-subject variability. Every individual is predicted by the same curve. When individual trajectories diverge systematically, random effects are needed to account for that heterogeneity. The mixed-effects tutorials show how to extend this model accordingly.\nDiagnostic tools are method-agnostic. Whether you use MLE, MAP, or any other estimation method in NoLimits.jl, functions such as plot_fits and plot_observation_distributions work identically, making it straightforward to compare results across methods.","category":"section"},{"location":"tutorials/mixed-effects-softtree-saem/#Mixed-Effects-Tutorial-4:-SoftTree-Differential-Equation-Components-(SAEM)","page":"Mixed-Effects Tutorial 4: SoftTree Differential-Equation Components (SAEM)","title":"Mixed-Effects Tutorial 4: SoftTree Differential-Equation Components (SAEM)","text":"When building mechanistic models of longitudinal data, you often know the broad structure of the system – compartments, conservation laws, transfer pathways – but not the precise functional forms that govern how material moves between states. Neural networks are one way to learn those unknown rate functions from data, as shown in Tutorial 3. Soft decision trees offer an appealing alternative. They can approximate arbitrary nonlinear mappings, yet their branching structure provides built-in feature selection and piecewise-smooth approximation that is often easier to interpret. For the low-dimensional inputs typical of scientific rate functions (a single state variable, or time itself), soft trees can match neural network flexibility with substantially fewer parameters.\n\nIn this tutorial, you will build a mixed-effects ODE model in which soft decision trees parameterize the ODE right-hand side, then estimate the model with the Stochastic Approximation Expectation-Maximization (SAEM) algorithm. The model is structurally parallel to Tutorial 3, so you can directly compare the two function-approximation strategies on the same data and compartmental structure.","category":"section"},{"location":"tutorials/mixed-effects-softtree-saem/#Learning-Goals","page":"Mixed-Effects Tutorial 4: SoftTree Differential-Equation Components (SAEM)","title":"Learning Goals","text":"By the end of this tutorial, you will be able to:\n\nDeclare SoftTreeParameters blocks that create differentiable decision trees whose flattened parameters join the fixed-effects vector, exposing callable functions (e.g., STA1) for use inside @DifferentialEquation.\nWire multiple soft trees into a two-compartment transfer ODE, letting the trees learn unknown rate functions from data.\nCouple each tree's parameter vector to subject-level random effects via MvNormal distributions, giving every individual a personalized version of the dynamics.\nFit the model with SAEM using closed-form Gaussian updates for the random-effect means.\nVisualize individual-level trajectories and observation distributions to assess model adequacy.","category":"section"},{"location":"tutorials/mixed-effects-softtree-saem/#Step-1:-Data-Setup","page":"Mixed-Effects Tutorial 4: SoftTree Differential-Equation Components (SAEM)","title":"Step 1: Data Setup","text":"In this step, you will load the Theophylline dataset used throughout these tutorials. The dataset records time-series measurements for 12 subjects and provides a clean example of two-compartment transfer dynamics: a substance enters a depot (input) compartment and moves to a central (observed) compartment, where it is measured and gradually cleared. You will reshape the data so that the initial amount d appears as a constant covariate for each subject.\n\nusing NoLimits\nusing CSV\nusing DataFrames\nusing Distributions\nusing Downloads\nusing Random\nusing LinearAlgebra\nusing OrdinaryDiffEq\nusing SciMLBase\nusing Turing\n\ninclude(joinpath(@__DIR__, \"_data_loaders.jl\"))\n\nRandom.seed!(654)\n\ntheoph_df = load_theoph()\n\nfunction build_theoph_non_event_df(tbl::DataFrame)\n    df = DataFrame(\n        ID=Int.(tbl.Subject),\n        t=Float64.(tbl.Time),\n        y=Float64.(tbl.conc),\n        d=Float64.(tbl.Wt .* tbl.Dose),\n    )\n    sort!(df, [:ID, :t])\n    return df\nend\n\ndf = build_theoph_non_event_df(theoph_df)\nfirst(df, 10)","category":"section"},{"location":"tutorials/mixed-effects-softtree-saem/#Step-2:-Define-SoftTree-Driven-ODE-Mixed-Effects-Model","page":"Mixed-Effects Tutorial 4: SoftTree Differential-Equation Components (SAEM)","title":"Step 2: Define SoftTree-Driven ODE Mixed-Effects Model","text":"In this step, you will construct the full mixed-effects model. The guiding idea is the same as in the neural ODE tutorial: rather than specifying closed-form rate laws, you let data-driven function approximators learn the rate functions directly from observations. The difference is the choice of approximator. Each SoftTreeParameters block declares a soft decision tree with a specified input dimension and depth. The depth_st parameter controls expressiveness – a tree of depth d has 2^d leaf nodes, each contributing a smooth local approximation. The block's flattened parameters become part of the fixed-effects vector, and the associated callable function (e.g., STA1) evaluates the tree at any input.\n\nThe ODE system wires four soft trees into a two-compartment transfer model:\n\nfA1(t) and fA2(t) govern the dynamics of the depot (input) compartment.\nfC1(t) and fC2(t) govern the dynamics of the central (observed) compartment.\n\nTo capture between-subject variability, each tree's parameter vector is paired with a subject-level random-effect vector drawn from an MvNormal distribution centered on the population parameters. This gives every individual a personalized version of the transfer dynamics while sharing structure across the population.\n\nusing NoLimits\nusing Distributions\nusing LinearAlgebra\nusing OrdinaryDiffEq\n\ndepth_st = 2\n\nmodel_raw = @Model begin\n    @helpers begin\n        softplus(u) = u > 20 ? u : log1p(exp(u))\n    end\n\n    @covariates begin\n        t = Covariate()\n        d = ConstantCovariate(constant_on=:ID)\n    end\n\n    @fixedEffects begin\n        sigma = RealNumber(1.0, scale=:log, prior=LogNormal(log(1.0), 0.5), calculate_se=true)\n\n        gA1 = SoftTreeParameters(1, depth_st; function_name=:STA1, calculate_se=false)\n        gA2 = SoftTreeParameters(1, depth_st; function_name=:STA2, calculate_se=false)\n        gC1 = SoftTreeParameters(1, depth_st; function_name=:STC1, calculate_se=false)\n        gC2 = SoftTreeParameters(1, depth_st; function_name=:STC2, calculate_se=false)\n    end\n\n    @randomEffects begin\n        etaA1 = RandomEffect(MvNormal(gA1, Diagonal(ones(length(gA1)))); column=:ID)\n        etaA2 = RandomEffect(MvNormal(gA2, Diagonal(ones(length(gA2)))); column=:ID)\n        etaC1 = RandomEffect(MvNormal(gC1, Diagonal(ones(length(gC1)))); column=:ID)\n        etaC2 = RandomEffect(MvNormal(gC2, Diagonal(ones(length(gC2)))); column=:ID)\n    end\n\n    @DifferentialEquation begin\n        a_A(t) = softplus(depot)\n        x_C(t) = softplus(center)\n\n        fA1(t) = softplus(STA1([t / 24], etaA1)[1])\n        fA2(t) = softplus(STA2([a_A(t)], etaA2)[1])\n        fC1(t) = -softplus(STC1([x_C(t)], etaC1)[1])\n        fC2(t) = softplus(STC2([t / 24], etaC2)[1])\n\n        D(depot) ~ -d * fA1(t) - fA2(t)\n        D(center) ~ d * fA1(t) + fA2(t) + fC1(t) + d * fC2(t)\n    end\n\n    @initialDE begin\n        depot = d\n        center = 0.0\n    end\n\n    @formulas begin\n        y ~ Normal(center(t), sigma)\n    end\nend\n\nmodel = set_solver_config(\n    model_raw;\n    saveat_mode=:saveat,\n    alg=AutoTsit5(Rosenbrock23()),\n    kwargs=(abstol=1e-2, reltol=1e-2),\n)\n\nBefore moving on, inspect the assembled model to verify that all blocks – covariates, fixed effects, random effects, ODE, and formulas – are correctly wired together.","category":"section"},{"location":"tutorials/mixed-effects-softtree-saem/#Model-Summary","page":"Mixed-Effects Tutorial 4: SoftTree Differential-Equation Components (SAEM)","title":"Model Summary","text":"model_summary = NoLimits.summarize(model)\nmodel_summary","category":"section"},{"location":"tutorials/mixed-effects-softtree-saem/#Step-3:-Build-DataModel-and-Configure-SAEM","page":"Mixed-Effects Tutorial 4: SoftTree Differential-Equation Components (SAEM)","title":"Step 3: Build DataModel and Configure SAEM","text":"In this step, you will pair the model with the observed data by constructing a DataModel, then configure the SAEM fitting algorithm. SAEM alternates between two phases: an E-step that samples subject-level random effects conditional on the current population parameters, and an M-step that updates those population parameters using stochastic sufficient statistics. Setting builtin_stats=:closed_form enables built-in closed-form block updates. This avoids gradient-based optimization for those mapped parameters and can substantially improve convergence stability, particularly when the random-effect vectors are high-dimensional (as they are here, since each tree's full parameter vector is individualized).\n\nSeveral configuration details are worth noting. The re_mean_params mapping tells SAEM which fixed-effect parameter serves as the population mean for each random-effect distribution. The ebe_multistart_n and ebe_multistart_k settings control multistart initialization of the empirical Bayes estimates, reducing the risk of poor local optima during early iterations. Finally, progress=true displays SAEM iteration progress at the outer level, while progress=false inside turing_kwargs suppresses verbose output from the inner sampler.\n\ndm = DataModel(model, df; primary_id=:ID, time_col=:t)\n\nsaem_method = NoLimits.SAEM(;\n    sampler=MH(),\n    builtin_stats=:closed_form,\n    re_mean_params=(; etaA1=:gA1, etaA2=:gA2, etaC1=:gC1, etaC2=:gC2),\n    re_cov_params=NamedTuple(),\n    resid_var_param=:sigma,\n    maxiters=1000,\n    mcmc_steps=80,\n    t0=30,\n    turing_kwargs=(n_samples=80, n_adapt=0, progress=false),\n    optim_kwargs=(maxiters=300,),\n    verbose=false,\n    progress=true,\n    ebe_multistart_n=300,\n    ebe_multistart_k=3,\n    ebe_rescue_on_high_grad=false\n)\n\nserialization = SciMLBase.EnsembleThreads()\n\nBefore fitting, review the data model summary to confirm that individuals, covariates, and grouping structures were parsed as expected.","category":"section"},{"location":"tutorials/mixed-effects-softtree-saem/#DataModel-Summary","page":"Mixed-Effects Tutorial 4: SoftTree Differential-Equation Components (SAEM)","title":"DataModel Summary","text":"dm_summary = NoLimits.summarize(dm)\ndm_summary","category":"section"},{"location":"tutorials/mixed-effects-softtree-saem/#Step-4:-Fit-and-Inspect-Core-Result-Summary","page":"Mixed-Effects Tutorial 4: SoftTree Differential-Equation Components (SAEM)","title":"Step 4: Fit and Inspect Core Result Summary","text":"In this step, you will run the SAEM algorithm and examine the results. The algorithm iterates up to 1000 times, using Metropolis-Hastings sampling for the random effects within each E-step. After fitting completes, you will extract the final objective value and parameter count as a quick sanity check before looking at more detailed diagnostics.\n\nres_saem = fit_model(\n    dm,\n    saem_method;\n    serialization=serialization,\n    rng=Random.Xoshiro(31),\n)\n\n(\n    objective=NoLimits.get_objective(res_saem),\n    n_params=length(NoLimits.get_params(res_saem; scale=:untransformed)),\n)\n\nFor a more detailed view – including parameter estimates and convergence diagnostics – call the summarize function on the fit result.\n\nfit_summary_saem = NoLimits.summarize(res_saem)\nfit_summary_saem","category":"section"},{"location":"tutorials/mixed-effects-softtree-saem/#Step-5:-Fitted-Trajectories-(First-2-Individuals)","page":"Mixed-Effects Tutorial 4: SoftTree Differential-Equation Components (SAEM)","title":"Step 5: Fitted Trajectories (First 2 Individuals)","text":"In this step, you will overlay the model's predicted trajectories on the raw observations for the first two subjects. Plotting fitted curves against data provides an immediate visual assessment of model adequacy: do the predicted dynamics capture the timing and magnitude of the observed response?\n\np_fit_saem = plot_fits(\n    res_saem;\n    observable=:y,\n    individuals_idx=[1, 2],\n    ncols=2,\n    shared_x_axis=true,\n    shared_y_axis=true,\n)\n\np_fit_saem","category":"section"},{"location":"tutorials/mixed-effects-softtree-saem/#Step-6:-Observation-Distribution-Diagnostic","page":"Mixed-Effects Tutorial 4: SoftTree Differential-Equation Components (SAEM)","title":"Step 6: Observation Distribution Diagnostic","text":"As a final check, you will examine the implied observation distribution at a single data point for the first individual. Rather than showing only a point prediction, this plot displays the full predictive distribution, letting you assess whether the residual variance is well-calibrated and the model's uncertainty envelope is reasonable.\n\np_obs_saem = plot_observation_distributions(\n    res_saem;\n    observables=:y,\n    individuals_idx=1,\n    obs_rows=1,\n)\n\np_obs_saem","category":"section"},{"location":"tutorials/mixed-effects-softtree-saem/#Interpretation-Notes","page":"Mixed-Effects Tutorial 4: SoftTree Differential-Equation Components (SAEM)","title":"Interpretation Notes","text":"This modeling pattern combines mechanistic compartmental structure with soft decision tree function approximators inside a single mixed-effects ODE. The compartments encode known domain knowledge (mass conservation, transfer pathways), while the trees learn the unknown rate functions from data. This separation means you retain interpretable system structure without needing to specify rate-law functional forms in advance.\nCompared to neural networks, soft trees can be more parameter-efficient for the low-dimensional inputs common in scientific rate functions, and their piecewise-smooth approximation may be easier to inspect post hoc. The choice between the two is problem-dependent; both can be embedded in the same NoLimits framework with minimal code changes (compare this tutorial with Tutorial 3).\nThe built-in closed-form updates (builtin_stats=:closed_form) materially improve SAEM convergence stability when the individualized parameter vectors are high-dimensional, as they are here.\nThe settings in this tutorial are intentionally modest to keep runtime short. For production analyses, consider increasing maxiters, mcmc_steps, and the number of MCMC samples to ensure thorough convergence.","category":"section"},{"location":"model-building/covariates/#@covariates","page":"@covariates","title":"@covariates","text":"Covariates encode subject-level attributes, time-varying measurements, and externally observed signals that inform model dynamics and observation distributions. The @covariates block declares all covariates used by formulas, random-effect distributions, and differential equation components.\n\nNoLimits distinguishes three covariate classes, each with distinct semantics for how values are resolved at evaluation time:\n\nVarying covariates (Covariate, CovariateVector) – values that change across observations and are accessed row-by-row from the data.\nConstant covariates (ConstantCovariate, ConstantCovariateVector) – values that remain fixed within a grouping level (e.g., per subject or per site) and are extracted once per group.\nDynamic covariates (DynamicCovariate, DynamicCovariateVector) – time-series values that are converted into continuous interpolating functions, enabling evaluation at arbitrary time points within differential equations and formulas.","category":"section"},{"location":"model-building/covariates/#Core-Syntax","page":"@covariates","title":"Core Syntax","text":"Inside @covariates, each line is an assignment that maps a name to a covariate constructor. No other statement forms are permitted.\n\nusing DataInterpolations\n\ncov = @covariates begin\n    t = Covariate()\n    x = ConstantCovariateVector([:Age, :BMI]; constant_on=:ID)\n    w = DynamicCovariate(; interpolation=LinearInterpolation)\nend","category":"section"},{"location":"model-building/covariates/#Constructor-Reference","page":"@covariates","title":"Constructor Reference","text":"The following constructor forms are available within @covariates:\n\nname = Covariate()\nname = CovariateVector([:col1, :col2, ...])\nname = ConstantCovariate(; constant_on=...)\nname = ConstantCovariateVector([:col1, :col2, ...]; constant_on=...)\nname = DynamicCovariate(; interpolation=...)\nname = DynamicCovariateVector([:col1, :col2, ...]; interpolations=[...])\n\nNote the following macro-level conventions:\n\nFor scalar constructors (Covariate, ConstantCovariate, DynamicCovariate), the left-hand side name determines the DataFrame column that will be read. Passing an explicit column name to these constructors is not supported.\nFor vector constructors, the column names are specified in the first positional argument.\nCovariate and CovariateVector accept no keyword arguments.","category":"section"},{"location":"model-building/covariates/#Supported-Interpolation-Types","page":"@covariates","title":"Supported Interpolation Types","text":"Dynamic covariates construct interpolating functions from observed time–value pairs using DataInterpolations.jl. The following interpolation methods are currently supported:\n\nConstantInterpolation\nSmoothedConstantInterpolation\nLinearInterpolation\nQuadraticInterpolation\nLagrangeInterpolation\nQuadraticSpline\nCubicSpline\nAkimaInterpolation\n\nWhen no interpolation is specified, DynamicCovariate defaults to LinearInterpolation. To use a non-default method, ensure that DataInterpolations is loaded in the model-definition environment (e.g., using DataInterpolations).","category":"section"},{"location":"model-building/covariates/#Example:-Mixed-Covariate-Specification","page":"@covariates","title":"Example: Mixed Covariate Specification","text":"The following example demonstrates all three covariate classes used together, including scalar and vector variants.\n\nusing NoLimits\nusing DataInterpolations\n\ncov = @covariates begin\n    t = Covariate()\n    z = CovariateVector([:z1, :z2, :z3])\n\n    x = ConstantCovariateVector([:Age, :BMI]; constant_on=:ID)\n    center = ConstantCovariate(; constant_on=:SITE)\n\n    w1 = DynamicCovariate(; interpolation=CubicSpline)\n    w2 = DynamicCovariateVector(\n        [:u1, :u2];\n        interpolations=[LinearInterpolation, AkimaInterpolation],\n    )\nend","category":"section"},{"location":"model-building/covariates/#Example:-Covariates-in-a-Nonlinear-Model","page":"@covariates","title":"Example: Covariates in a Nonlinear Model","text":"Covariates can appear in random-effect distributions and observation formulas, enabling covariate-dependent distributional parameters. In the model below, subject-level attributes modulate both the random-effect distribution and the outcome probability.\n\nusing NoLimits\nusing Distributions\nusing DataInterpolations\n\nmodel = @Model begin\n    @fixedEffects begin\n        b0 = RealNumber(0.2)\n        sη = RealNumber(0.5, scale=:log)\n    end\n\n    @covariates begin\n        t = Covariate()\n        x = ConstantCovariateVector([:Age, :BMI]; constant_on=:ID)\n        w = DynamicCovariate(; interpolation=QuadraticSpline)\n    end\n\n    @randomEffects begin\n        η = RandomEffect(LogNormal(b0 + 0.01 * x.Age^2 + log1p(abs(x.BMI)), sη); column=:ID)\n    end\n\n    @formulas begin\n        p = 1 / (1 + exp(-(0.3 + tanh(η) + 0.02 * w(t)^2 + 0.001 * x.BMI^2)))\n        y ~ Bernoulli(p)\n    end\nend","category":"section"},{"location":"model-building/covariates/#constant_on-and-Random-Effect-Grouping","page":"@covariates","title":"constant_on and Random-Effect Grouping","text":"Constant covariates that appear inside random-effect distributions must be invariant within the corresponding grouping level. The following rules apply:\n\nIf there is exactly one random-effect grouping column, a missing constant_on keyword is automatically set to that column.\nIf there are multiple random-effect grouping columns, constant_on must be specified explicitly.\nA constant covariate used in a random-effect distribution must declare constant_on for (at least) that random effect's grouping column.","category":"section"},{"location":"model-building/covariates/#Covariate-Rules-in-Differential-Equations","page":"@covariates","title":"Covariate Rules in Differential Equations","text":"Covariates used within @DifferentialEquation are subject to restrictions that reflect the continuous-time nature of ODE integration:\n\nConstant covariates may appear as ordinary variables (they do not depend on time).\nDynamic covariates must be called as functions of time, e.g., w(t). Referencing a dynamic covariate without (t) is rejected.\nVarying covariates (Covariate, CovariateVector) are not permitted in differential equations, as they lack a continuous-time representation. Use DynamicCovariate with an appropriate interpolation instead.\nCalling a constant covariate as a function (e.g., x(t)) is rejected.","category":"section"},{"location":"model-building/covariates/#Data-Model-Validation","page":"@covariates","title":"Data-Model Validation","text":"When a DataModel is constructed, the covariate declarations are validated against the supplied DataFrame:\n\nThe configured time_col must be declared as Covariate() or DynamicCovariate() in the @covariates block.\nAll declared covariate columns must be present in the DataFrame and free of missing values.\nConstant covariates are checked for consistency: values must be identical within each level of the primary_id column and within each declared constant_on group.","category":"section"},{"location":"tutorials/mixed-effects-left-censored-virload50-laplace/#Mixed-Effects-Tutorial-6:-Left-Censored-Nonlinear-Model-(Laplace)","page":"Mixed-Effects Tutorial 6: Left-Censored Nonlinear Model (Laplace)","title":"Mixed-Effects Tutorial 6: Left-Censored Nonlinear Model (Laplace)","text":"In many biomedical assays, measurements below a detection threshold cannot be reliably quantified. This situation, known as left-censoring, arises whenever an instrument's lower limit of quantification (LLOQ) truncates the observable range. HIV viral load monitoring is a canonical example: modern RT-PCR assays report \"below detectable limit\" for viral RNA concentrations under approximately 50 copies/mL – or equivalently, below about 1.7 on the log10 scale. In patients responding well to antiretroviral therapy, censored observations can account for 30–40% of all measurements, making proper statistical treatment essential.\n\nHow should these below-limit values be handled? The answer matters. Dropping censored rows discards information and inflates uncertainty. Substituting the detection limit as though it were a true measurement concentrates probability mass at that value and biases parameter estimates downward. The principled approach is a censored likelihood: uncensored observations contribute their usual probability density, while censored observations contribute the cumulative probability of falling at or below the detection threshold. This formulation correctly encodes what we actually know – that the true value lies somewhere below the limit, without committing to a specific magnitude.\n\nIn this tutorial, you will fit a nonlinear mixed-effects model to the virload50 dataset from the npde R package, which contains longitudinal log10 viral load measurements from 50 HIV-positive patients. The structural model is a bi-exponential decay function that captures two distinct phases of viral dynamics: rapid initial suppression and slower long-term decline. Subject-specific parameters enter through LogNormal random effects, making the model nonlinear in the random effects. You will handle left-censoring directly in the observation model using NoLimits' censored(...) syntax and estimate the model with the Laplace approximation.","category":"section"},{"location":"tutorials/mixed-effects-left-censored-virload50-laplace/#Learning-Goals","page":"Mixed-Effects Tutorial 6: Left-Censored Nonlinear Model (Laplace)","title":"Learning Goals","text":"By the end of this tutorial, you will know how to:\n\nPrepare censored longitudinal data. Structure a dataset where left-censored observations are flagged by an indicator variable and pinned at the detection limit value.\nSpecify a nonlinear mixed-effects model. Define subject-specific LogNormal random effects that parameterize a bi-exponential mean function, ensuring predicted viral loads remain positive.\nEncode left-censoring in the likelihood. Use censored(Normal(mu, sigma), lower=1.7, upper=Inf) so that the likelihood automatically distinguishes between density contributions (for observed values) and cumulative probability contributions (for censored values).\nEstimate with the Laplace approximation. Integrate over the random effects using a second-order expansion of the log-posterior around the empirical Bayes estimates.\nDiagnose and quantify uncertainty. Inspect fitted trajectories, predictive observation distributions, and Wald-based confidence intervals for the fixed-effects parameters.","category":"section"},{"location":"tutorials/mixed-effects-left-censored-virload50-laplace/#Step-1:-Data-Setup","page":"Mixed-Effects Tutorial 6: Left-Censored Nonlinear Model (Laplace)","title":"Step 1: Data Setup","text":"In this step, you will load the virload50 dataset and prepare it for modeling. The dataset contains four columns: a subject identifier (ID), observation time (Time), log10 viral load (Log_VL), and a censoring indicator (cens, where 1 flags values at or below the detection limit of 1.7). After selecting these columns, you will enforce the correct types and sort by subject and time – a requirement for the internal data structures used by NoLimits.\n\nThe summary statistics printed at the end provide a quick overview of the dataset dimensions and the fraction of censored observations.\n\nusing NoLimits\nusing CSV\nusing DataFrames\nusing Distributions\nusing Downloads\nusing Random\nusing SciMLBase\n\ninclude(joinpath(@__DIR__, \"_data_loaders.jl\"))\n\nRandom.seed!(2026)\n\ndf = load_virload50()\nselect!(df, [:ID, :Time, :Log_VL, :cens])\ndf.ID = string.(df.ID)\ndf.Time = Float64.(df.Time)\ndf.Log_VL = Float64.(df.Log_VL)\ndf.cens = Int.(df.cens)\nsort!(df, [:ID, :Time])\n\n(\n    n_rows = nrow(df),\n    n_subjects = length(unique(df.ID)),\n    n_censored = count(==(1), df.cens),\n)","category":"section"},{"location":"tutorials/mixed-effects-left-censored-virload50-laplace/#Step-2:-Define-the-Nonlinear-Left-Censored-Mixed-Effects-Model","page":"Mixed-Effects Tutorial 6: Left-Censored Nonlinear Model (Laplace)","title":"Step 2: Define the Nonlinear Left-Censored Mixed-Effects Model","text":"In this step, you will define the structural model for viral load dynamics. The bi-exponential decay function captures two biologically distinct phases: the rapid initial clearance of free virus from the plasma, and the slower decline driven by the loss of productively infected cells. On the original scale, the model for subject i at time t is:\n\nV_i(t) = A_i e^-k_1i t + B_i e^-k_2i t\nqquad mu_it = log_10(V_i(t))\n\nEach subject-specific parameter (A_i, B_i, k_1i, k_2i) is drawn from a LogNormal distribution, which guarantees positivity – a necessary constraint since both amplitudes and rate constants must be strictly positive for the model to be biologically meaningful. The fixed effects (beta_A, beta_B, beta_k1, beta_k2) represent population-level medians on the log scale, while the omega parameters govern the magnitude of between-subject variability for each parameter.\n\nThe observation model is where the censoring logic enters. By writing censored(Normal(mu, sigma), lower=1.7, upper=Inf), you specify a Normal distribution that is left-censored at the log10 detection limit of 1.7. NoLimits handles this as follows: when the recorded Log_VL value exceeds 1.7, the observation contributes the standard Normal density; when Log_VL equals 1.7 (the pinned value for censored rows in this dataset), the observation instead contributes the cumulative probability Phibigl((17 - mu)  sigmabigr), representing the probability that the true value falls below the detection limit. This censored-likelihood approach is statistically exact and avoids the biases introduced by ad hoc imputation or deletion strategies.\n\nmodel = @Model begin\n    @covariates begin\n        Time = Covariate()\n    end\n\n    @fixedEffects begin\n        beta_A = RealNumber(5.5, calculate_se=true)\n        beta_B = RealNumber(4.2, calculate_se=true)\n        beta_k1 = RealNumber(-1.6, calculate_se=true)\n        beta_k2 = RealNumber(-3.5, calculate_se=true)\n\n        omega_A = RealNumber(0.20, scale=:log, calculate_se=true)\n        omega_B = RealNumber(0.20, scale=:log, calculate_se=true)\n        omega_k1 = RealNumber(0.20, scale=:log, calculate_se=true)\n        omega_k2 = RealNumber(0.20, scale=:log, calculate_se=true)\n\n        sigma = RealNumber(0.18, scale=:log, calculate_se=true)\n    end\n\n    @randomEffects begin\n        A_i = RandomEffect(LogNormal(beta_A, omega_A); column=:ID)\n        B_i = RandomEffect(LogNormal(beta_B, omega_B); column=:ID)\n        k1_i = RandomEffect(LogNormal(beta_k1, omega_k1); column=:ID)\n        k2_i = RandomEffect(LogNormal(beta_k2, omega_k2); column=:ID)\n    end\n\n    @formulas begin\n        V_i = A_i * exp(-k1_i * Time) + B_i * exp(-k2_i * Time)\n        mu = log10(V_i)\n\n        Log_VL ~ censored(Normal(mu, sigma), lower=1.7, upper=Inf)\n    end\nend\n\nNoLimits.summarize(model)","category":"section"},{"location":"tutorials/mixed-effects-left-censored-virload50-laplace/#Step-3:-Build-DataModel-and-Configure-Laplace","page":"Mixed-Effects Tutorial 6: Left-Censored Nonlinear Model (Laplace)","title":"Step 3: Build DataModel and Configure Laplace","text":"In this step, you will bind the model to the dataset by constructing a DataModel. This validates that all required columns are present and correctly typed, groups observations by subject, and assembles the internal data structures needed for estimation.\n\nNext, you will configure the Laplace approximation. This method approximates the marginal likelihood by finding the mode of each subject's conditional random-effects distribution (the empirical Bayes estimates) and using a second-order Taylor expansion around those modes to integrate out the random effects analytically. Two sets of optimization controls are available: inner_kwargs governs the per-subject random-effects optimization, while optim_kwargs governs the outer fixed-effects optimization. Multistart is disabled here for speed, but enabling it can help avoid local optima in models with more complex likelihood surfaces.\n\ndm = DataModel(model, df; primary_id=:ID, time_col=:Time)\n\nlaplace_method = NoLimits.Laplace(;\n    optim_kwargs=(maxiters=250,),\n    inner_kwargs=(maxiters=100,),\n    multistart_n=0,\n    multistart_k=0,\n)\n\nserialization = SciMLBase.EnsembleThreads()\n\nNoLimits.summarize(dm)","category":"section"},{"location":"tutorials/mixed-effects-left-censored-virload50-laplace/#Step-4:-Fit-and-Inspect-Core-Summary","page":"Mixed-Effects Tutorial 6: Left-Censored Nonlinear Model (Laplace)","title":"Step 4: Fit and Inspect Core Summary","text":"With the data model and estimation method in place, you can now run the fit. The Laplace algorithm alternates between two stages: an inner loop that updates the empirical Bayes estimates of the random effects for each subject, and an outer loop that optimizes the fixed-effects parameters with respect to the Laplace-approximated marginal likelihood. After convergence, the summary reports the estimated parameter values alongside the final objective function value.\n\nres = fit_model(\n    dm,\n    laplace_method;\n    serialization=serialization,\n    rng=Random.Xoshiro(7003),\n)\n\nNoLimits.summarize(res)","category":"section"},{"location":"tutorials/mixed-effects-left-censored-virload50-laplace/#Step-5:-Fitted-Trajectories-(First-2-Individuals)","page":"Mixed-Effects Tutorial 6: Left-Censored Nonlinear Model (Laplace)","title":"Step 5: Fitted Trajectories (First 2 Individuals)","text":"In this step, you will overlay the model's fitted trajectories on the observed data for a visual check of model adequacy. For subjects with censored observations, pay attention to the predicted curve near the detection limit: a well-fitting model should produce predicted values at or near 1.7 at censored time points, reflecting the fact that the true viral load lies somewhere below this threshold rather than at a precisely known value.\n\np_fit = plot_fits(\n    res;\n    observable=:Log_VL,\n    individuals_idx=[1, 2],\n    ncols=2,\n    shared_x_axis=true,\n    shared_y_axis=true,\n)\n\np_fit","category":"section"},{"location":"tutorials/mixed-effects-left-censored-virload50-laplace/#Step-6:-Observation-Distribution-Diagnostic-(First-Individual)","page":"Mixed-Effects Tutorial 6: Left-Censored Nonlinear Model (Laplace)","title":"Step 6: Observation Distribution Diagnostic (First Individual)","text":"This diagnostic reveals the full predictive distribution at selected time points for a single subject. For uncensored observations, you will see a Normal density centered on the predicted log10 viral load. For censored observations, the distribution is truncated at the detection limit, with the probability mass below 1.7 collapsed into a point mass. Examining these distributions is a useful way to verify that the censored likelihood is behaving as intended and that the model assigns appropriate probability to the below-limit region.\n\np_obs = plot_observation_distributions(\n    res;\n    observables=:Log_VL,\n    individuals_idx=1,\n    obs_rows=[1, 2],\n)\n\np_obs","category":"section"},{"location":"tutorials/mixed-effects-left-censored-virload50-laplace/#Step-7:-Wald-Uncertainty-Quantification","page":"Mixed-Effects Tutorial 6: Left-Censored Nonlinear Model (Laplace)","title":"Step 7: Wald Uncertainty Quantification","text":"In this final step, you will assess the precision of the estimated fixed effects by computing Wald-based confidence intervals. The Wald method constructs approximate 95% intervals from the observed Fisher information matrix – that is, the curvature of the log-likelihood at the optimum. This is computationally inexpensive and provides a practical first assessment of parameter identifiability: wide intervals may signal that the data contain insufficient information to pin down a particular parameter.\n\nuq = compute_uq(\n    res;\n    method=:wald,\n    n_draws=800,\n    level=0.95,\n)\n\nNoLimits.summarize(uq)\n\nFor a consolidated report combining point estimates and their uncertainty, pass both the fit result and the uncertainty object to summarize. This tabular format is convenient for inclusion in manuscripts and supplementary materials.\n\nNoLimits.summarize(res, uq)\n\nFinally, you can visualize the implied sampling distributions of the fixed-effects parameters on the natural (untransformed) scale. Parameters estimated on the log scale (the omega and sigma parameters) are back-transformed before plotting, so the density plots reflect the scale on which these quantities are scientifically interpretable.\n\nplot_uq_distributions(uq; scale=:natural, plot_type=:density, show_legend=false)","category":"section"},{"location":"tutorials/mixed-effects-left-censored-virload50-laplace/#Interpretation-Notes","page":"Mixed-Effects Tutorial 6: Left-Censored Nonlinear Model (Laplace)","title":"Interpretation Notes","text":"Nonlinearity and the Laplace approximation. The model is nonlinear in its random effects because subject-level LogNormal parameters (A_i, B_i, k1_i, k2_i) appear inside a bi-exponential trajectory. This nonlinearity is precisely what necessitates the Laplace approximation rather than a simpler linear mixed-effects approach.\nWhy the censored likelihood matters. Left-censored rows contribute through the cumulative Normal probability of falling below 1.7, not through a standard density evaluated at the pinned recorded value. This distinction is critical for unbiased estimation whenever detection limits are present.\nA reusable template. The workflow demonstrated here – model definition, data binding, Laplace estimation, diagnostics, and uncertainty quantification – serves as a baseline template for censored nonlinear mixed-effects analyses in NoLimits. For datasets with higher censoring fractions or more complex censoring patterns (e.g., interval censoring), the same censored(...) syntax generalizes naturally.","category":"section"}]
}
